{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52fcc20",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2de5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pandas as pd, itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n",
    "\n",
    "\n",
    "\n",
    "# Configure plotly to use static rendering if widgets fail\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "float_formatter = \"{:.5f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15f0d1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1437fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [-inf, -inf, 0., -inf, -inf],\n",
      "        [-inf, -inf, 0., 0., -inf]])\n"
     ]
    }
   ],
   "source": [
    "# ---------- constants ----------\n",
    "MODEL_NAME = 'v3_2layer_100dig_128d'\n",
    "MODEL_PATH = \"models/\" + MODEL_NAME + \".pt\"\n",
    "\n",
    "DATASET_NAME = \"listlen2_digits100_dupes_traindupesonly\"\n",
    "# listlen2_digits10_dupes\n",
    "# listlen2_digits10_nodupes\n",
    "# listlen2_digits100_dupes\n",
    "# listlen2_digits100_nodupes\n",
    "\n",
    "LIST_LEN = 2 # [d1, d2]\n",
    "SEQ_LEN = LIST_LEN * 2 + 1 # [d1, d2, SEP, o1, o2]\n",
    "\n",
    "N_DIGITS = 100\n",
    "DIGITS = list(range(N_DIGITS)) # 100 digits from 0 to 99\n",
    "PAD = N_DIGITS # special padding token\n",
    "SEP = N_DIGITS + 1 # special seperator token for the model to think about the input (+1 to avoid confusion with the last digit)\n",
    "VOCAB = len(DIGITS) + 2  # + the special tokens\n",
    "\n",
    "D_MODEL = 128\n",
    "N_HEAD = 1 # 1\n",
    "N_LAYER = 2 # 2\n",
    "USE_LN = False # use layer norm in model\n",
    "USE_BIAS = False # use bias in model\n",
    "FREEZE_WV = True # no value matrix in attn \n",
    "FREEZE_WO = True # no output matrix in attn (i.e. attn head can only copy inputs to outputs)\n",
    "\n",
    "LEARNING_RATE = 1e-3 # default 1e-3\n",
    "WEIGHT_DECAY = 0.01 # default 0.01\n",
    "MAX_TRAIN_STEPS = 500_000 # max training steps\n",
    "USE_CHECKPOINTING = True # whether to use checkpointing for training\n",
    "TRACK_ATTN_FLOW = False # whether to track attention flow during training\n",
    "\n",
    "DEV = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "device = DEV\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ---------- mask ----------\n",
    "# attention mask for [d1, d2, SEP, o1, o2] looks like this (query rows are horizontal, key columns are vertical):\n",
    "# -    d1    d2    SEP    o1    o2   (keys)\n",
    "# d1  -inf  -inf   -inf  -inf  -inf\n",
    "# d2   0    -inf   -inf  -inf  -inf\n",
    "# SEP  0      0    -inf  -inf  -inf\n",
    "# o1  -inf  -inf    0    -inf   -inf\n",
    "# o2  -inf  -inf    0      0    -inf\n",
    "# (queries)\n",
    "\n",
    "mask_bias = torch.triu(torch.ones(SEQ_LEN, SEQ_LEN) * float(\"-inf\")) # upper triangular bias mask (lead_diag & above = -inf, rest = 0)\n",
    "mask_bias[0, 0] = 0. #Â don't want a full row of -inf! otherwise we get nan erros & training breaks\n",
    "mask_bias[LIST_LEN+1:, :LIST_LEN] = float(\"-inf\") # stop output tokens from attending to input tokens\n",
    "mask_bias = mask_bias.unsqueeze(0).unsqueeze(0) # (1,1,T,T) broadcastable across batch and heads\n",
    "\n",
    "print(mask_bias.cpu()[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a60abc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([ 37,   1, 101, 100, 100])\n",
      "Target: tensor([ 37,   1, 101,  37,   1])\n",
      "Train dataset size: 8020, Validation dataset size: 1980\n"
     ]
    }
   ],
   "source": [
    "# ---------- data ----------\n",
    "DATASET_PATH = f\"data/{DATASET_NAME}.pt\"\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}. Please run data.py to generate it.\")\n",
    "\n",
    "saved_data = torch.load(DATASET_PATH, weights_only=False)\n",
    "train_ds = saved_data['train']\n",
    "val_ds = saved_data['val']\n",
    "\n",
    "train_batch_size = min(128, len(train_ds))\n",
    "val_batch_size = min(256, len(val_ds))\n",
    "train_dl = DataLoader(train_ds, train_batch_size, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, val_batch_size, drop_last=False)\n",
    "\n",
    "print(\"Input:\", train_ds[0][0])\n",
    "print(\"Target:\", train_ds[0][1])\n",
    "print(f\"Train dataset size: {len(train_ds)}, Validation dataset size: {len(val_ds)}\")\n",
    "\n",
    "# Sanity check: dataset token range must fit VOCAB\n",
    "with torch.no_grad():\n",
    "    in_max = train_ds.tensors[0].max().item()\n",
    "    tgt_max = train_ds.tensors[1].max().item()\n",
    "    ds_max = max(in_max, tgt_max)\n",
    "if ds_max >= VOCAB:\n",
    "    raise ValueError(\n",
    "        f\"Dataset contains token id {ds_max} but model VOCAB={VOCAB}. \"\n",
    "        f\"Mismatch: did you set N_DIGITS to {ds_max-1} (PAD={ds_max-1}, SEP={ds_max})?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308163a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7dbed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- config helper ----------\n",
    "def attach_custom_mask(model):\n",
    "    def _mask(scores, hook=None):\n",
    "        # scores: (batch, heads, Q, K)\n",
    "        return scores + mask_bias.to(scores.device)\n",
    "    \n",
    "    # register the same mask hook on every layer\n",
    "    for block in model.blocks:\n",
    "        block.attn.hook_attn_scores.add_perma_hook(_mask, dir=\"fwd\")\n",
    "\n",
    "\n",
    "def strip_bias(m):\n",
    "    for mod in m.modules():\n",
    "        if hasattr(mod, \"bias\") and mod.bias is not None:\n",
    "            mod.bias.requires_grad_(False)\n",
    "            torch.nn.init.zeros_(mod.bias)\n",
    "            print(mod)\n",
    "\n",
    "    # remove biases from attention layers\n",
    "    attn_biases = ['b_Q', 'b_K', 'b_V', 'b_O']\n",
    "    for block in m.blocks:\n",
    "        for b in attn_biases:\n",
    "            mod = getattr(block.attn, b, None)\n",
    "            if mod is not None:\n",
    "                mod.requires_grad_(False)\n",
    "                torch.nn.init.zeros_(mod)\n",
    "\n",
    "    # remove unembed bias\n",
    "    if hasattr(m, \"unembed\") and m.b_U is not None:\n",
    "        m.unembed.b_U.requires_grad_(False)\n",
    "        torch.nn.init.zeros_(m.unembed.b_U)\n",
    "\n",
    "def set_WV_identity_and_freeze(model, d_model):\n",
    "    with torch.no_grad():\n",
    "        # Create a stack of identity-like matrices for W_V\n",
    "        # Each matrix is of shape (d_model, d_head)\n",
    "        # We take the first d_head columns of the d_model x d_model identity matrix\n",
    "        identity_slice = torch.eye(d_model, model.cfg.d_head)\n",
    "        # Repeat for each head\n",
    "        W_V_identity = identity_slice.unsqueeze(0).repeat(model.cfg.n_heads, 1, 1)\n",
    "        \n",
    "        for block in model.blocks:\n",
    "            block.attn.W_V.copy_(W_V_identity)\n",
    "            block.attn.W_V.requires_grad = False\n",
    "\n",
    "def set_WO_identity_and_freeze(model, d_model):\n",
    "    with torch.no_grad():\n",
    "        # Create a stack of identity-like matrices for W_O\n",
    "        # Each matrix is of shape (d_head, d_model)\n",
    "        # We take the first d_head rows of the d_model x d_model identity matrix\n",
    "        identity_slice = torch.eye(model.cfg.d_head, d_model)\n",
    "        # Repeat for each head\n",
    "        W_O_identity = identity_slice.unsqueeze(0).repeat(model.cfg.n_heads, 1, 1)\n",
    "\n",
    "        for block in model.blocks:\n",
    "            block.attn.W_O.copy_(W_O_identity)\n",
    "            block.attn.W_O.requires_grad = False\n",
    "\n",
    "\n",
    "def make_model(n_layers=N_LAYER, n_heads=N_HEAD, d_model=D_MODEL, ln=USE_LN, use_bias=USE_BIAS, freeze_wv=FREEZE_WV, freeze_wo=FREEZE_WO):\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers = n_layers,\n",
    "        n_heads = n_heads,\n",
    "        d_model = d_model,\n",
    "        d_head = d_model//n_heads,\n",
    "        n_ctx=SEQ_LEN,\n",
    "        d_vocab=VOCAB,\n",
    "        attn_only=True, # no MLP!\n",
    "        normalization_type=(\"LN\" if ln else None),\n",
    "    )\n",
    "    model = HookedTransformer(cfg).to(DEV)\n",
    "    if freeze_wv:\n",
    "        set_WV_identity_and_freeze(model, d_model)\n",
    "    if freeze_wo:\n",
    "        set_WO_identity_and_freeze(model, d_model)\n",
    "    if not use_bias:\n",
    "        strip_bias(model)\n",
    "    \n",
    "    attach_custom_mask(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e326dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Model saving / loading helpers ------\n",
    "def save_model(model, path = MODEL_PATH):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(path = MODEL_PATH, device = DEV):\n",
    "    print(\"Loading model from\", path)\n",
    "    model = make_model()\n",
    "    model.load_state_dict(\n",
    "        torch.load(path, map_location=device)\n",
    "    )  # map weights to target device\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "640a2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NEW: save an attention flow figure for a single example\n",
    "def save_attention_flow_figure(m, example_input, out_path, title=\"Attention Flow\", threshold=0.05):\n",
    "    m.eval()\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, cache = m.run_with_cache(example_input.to(DEV), return_type=\"logits\")\n",
    "\n",
    "    # Collect attention patterns per layer -> [L, Q, K]\n",
    "    att = (\n",
    "        torch.stack(\n",
    "            [cache[f\"blocks.{layer}.attn.hook_pattern\"] for layer in range(m.cfg.n_layers)],\n",
    "            dim=0,\n",
    "        )\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "        .squeeze()\n",
    "    )\n",
    "\n",
    "    # Residual stream (embed + post-resid after each layer)\n",
    "    resid_keys = [\"hook_embed\"] + [f\"blocks.{l}.hook_resid_post\" for l in range(m.cfg.n_layers)]\n",
    "    resid_values = torch.stack([cache[k] for k in resid_keys], dim=0)  # [L+1, 1, seq, d_model]\n",
    "\n",
    "    # Get W_U (compatibly)\n",
    "    W_U = getattr(m, \"W_U\", m.unembed.W_U)\n",
    "\n",
    "    # Logit lens: decode most likely token at each position after each layer\n",
    "    position_tokens = (resid_values @ W_U).squeeze(1).argmax(-1)  # [L+1, seq]\n",
    "\n",
    "    L, N, _ = att.shape\n",
    "    x_positions = np.arange(L + 1)  # input + after each layer\n",
    "    y_positions = np.arange(N)[::-1]  # top token = index 0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Draw nodes and token labels at each layer column\n",
    "    for lx in range(L + 1):\n",
    "        xs = np.full(N, lx)\n",
    "        ax.scatter(xs, y_positions, s=50)\n",
    "        for i, y in enumerate(y_positions):\n",
    "            ax.text(\n",
    "                lx + 0.03,\n",
    "                y + 0.03,\n",
    "                str(position_tokens[lx][i].item()),\n",
    "                fontsize=10,\n",
    "                va=\"bottom\",\n",
    "                ha=\"left\",\n",
    "            )\n",
    "\n",
    "    # Horizontal dashed arrows between all dots in the same row (residual stream)\n",
    "    for lx in range(L):\n",
    "        for y in y_positions:\n",
    "            ax.annotate(\n",
    "                \"\",\n",
    "                xy=(lx + 1, y),\n",
    "                xytext=(lx, y),\n",
    "                arrowprops=dict(\n",
    "                    arrowstyle=\"->\", lw=1.2, linestyle=\"--\", color=\"gray\", alpha=0.6\n",
    "                ),\n",
    "                zorder=0,\n",
    "            )\n",
    "\n",
    "    # Attention edges from layer l (keys) to layer l+1 (queries)\n",
    "    for l in range(L):\n",
    "        for q in range(N):\n",
    "            for k in range(N):\n",
    "                w = att[l, q, k]\n",
    "                if w <= threshold:\n",
    "                    continue\n",
    "                x0, y0 = l, y_positions[k]\n",
    "                x1, y1 = l + 1, y_positions[q]\n",
    "                ax.annotate(\n",
    "                    \"\",\n",
    "                    xy=(x1, y1),\n",
    "                    xytext=(x0, y0),\n",
    "                    arrowprops=dict(arrowstyle=\"->\", lw=1 + 4 * w, alpha=w),\n",
    "                )\n",
    "\n",
    "    # Labels and layout\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels([\"Input\"] + [f\"After L{l+1}\" for l in range(m.cfg.n_layers)])\n",
    "    ax.set_yticks(y_positions)\n",
    "    position_names = [\"d1\", \"d2\", \"SEP\", \"o1\", \"o2\"]\n",
    "    ax.set_yticklabels(position_names)\n",
    "    ax.set_xlim(-0.5, L + 0.5)\n",
    "    ax.set_ylim(-0.5, N - 0.5)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(False)\n",
    "    ax.set_aspect(\"auto\")\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], linestyle=\"--\", color=\"gray\", lw=1.5, label=\"Residual stream (dotted)\"),\n",
    "        Line2D([0], [0], linestyle=\"-\", color=\"black\", lw=1.5, label=\"Attention (solid)\"),\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=legend_elements,\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, -0.18),\n",
    "        frameon=False,\n",
    "        ncol=2,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95071794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- utilities ----------\n",
    "def accuracy(m):\n",
    "    m.eval()\n",
    "    hits = tots = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dl:\n",
    "            logits = m(inputs.to(DEV))[:, LIST_LEN+1:]  # (batch, 2, vocab)\n",
    "            preds = logits.argmax(-1)\n",
    "            hits += (preds == targets[:, LIST_LEN+1:].to(DEV)).sum().item()\n",
    "            tots += preds.numel()\n",
    "    return hits / tots\n",
    "\n",
    "\n",
    "# def train(m, max_steps=10_000, early_stop_acc=0.999, checkpoints=False, weight_decay=WEIGHT_DECAY, verbose=True):\n",
    "#     opt = torch.optim.AdamW(m.parameters(), 1e-3, weight_decay=weight_decay)\n",
    "#     ce = torch.nn.CrossEntropyLoss()\n",
    "#     dl = itertools.cycle(train_dl)  # infinite iterator\n",
    "#     for step in tqdm(range(max_steps), desc=\"Training\"):\n",
    "#         inputs, targets = next(dl)\n",
    "#         # get logits/loss for output tokens only\n",
    "#         logits = m(inputs.to(DEV))[:, LIST_LEN+1:].reshape(-1, VOCAB) \n",
    "#         loss = ce(logits, targets[:, LIST_LEN+1:].reshape(-1).to(DEV))\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         opt.zero_grad()\n",
    "#         if (step + 1) % 100 == 0:\n",
    "#             acc = accuracy(m)\n",
    "#             if acc >= early_stop_acc:\n",
    "#                 print(f\"Early stopping at step {step + 1} with accuracy {acc:.2%} >= {early_stop_acc:.2%}\")\n",
    "#                 break\n",
    "#             update_every = max(min(10_000, 0.05*max_steps), 1000)\n",
    "#             if verbose and (step+1) % update_every == 0:\n",
    "#                 print(f\"Step {step + 1}, Loss: {loss.item():.4f}, Accuracy: {acc:.2%}\")\n",
    "#             if checkpoints and (step+1) % 50_000 == 0:\n",
    "#                 save_model(m, MODEL_PATH)\n",
    "            \n",
    "#     print(f\"Final accuracy: {accuracy(m):.2%}\")\n",
    "\n",
    "\n",
    "def train(m, max_steps=10_000, early_stop_acc=0.999, checkpoints=False, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, verbose=True,\n",
    "          vis_every=None, vis_dir=None, vis_example_idx=0):\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr, weight_decay=weight_decay)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "    dl = itertools.cycle(train_dl)  # infinite iterator\n",
    "\n",
    "    # Setup visualization defaults\n",
    "    if vis_every is not None and vis_every > 0:\n",
    "        if vis_dir is None:\n",
    "            # Save under artifacts/attn_flow/<MODEL_NAME>/\n",
    "            vis_dir = os.path.join(\"artifacts\", \"attn_flow\", MODEL_NAME)\n",
    "        os.makedirs(vis_dir, exist_ok=True)\n",
    "        # Fixed example from validation set\n",
    "        example_input = val_ds.tensors[0][vis_example_idx].unsqueeze(0).to(DEV)\n",
    "\n",
    "        # Save an initial snapshot at step 0\n",
    "        save_attention_flow_figure(\n",
    "            m, example_input,\n",
    "            out_path=os.path.join(vis_dir, f\"step_000000.png\"),\n",
    "            title=f\"Attention Flow (Step 0)\"\n",
    "        )\n",
    "\n",
    "    last_saved_step = -1\n",
    "\n",
    "    for step in tqdm(range(max_steps), desc=\"Training\"):\n",
    "        inputs, targets = next(dl)\n",
    "        # get logits/loss for output tokens only\n",
    "        logits = m(inputs.to(DEV))[:, LIST_LEN+1:].reshape(-1, VOCAB) \n",
    "        loss = ce(logits, targets[:, LIST_LEN+1:].reshape(-1).to(DEV))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Periodic eval/log\n",
    "        if (step + 1) % 100 == 0:\n",
    "            acc = accuracy(m)\n",
    "            if acc >= early_stop_acc:\n",
    "                print(f\"Early stopping at step {step + 1} with accuracy {acc:.2%} >= {early_stop_acc:.2%}\")\n",
    "                # Save a final snapshot before breaking\n",
    "                if vis_every is not None and vis_every > 0:\n",
    "                    save_attention_flow_figure(\n",
    "                        m, example_input,\n",
    "                        out_path=os.path.join(vis_dir, f\"step_{step+1:06d}.png\"),\n",
    "                        title=f\"Attention Flow (Step {step+1})\"\n",
    "                    )\n",
    "                break\n",
    "            update_every = max(min(10_000, 0.05*max_steps), 1000)\n",
    "            if verbose and (step+1) % update_every == 0:\n",
    "                print(f\"Step {step + 1}, Loss: {loss.item():.4f}, Accuracy: {acc:.2%}\")\n",
    "            if checkpoints and (step+1) % 50_000 == 0:\n",
    "                save_model(m, MODEL_PATH)\n",
    "\n",
    "        # Periodic attention flow snapshots\n",
    "        if TRACK_ATTN_FLOW and vis_every is not None and vis_every > 0 and ((step + 1) % vis_every == 0):\n",
    "            save_attention_flow_figure(\n",
    "                m, example_input,\n",
    "                out_path=os.path.join(vis_dir, f\"step_{step+1:06d}.png\"),\n",
    "                title=f\"Attention Flow (Step {step+1})\"\n",
    "            )\n",
    "            last_saved_step = step + 1\n",
    "            \n",
    "    # Final accuracy print and final snapshot if not already saved at this step\n",
    "    print(f\"Final accuracy: {accuracy(m):.2%}\")\n",
    "    if vis_every is not None and vis_every > 0 and last_saved_step != max_steps:\n",
    "        # Save final snapshot at max_steps (or last step reached)\n",
    "        save_attention_flow_figure(\n",
    "            m, example_input,\n",
    "            out_path=os.path.join(vis_dir, f\"step_{min(last_saved_step, max_steps):06d}_final.png\"),\n",
    "            title=f\"Attention Flow (Final)\"\n",
    "        )\n",
    "# ...existing code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea857e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 37,   1, 101, 100, 100],\n",
       "         [ 51,  34, 101, 100, 100],\n",
       "         [ 51,  23, 101, 100, 100],\n",
       "         [ 55,   9, 101, 100, 100],\n",
       "         [ 72,  11, 101, 100, 100]]),\n",
       " tensor([[ 37,   1, 101,  37,   1],\n",
       "         [ 51,  34, 101,  51,  34],\n",
       "         [ 51,  23, 101,  51,  23],\n",
       "         [ 55,   9, 101,  55,   9],\n",
       "         [ 72,  11, 101,  72,  11]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check train set\n",
    "train_ds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training model: d128 ---\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d524781efdd747c2b04671fd10944d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500, Loss: 3.1113, Accuracy: 49.90%\n",
      "Step 5000, Loss: 2.1229, Accuracy: 50.83%\n",
      "Step 7500, Loss: 1.4310, Accuracy: 52.93%\n",
      "Step 10000, Loss: 1.0962, Accuracy: 53.36%\n",
      "Step 12500, Loss: 0.9009, Accuracy: 61.04%\n",
      "Step 15000, Loss: 0.7781, Accuracy: 64.90%\n",
      "Step 17500, Loss: 0.7198, Accuracy: 69.29%\n",
      "Step 20000, Loss: 0.6778, Accuracy: 71.67%\n",
      "Step 22500, Loss: 0.6985, Accuracy: 72.68%\n",
      "Step 25000, Loss: 0.6510, Accuracy: 72.60%\n",
      "Step 27500, Loss: 0.6277, Accuracy: 73.23%\n",
      "Step 30000, Loss: 0.6621, Accuracy: 73.11%\n",
      "Step 32500, Loss: 0.6400, Accuracy: 73.13%\n",
      "Step 35000, Loss: 0.6074, Accuracy: 73.46%\n",
      "Step 37500, Loss: 0.6348, Accuracy: 73.43%\n",
      "Step 40000, Loss: 0.5950, Accuracy: 73.46%\n",
      "Step 42500, Loss: 0.5994, Accuracy: 73.71%\n",
      "Step 45000, Loss: 0.5982, Accuracy: 73.91%\n",
      "Step 47500, Loss: 0.5727, Accuracy: 73.89%\n",
      "Step 50000, Loss: 0.5549, Accuracy: 73.81%\n",
      "Final accuracy: 73.81%\n",
      "| name   |   n_layers |   n_heads |   d_model | ln    | bias   | freeze_wv   | freeze_wo   |   weight_decay |   val_acc |\n",
      "|:-------|-----------:|----------:|----------:|:------|:-------|:------------|:------------|---------------:|----------:|\n",
      "| d128   |          2 |         1 |       128 | False | False  | True        | True        |              1 |    0.7381 |\n"
     ]
    }
   ],
   "source": [
    "# ---------- experiment grid ----------\n",
    "def make_name(d_model, n_layers, ln, use_bias, freeze_wv, freeze_wo):\n",
    "    parts = [\n",
    "        f\"d{d_model}\",\n",
    "        f\"{n_layers}L\",\n",
    "        (\"LN\" if ln else \"noLN\"),\n",
    "        (\"Bias\" if use_bias else \"noBias\"),\n",
    "        (\"fWV\" if freeze_wv else \"uWV\"), # freeze / unfreeze\n",
    "        (\"fWO\" if freeze_wo else \"uWO\"),\n",
    "    ]\n",
    "    return \"_\".join(parts)\n",
    "\n",
    "specs = [\n",
    "    # {'name': 'd256', 'd_model': 256},\n",
    "    # {'name': 'd128', 'd_model': 128, 'weight_decay': 1.0},\n",
    "    # {'name': 'd64', 'd_model': 64},\n",
    "    \n",
    "    # {'name': 'd32', 'd_model': 32},\n",
    "    # {'name': 'd32_ln_bias', 'd_model': 32, 'ln': True, 'use_bias': True},\n",
    "    # {'name': 'd32_noLN', 'd_model': 32, 'ln': False, 'use_bias': True},\n",
    "    # {'name': 'd32_noBias', 'd_model': 32, 'ln': True, 'use_bias': False},\n",
    "    # {'name': 'd32_noLNnoBias', 'd_model': 32, 'ln': False, 'use_bias': False},\n",
    "    # {'name': 'd32_fwo', 'd_model': 32, 'freeze_wo': True},\n",
    "    # {'name': 'd32_unfwo', 'd_model': 32, 'freeze_wo': False},\n",
    "\n",
    "    # {'name': 'd16', 'd_model': 16},\n",
    "    # {'name': 'd16_ln_bias', 'd_model': 16, 'ln': True, 'use_bias': True},\n",
    "    # {'name': 'd16_noLN', 'd_model': 16, 'ln': False, 'use_bias': True},\n",
    "    # {'name': 'd16_noBias', 'd_model': 16, 'ln': True, 'use_bias': False},\n",
    "    # {'name': 'd16_noLNnoBias', 'd_model': 16, 'ln': False, 'use_bias': False},\n",
    "    # {'name': 'd16_fwo', 'd_model': 16, 'freeze_wo': True},\n",
    "    # {'name': 'd16_unfwo', 'd_model': 16, 'freeze_wo': False},\n",
    "\n",
    "    # {'name': 'd8', 'd_model': 8},\n",
    "    # {'name': 'd8_ln_bias', 'd_model': 8, 'ln': True, 'use_bias': True},\n",
    "    # {'name': 'd8_noLN', 'd_model': 8, 'ln': False, 'use_bias': True},\n",
    "    # {'name': 'd8_noBias', 'd_model': 8, 'ln': True, 'use_bias': False},\n",
    "    # {'name': 'd8_noLNnoBias', 'd_model': 8, 'ln': False, 'use_bias': False},\n",
    "    # {'name': 'd8_fwo', 'd_model': 8, 'freeze_wo': True},\n",
    "    # {'name': 'd8_unfwo', 'd_model': 8, 'freeze_wo': False},\n",
    "\n",
    "    # {'name': 'd4', 'd_model': 4},\n",
    "]\n",
    "\n",
    "from itertools import product\n",
    "# specs = []\n",
    "# d_model = 128\n",
    "# for n_layers, ln, use_bias, freeze_wv, freeze_wo in product(\n",
    "#     [2, 3],            # layers\n",
    "#     [False, True],     # ln\n",
    "#     [False, True],     # use_bias\n",
    "#     [False, True],     # freeze_wv\n",
    "#     [False, True],     # freeze_wo\n",
    "# ):\n",
    "#     specs.append({\n",
    "#         \"name\": make_name(d_model, n_layers, ln, use_bias, freeze_wv, freeze_wo),\n",
    "#         \"d_model\": d_model,\n",
    "#         \"n_layers\": n_layers,\n",
    "#         \"ln\": ln,\n",
    "#         \"use_bias\": use_bias,\n",
    "#         \"freeze_wv\": freeze_wv,\n",
    "#         \"freeze_wo\": freeze_wo,\n",
    "#     })\n",
    "\n",
    "# -----------------------\n",
    "rows = []\n",
    "for spec in specs:\n",
    "    # Create a full spec by starting with defaults and updating with the current spec\n",
    "    full_spec = {\n",
    "        'n_layers': N_LAYER,\n",
    "        'n_heads': N_HEAD,\n",
    "        'd_model': D_MODEL,\n",
    "        'ln': USE_LN,\n",
    "        'bias': USE_BIAS,\n",
    "        'freeze_wv': FREEZE_WV,\n",
    "        'freeze_wo': FREEZE_WO,\n",
    "        'lr': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "    }\n",
    "    full_spec.update(spec) # Overwrite defaults with provided spec values\n",
    "\n",
    "    print(f\"--- Training model: {full_spec['name']} ---\")\n",
    "    model = make_model(\n",
    "        n_layers=full_spec['n_layers'],\n",
    "        n_heads=full_spec['n_heads'],\n",
    "        d_model=full_spec['d_model'], \n",
    "        ln=full_spec['ln'],\n",
    "        use_bias=full_spec['bias'],\n",
    "        freeze_wv=full_spec['freeze_wv'],\n",
    "        freeze_wo=full_spec['freeze_wo'],\n",
    "    )\n",
    "\n",
    "    train(model, max_steps=50_000, lr=full_spec['lr'], weight_decay=full_spec['weight_decay'], verbose=True)\n",
    "    \n",
    "    # Add all spec parameters to the results\n",
    "    result = full_spec.copy()\n",
    "    result['val_acc'] = round(accuracy(model), 4)\n",
    "    rows.append(result)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Move 'name' column to the front for better readability\n",
    "if 'name' in df.columns:\n",
    "    cols = ['name'] + [col for col in df.columns if col != 'name']\n",
    "    df = df[cols]\n",
    "\n",
    "print(df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d7ce1",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "\n",
    "| name                        |   n_layers |   n_heads |   d_model | ln    | use_bias   | freeze_wv   | freeze_wo   |   weight_decay |   val_acc |\n",
    "|:----------------------------|-----------:|----------:|----------:|:------|:-----------|:------------|:------------|---------------:|----------:|\n",
    "| d128_2L_noLN_noBias_uWV_uWO |          2 |         1 |       128 | False | False      | False       | False       |           0.01 |    0.4625 |\n",
    "| d128_2L_noLN_noBias_uWV_fWO |          2 |         1 |       128 | False | False      | False       | True        |           0.01 |    0.4895 |\n",
    "| d128_2L_noLN_noBias_fWV_uWO |          2 |         1 |       128 | False | False      | True        | False       |           0.01 |    0.463  |\n",
    "| d128_2L_noLN_noBias_fWV_fWO |          2 |         1 |       128 | False | False      | True        | True        |           0.01 |    0.9173 |\n",
    "| d128_2L_noLN_Bias_uWV_uWO   |          2 |         1 |       128 | False | True       | False       | False       |           0.01 |    0.868  |\n",
    "| d128_2L_noLN_Bias_uWV_fWO   |          2 |         1 |       128 | False | True       | False       | True        |           0.01 |    0.8945 |\n",
    "| d128_2L_noLN_Bias_fWV_uWO   |          2 |         1 |       128 | False | True       | True        | False       |           0.01 |    0.4645 |\n",
    "| d128_2L_noLN_Bias_fWV_fWO   |          2 |         1 |       128 | False | True       | True        | True        |           0.01 |    0.9183 |\n",
    "| d128_2L_LN_noBias_uWV_uWO   |          2 |         1 |       128 | True  | False      | False       | False       |           0.01 |    0.4743 |\n",
    "| d128_2L_LN_noBias_uWV_fWO   |          2 |         1 |       128 | True  | False      | False       | True        |           0.01 |    0.4607 |\n",
    "| d128_2L_LN_noBias_fWV_uWO   |          2 |         1 |       128 | True  | False      | True        | False       |           0.01 |    0.4632 |\n",
    "| d128_2L_LN_noBias_fWV_fWO   |          2 |         1 |       128 | True  | False      | True        | True        |           0.01 |    0.4485 |\n",
    "| d128_2L_LN_Bias_uWV_uWO     |          2 |         1 |       128 | True  | True       | False       | False       |           0.01 |    0.4733 |\n",
    "| d128_2L_LN_Bias_uWV_fWO     |          2 |         1 |       128 | True  | True       | False       | True        |           0.01 |    0.4647 |\n",
    "| d128_2L_LN_Bias_fWV_uWO     |          2 |         1 |       128 | True  | True       | True        | False       |           0.01 |    0.4755 |\n",
    "| d128_2L_LN_Bias_fWV_fWO     |          2 |         1 |       128 | True  | True       | True        | True        |           0.01 |    0.4602 |\n",
    "\n",
    "| name   |   n_layers |   n_heads |   d_model | ln    | use_bias   | freeze_wv   | freeze_wo   |   weight_decay |   val_acc |\n",
    "|:-------|-----------:|----------:|----------:|:------|:-----------|:------------|:------------|---------------:|----------:|\n",
    "| d256   |          2 |         1 |       256 | False | False  | True        | True        |           0.01 |    0.8697 |\n",
    "| d128   |          2 |         1 |       128 | False | False      | True        | True        |           0.01 |    0.9038 |\n",
    "| d64    |          2 |         1 |        64 | False | False      | True        | True        |           0.01 |    0.6836 |\n",
    "| d32    |          2 |         1 |        32 | False | False      | True        | True        |           0.01 |    0.4278 |\n",
    "| d16    |          2 |         1 |        16 | False | False      | True        | True        |           0.01 |    0.4497 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0e08dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93666e7420ad43f3a09b0f2c8b9d34d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10000, Loss: 0.5483, Accuracy: 50.15%\n",
      "Step 20000, Loss: 0.1489, Accuracy: 86.16%\n",
      "Step 30000, Loss: 0.1406, Accuracy: 88.69%\n",
      "Step 40000, Loss: 0.0804, Accuracy: 90.28%\n",
      "Step 50000, Loss: 0.0858, Accuracy: 90.76%\n",
      "Model saved to models/v3_2layer_100dig_128d.pt\n",
      "Step 60000, Loss: 0.0618, Accuracy: 91.82%\n",
      "Step 70000, Loss: 0.0557, Accuracy: 90.86%\n",
      "Step 80000, Loss: 0.0545, Accuracy: 90.40%\n",
      "Step 90000, Loss: 0.0436, Accuracy: 91.16%\n",
      "Step 100000, Loss: 0.0837, Accuracy: 91.54%\n",
      "Model saved to models/v3_2layer_100dig_128d.pt\n",
      "Step 110000, Loss: 0.0418, Accuracy: 91.67%\n",
      "Step 120000, Loss: 0.0439, Accuracy: 91.67%\n",
      "Step 130000, Loss: 0.0460, Accuracy: 91.41%\n",
      "Step 140000, Loss: 0.0667, Accuracy: 91.52%\n",
      "Step 150000, Loss: 0.0682, Accuracy: 91.59%\n",
      "Model saved to models/v3_2layer_100dig_128d.pt\n",
      "Step 160000, Loss: 0.0558, Accuracy: 92.07%\n",
      "Step 170000, Loss: 0.0556, Accuracy: 91.64%\n",
      "Step 180000, Loss: 0.0585, Accuracy: 91.72%\n",
      "Step 190000, Loss: 0.0596, Accuracy: 91.64%\n",
      "Step 200000, Loss: 0.0381, Accuracy: 92.17%\n",
      "Model saved to models/v3_2layer_100dig_128d.pt\n",
      "Step 210000, Loss: 0.0636, Accuracy: 91.34%\n",
      "Step 220000, Loss: 0.0580, Accuracy: 91.11%\n",
      "Step 230000, Loss: 0.0624, Accuracy: 91.57%\n",
      "Step 240000, Loss: 0.0529, Accuracy: 91.09%\n",
      "Step 250000, Loss: 0.0539, Accuracy: 91.41%\n",
      "Model saved to models/v3_2layer_100dig_128d.pt\n",
      "Step 260000, Loss: 0.0546, Accuracy: 91.52%\n",
      "Step 270000, Loss: 0.0425, Accuracy: 91.19%\n",
      "Step 280000, Loss: 0.0583, Accuracy: 91.46%\n",
      "Step 290000, Loss: 0.0327, Accuracy: 91.72%\n",
      "Step 300000, Loss: 0.0694, Accuracy: 91.39%\n",
      "Model saved to models/v3_2layer_100dig_128d.pt\n",
      "Step 310000, Loss: 0.0503, Accuracy: 91.67%\n",
      "Step 320000, Loss: 0.0389, Accuracy: 91.69%\n",
      "Step 330000, Loss: 0.0393, Accuracy: 91.72%\n",
      "Step 340000, Loss: 0.0755, Accuracy: 91.34%\n",
      "Step 350000, Loss: 0.0514, Accuracy: 91.24%\n",
      "Model saved to models/v3_2layer_100dig_128d.pt\n",
      "Step 360000, Loss: 0.0526, Accuracy: 91.24%\n",
      "Step 370000, Loss: 0.0609, Accuracy: 91.41%\n",
      "Step 380000, Loss: 0.0477, Accuracy: 91.54%\n",
      "Step 390000, Loss: 0.0445, Accuracy: 91.34%\n",
      "Step 400000, Loss: 0.0404, Accuracy: 91.41%\n",
      "Model saved to models/v3_2layer_100dig_128d.pt\n",
      "Step 410000, Loss: 0.0674, Accuracy: 91.21%\n",
      "Step 420000, Loss: 0.0410, Accuracy: 91.01%\n",
      "Step 430000, Loss: 0.0368, Accuracy: 91.21%\n",
      "Step 440000, Loss: 0.0444, Accuracy: 91.57%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     model = make_model()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_TRAIN_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_acc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m          \u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_CHECKPOINTING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvis_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvis_example_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 64,8\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m          \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     save_model(model, MODEL_PATH)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# from torchinfo import summary\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# summary(model) \u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(m, max_steps, early_stop_acc, checkpoints, weight_decay, verbose, vis_every, vis_dir, vis_example_idx)\u001b[39m\n\u001b[32m     65\u001b[39m inputs, targets = \u001b[38;5;28mnext\u001b[39m(dl)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# get logits/loss for output tokens only\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m logits = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[:, LIST_LEN+\u001b[32m1\u001b[39m:].reshape(-\u001b[32m1\u001b[39m, VOCAB) \n\u001b[32m     68\u001b[39m loss = ce(logits, targets[:, LIST_LEN+\u001b[32m1\u001b[39m:].reshape(-\u001b[32m1\u001b[39m).to(DEV))\n\u001b[32m     69\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:620\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    616\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    617\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    618\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     residual = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    630\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/transformer_lens/components/transformer_block.py:160\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    153\u001b[39m     key_input = attn_in\n\u001b[32m    154\u001b[39m     value_input = attn_in\n\u001b[32m    156\u001b[39m attn_out = (\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[32m    174\u001b[39m     attn_out = \u001b[38;5;28mself\u001b[39m.ln1_post(attn_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/transformer_lens/components/abstract_attention.py:302\u001b[39m, in \u001b[36mAbstractAttention.forward\u001b[39m\u001b[34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[39m\n\u001b[32m    299\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.b_O.device != z.device:\n\u001b[32m    300\u001b[39m             z = z.to(\u001b[38;5;28mself\u001b[39m.b_O.device)\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m         out = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m            \u001b[49m\u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43md_head\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m            \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mb_O\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    308\u001b[39m     \u001b[38;5;66;03m# Explicitly calculate the attention result so it can be accessed by a hook\u001b[39;00m\n\u001b[32m    309\u001b[39m     \u001b[38;5;66;03m# This is off by default because it can easily eat through your GPU memory.\u001b[39;00m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.load_in_4bit:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# LOAD existing or train and SAVE new model\n",
    "load_existing = True  # Set to False to always train a new model\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and load_existing:\n",
    "    model = load_model(MODEL_PATH, device=DEV)\n",
    "else:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        MODEL_PATH = MODEL_PATH.replace(\".pt\", \"_new.pt\")\n",
    "        print(f\"Model path already exists. Saving new model to {MODEL_PATH}\")\n",
    "    print(\"Training model\")\n",
    "    model = make_model()\n",
    "    train(model, max_steps=MAX_TRAIN_STEPS, early_stop_acc=0.999, \n",
    "          checkpoints=USE_CHECKPOINTING, vis_every=10000, vis_example_idx=0\n",
    "          )\n",
    "    save_model(model, MODEL_PATH)\n",
    "\n",
    "# from torchinfo import summary\n",
    "# summary(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Overview of Model Parameters ---\n",
      "Parameter Name                           | Shape                | Trainable \n",
      "--------------------------------------------------------------------------------\n",
      "embed.W_E                                | (102, 128)           | Yes       \n",
      "pos_embed.W_pos                          | (5, 128)             | Yes       \n",
      "blocks.0.attn.W_Q                        | (1, 128, 128)        | Yes       \n",
      "blocks.0.attn.W_K                        | (1, 128, 128)        | Yes       \n",
      "blocks.1.attn.W_Q                        | (1, 128, 128)        | Yes       \n",
      "blocks.1.attn.W_K                        | (1, 128, 128)        | Yes       \n",
      "unembed.W_U                              | (128, 102)           | Yes       \n",
      "--------------------------------------------------------------------------------\n",
      "Total parameters: 158950\n",
      "Trainable parameters: 92288\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Model Parameters Overview ---\n",
    "print(\"--- Overview of Model Parameters ---\")   \n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "\n",
    "# Use a formatted string for better alignment\n",
    "print(f\"{'Parameter Name':<40} | {'Shape':<20} | {'Trainable':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    shape_str = str(tuple(param.shape))\n",
    "    is_trainable = \"Yes\" if param.requires_grad else \"No\"\n",
    "    total_params += param.numel()\n",
    "\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    # Print only trainable parameters\n",
    "    print(f\"{name:<40} | {shape_str:<20} | {is_trainable:<10}\")\n",
    "    trainable_params += param.numel()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57f82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
