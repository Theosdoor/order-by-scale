{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52fcc20",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2de5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from datetime import datetime # for unique model naming\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "import umap\n",
    "\n",
    "import einops\n",
    "import pandas as pd, itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n",
    "\n",
    "from data import get_dataset\n",
    "\n",
    "# Configure plotly to use static rendering if widgets fail\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "float_formatter = \"{:.5f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15f0d1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1437fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x74c06c518710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- parameters ----------\n",
    "LIST_LEN = 2 # [d1, d2]\n",
    "SEQ_LEN = LIST_LEN * 2 + 1 # [d1, d2, SEP, o1, o2]\n",
    "\n",
    "N_DIGITS = 100\n",
    "DIGITS = list(range(N_DIGITS)) # 100 digits from 0 to 99\n",
    "MASK = N_DIGITS # special masking token for o1 and o2\n",
    "SEP = N_DIGITS + 1 # special seperator token for the model to think about the input (+1 to avoid confusion with the last digit)\n",
    "VOCAB = len(DIGITS) + 2  # + the special tokens\n",
    "\n",
    "D_MODEL = 64\n",
    "N_HEAD = 1\n",
    "N_LAYER = 2\n",
    "USE_LN = False # use layer norm in model\n",
    "USE_BIAS = False # use bias in model\n",
    "FREEZE_WV = True # no value matrix in attn \n",
    "FREEZE_WO = True # no output matrix in attn (i.e. attn head can only copy inputs to outputs)\n",
    "\n",
    "LEARNING_RATE = 1e-3 # default 1e-3\n",
    "WEIGHT_DECAY = 0.01 # default 0.01\n",
    "MAX_TRAIN_STEPS = 50_000 # max training steps\n",
    "USE_CHECKPOINTING = False # whether to use checkpointing for training\n",
    "\n",
    "RUN_TS = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_NAME = f'{N_LAYER}layer_{N_DIGITS}dig_{D_MODEL}d_{RUN_TS}'\n",
    "# MODEL_NAME = \n",
    "MODEL_PATH = \"models/\" + MODEL_NAME + \".pt\"\n",
    "\n",
    "# --- dataset --- (not necessary as we fix seed?)\n",
    "# DATASET_NAME = None # None ==> generate new one\n",
    "# listlen2_digits10_dupes\n",
    "# listlen2_digits10_nodupes\n",
    "# listlen2_digits100_dupes_traindupesonly\n",
    "# listlen2_digits100_dupes\n",
    "# listlen2_digits100_nodupes\n",
    "\n",
    "DEV = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316797fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [-inf, -inf, 0., -inf, -inf],\n",
      "        [-inf, -inf, 0., 0., -inf]])\n"
     ]
    }
   ],
   "source": [
    "# ---------- mask ----------\n",
    "# attention mask for [d1, d2, SEP, o1, o2] looks like this:\n",
    "# -    d1    d2    SEP    o1    o2   (keys)\n",
    "# d1  -inf  -inf   -inf  -inf  -inf\n",
    "# d2   0    -inf   -inf  -inf  -inf\n",
    "# SEP  0      0    -inf  -inf  -inf\n",
    "# o1  -inf  -inf    0    -inf   -inf\n",
    "# o2  -inf  -inf    0      0    -inf\n",
    "# (queries)\n",
    "\n",
    "mask_bias = torch.triu(torch.ones(SEQ_LEN, SEQ_LEN) * float(\"-inf\")) # upper triangular bias mask (lead_diag & above = -inf, rest = 0)\n",
    "mask_bias[0, 0] = 0. # don't want a full row of -inf! otherwise we get nan erros & training breaks\n",
    "mask_bias[LIST_LEN+1:, :LIST_LEN] = float(\"-inf\") # stop output tokens from attending to input tokens\n",
    "mask_bias = mask_bias.unsqueeze(0).unsqueeze(0) # (1,1,T,T) broadcastable across batch and heads\n",
    "\n",
    "# L0: keep outputs self-only and allow SEP->digits; avoid all -inf rows\n",
    "mask_bias_l0 = mask_bias.clone()\n",
    "mask_bias_l0[..., LIST_LEN+1:, :] = float(\"-inf\") # block all for outputs\n",
    "idx = torch.arange(LIST_LEN+1, SEQ_LEN)  # re-enable self for outputs\n",
    "mask_bias_l0[..., idx, idx] = 0.0\n",
    "\n",
    "print(mask_bias.cpu()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60abc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([ 60,  44, 101, 100, 100])\n",
      "Target: tensor([ 60,  44, 101,  60,  44])\n",
      "Train dataset size: 8000, Validation dataset size: 2000\n"
     ]
    }
   ],
   "source": [
    "# ---------- dataset ----------\n",
    "train_ds, val_ds = get_dataset(\n",
    "    list_len=LIST_LEN, \n",
    "    n_digits=N_DIGITS, \n",
    "    train_split=0.8,\n",
    "    mask_tok=MASK, # use MASK as mask token\n",
    "    sep_tok=SEP, # use SEP as separator token\n",
    "    )\n",
    "\n",
    "train_batch_size = min(128, len(train_ds))\n",
    "val_batch_size = min(256, len(val_ds))\n",
    "train_dl = DataLoader(train_ds, train_batch_size, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, val_batch_size, drop_last=False)\n",
    "\n",
    "print(\"Input:\", train_ds[0][0])\n",
    "print(\"Target:\", train_ds[0][1])\n",
    "print(f\"Train dataset size: {len(train_ds)}, Validation dataset size: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7dbed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- config helper ----------\n",
    "def attach_custom_mask(model):\n",
    "    def _mask(scores, hook=None):\n",
    "        # scores: (batch, heads, Q, K)\n",
    "        return scores + mask_bias.to(scores.device)\n",
    "    \n",
    "    def _mask_l0(scores, hook=None):\n",
    "        # layer-0 special mask: o1/o2 only self; SEP can read d1/d2\n",
    "        return scores + mask_bias_l0.to(scores.device)\n",
    "    \n",
    "    # Completely suppress attention for oi in L0 (safe: zero pattern rows, not -inf scores)\n",
    "    def _zero_o_rows(pattern, hook=None):\n",
    "        # pattern: [B, H, Q, K]\n",
    "        start_o = LIST_LEN+1 # first o_i index\n",
    "        if start_o < SEQ_LEN:\n",
    "            pattern = pattern.clone()\n",
    "            pattern[..., start_o:SEQ_LEN, :] = 0.0\n",
    "        return pattern\n",
    "    \n",
    "    # register the same mask hook on every layer\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        if i == 0:\n",
    "            block.attn.hook_attn_scores.add_perma_hook(_mask_l0, dir=\"fwd\")\n",
    "            block.attn.hook_pattern.add_perma_hook(_zero_o_rows, dir=\"fwd\")\n",
    "        else:\n",
    "            block.attn.hook_attn_scores.add_perma_hook(_mask, dir=\"fwd\")\n",
    "\n",
    "\n",
    "def strip_bias(m):\n",
    "    for mod in m.modules():\n",
    "        if hasattr(mod, \"bias\") and mod.bias is not None:\n",
    "            mod.bias.requires_grad_(False)\n",
    "            torch.nn.init.zeros_(mod.bias)\n",
    "\n",
    "    # remove biases from attention layers\n",
    "    attn_biases = ['b_Q', 'b_K', 'b_V', 'b_O']\n",
    "    for block in m.blocks:\n",
    "        for b in attn_biases:\n",
    "            mod = getattr(block.attn, b, None)\n",
    "            if mod is not None:\n",
    "                mod.requires_grad_(False)\n",
    "                torch.nn.init.zeros_(mod)\n",
    "\n",
    "    # remove unembed bias\n",
    "    b_U = getattr(m, \"b_U\", None)\n",
    "    if b_U is None and hasattr(m, \"unembed\"):\n",
    "        b_U = getattr(m.unembed, \"b_U\", None)\n",
    "    if b_U is not None:\n",
    "        b_U.requires_grad_(False)\n",
    "        torch.nn.init.zeros_(b_U)\n",
    "\n",
    "def set_WV_identity_and_freeze(model, d_model):\n",
    "    with torch.no_grad():\n",
    "        # Create a stack of identity-like matrices for W_V\n",
    "        # Each matrix is of shape (d_model, d_head)\n",
    "        # We take the first d_head columns of the d_model x d_model identity matrix\n",
    "        identity_slice = torch.eye(d_model, model.cfg.d_head)\n",
    "        # Repeat for each head\n",
    "        W_V_identity = identity_slice.unsqueeze(0).repeat(model.cfg.n_heads, 1, 1)\n",
    "        \n",
    "        for block in model.blocks:\n",
    "            block.attn.W_V.copy_(W_V_identity)\n",
    "            block.attn.W_V.requires_grad = False\n",
    "\n",
    "def set_WO_identity_and_freeze(model, d_model):\n",
    "    with torch.no_grad():\n",
    "        # Create a stack of identity-like matrices for W_O\n",
    "        # Each matrix is of shape (d_head, d_model)\n",
    "        # We take the first d_head rows of the d_model x d_model identity matrix\n",
    "        identity_slice = torch.eye(model.cfg.d_head, d_model)\n",
    "        # Repeat for each head\n",
    "        W_O_identity = identity_slice.unsqueeze(0).repeat(model.cfg.n_heads, 1, 1)\n",
    "\n",
    "        for block in model.blocks:\n",
    "            block.attn.W_O.copy_(W_O_identity)\n",
    "            block.attn.W_O.requires_grad = False\n",
    "\n",
    "\n",
    "def make_model(n_layers=N_LAYER, n_heads=N_HEAD, d_model=D_MODEL, ln=USE_LN, use_bias=USE_BIAS, freeze_wv=FREEZE_WV, freeze_wo=FREEZE_WO):\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers = n_layers,\n",
    "        n_heads = n_heads,\n",
    "        d_model = d_model,\n",
    "        d_head = d_model//n_heads,\n",
    "        n_ctx=SEQ_LEN,\n",
    "        d_vocab=VOCAB,\n",
    "        attn_only=True, # no MLP!\n",
    "        normalization_type=(\"LN\" if ln else None),\n",
    "    )\n",
    "    model = HookedTransformer(cfg).to(DEV)\n",
    "    if freeze_wv:\n",
    "        set_WV_identity_and_freeze(model, d_model)\n",
    "    if freeze_wo:\n",
    "        set_WO_identity_and_freeze(model, d_model)\n",
    "    if not use_bias:\n",
    "        strip_bias(model)\n",
    "    \n",
    "    attach_custom_mask(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e326dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Model saving / loading helpers ------\n",
    "def save_model(model, path = MODEL_PATH):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(path = MODEL_PATH, device = DEV):\n",
    "    print(\"Loading model from\", path)\n",
    "    model = make_model()\n",
    "    model.load_state_dict(\n",
    "        torch.load(path, map_location=device)\n",
    "    )  # map weights to target device\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95071794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- utilities ----------\n",
    "def accuracy(m):\n",
    "    m.eval()\n",
    "    hits = tots = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dl:\n",
    "            logits = m(inputs.to(DEV))[:, LIST_LEN+1:]  # (batch, 2, vocab)\n",
    "            preds = logits.argmax(-1)\n",
    "            hits += (preds == targets[:, LIST_LEN+1:].to(DEV)).sum().item()\n",
    "            tots += preds.numel()\n",
    "    return hits / tots\n",
    "\n",
    "\n",
    "def train(m, max_steps=10_000, early_stop_acc=0.999, checkpoints=False, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, verbose=True):\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr, weight_decay=weight_decay)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "    dl = itertools.cycle(train_dl)  # infinite iterator\n",
    "    for step in tqdm(range(max_steps), desc=\"Training\"):\n",
    "        inputs, targets = next(dl)\n",
    "        # get logits/loss for output tokens only\n",
    "        logits = m(inputs.to(DEV))[:, LIST_LEN+1:].reshape(-1, VOCAB) \n",
    "        loss = ce(logits, targets[:, LIST_LEN+1:].reshape(-1).to(DEV))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        if (step + 1) % 100 == 0:\n",
    "            acc = accuracy(m)\n",
    "            if acc > early_stop_acc:\n",
    "                print(f\"Early stopping at step {step + 1} with accuracy {acc:.2%} >= {early_stop_acc:.2%}\")\n",
    "                break\n",
    "            update_every = max(min(10_000, max_steps//20), 1000)\n",
    "            if verbose and (step+1) % update_every == 0:\n",
    "                print(f\"Step {step + 1}, Loss: {loss.item():.4f}, Accuracy: {acc:.2%}\")\n",
    "            if checkpoints and (step+1) % 50_000 == 0:\n",
    "                save_model(m, MODEL_PATH)\n",
    "            \n",
    "    print(f\"Final accuracy: {accuracy(m):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea857e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 60,  44, 101, 100, 100],\n",
       "         [ 28,  90, 101, 100, 100],\n",
       "         [ 93,  99, 101, 100, 100],\n",
       "         [ 19,  17, 101, 100, 100],\n",
       "         [ 49,  19, 101, 100, 100]]),\n",
       " tensor([[ 60,  44, 101,  60,  44],\n",
       "         [ 28,  90, 101,  28,  90],\n",
       "         [ 93,  99, 101,  93,  99],\n",
       "         [ 19,  17, 101,  19,  17],\n",
       "         [ 49,  19, 101,  49,  19]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check train set\n",
    "train_ds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a29f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- experiment grid ----------\n",
    "def make_name(d_model, n_layers, ln, use_bias, freeze_wv, freeze_wo):\n",
    "    parts = [\n",
    "        f\"d{d_model}\",\n",
    "        f\"{n_layers}L\",\n",
    "        (\"LN\" if ln else \"noLN\"),\n",
    "        (\"Bias\" if use_bias else \"noBias\"),\n",
    "        (\"fWV\" if freeze_wv else \"uWV\"), # freeze / unfreeze\n",
    "        (\"fWO\" if freeze_wo else \"uWO\"),\n",
    "    ]\n",
    "    return \"_\".join(parts)\n",
    "\n",
    "specs = [\n",
    "    # {'name': 'd256', 'd_model': 256},\n",
    "    # {'name': 'd128', 'd_model': 128, 'weight_decay': 1.0},\n",
    "    # {'name': 'd64', 'd_model': 64},\n",
    "    \n",
    "    # {'name': 'd32', 'd_model': 32},\n",
    "    # {'name': 'd32_ln_bias', 'd_model': 32, 'ln': True, 'use_bias': True},\n",
    "    # {'name': 'd32_noLN', 'd_model': 32, 'ln': False, 'use_bias': True},\n",
    "    # {'name': 'd32_noBias', 'd_model': 32, 'ln': True, 'use_bias': False},\n",
    "    # {'name': 'd32_noLNnoBias', 'd_model': 32, 'ln': False, 'use_bias': False},\n",
    "    # {'name': 'd32_fwo', 'd_model': 32, 'freeze_wo': True},\n",
    "    # {'name': 'd32_unfwo', 'd_model': 32, 'freeze_wo': False},\n",
    "\n",
    "    # {'name': 'd16', 'd_model': 16},\n",
    "    # {'name': 'd16_ln_bias', 'd_model': 16, 'ln': True, 'use_bias': True},\n",
    "    # {'name': 'd16_noLN', 'd_model': 16, 'ln': False, 'use_bias': True},\n",
    "    # {'name': 'd16_noBias', 'd_model': 16, 'ln': True, 'use_bias': False},\n",
    "    # {'name': 'd16_noLNnoBias', 'd_model': 16, 'ln': False, 'use_bias': False},\n",
    "    # {'name': 'd16_fwo', 'd_model': 16, 'freeze_wo': True},\n",
    "    # {'name': 'd16_unfwo', 'd_model': 16, 'freeze_wo': False},\n",
    "\n",
    "    # {'name': 'd8', 'd_model': 8},\n",
    "    # {'name': 'd8_ln_bias', 'd_model': 8, 'ln': True, 'use_bias': True},\n",
    "    # {'name': 'd8_noLN', 'd_model': 8, 'ln': False, 'use_bias': True},\n",
    "    # {'name': 'd8_noBias', 'd_model': 8, 'ln': True, 'use_bias': False},\n",
    "    # {'name': 'd8_noLNnoBias', 'd_model': 8, 'ln': False, 'use_bias': False},\n",
    "    # {'name': 'd8_fwo', 'd_model': 8, 'freeze_wo': True},\n",
    "    # {'name': 'd8_unfwo', 'd_model': 8, 'freeze_wo': False},\n",
    "\n",
    "    # {'name': 'd4', 'd_model': 4},\n",
    "]\n",
    "\n",
    "from itertools import product\n",
    "# specs = []\n",
    "# d_model = 128\n",
    "# for n_layers, ln, use_bias, freeze_wv, freeze_wo in product(\n",
    "#     [2, 3],            # layers\n",
    "#     [False, True],     # ln\n",
    "#     [False, True],     # use_bias\n",
    "#     [False, True],     # freeze_wv\n",
    "#     [False, True],     # freeze_wo\n",
    "# ):\n",
    "#     specs.append({\n",
    "#         \"name\": make_name(d_model, n_layers, ln, use_bias, freeze_wv, freeze_wo),\n",
    "#         \"d_model\": d_model,\n",
    "#         \"n_layers\": n_layers,\n",
    "#         \"ln\": ln,\n",
    "#         \"use_bias\": use_bias,\n",
    "#         \"freeze_wv\": freeze_wv,\n",
    "#         \"freeze_wo\": freeze_wo,\n",
    "#     })\n",
    "\n",
    "# -----------------------\n",
    "rows = []\n",
    "for spec in specs:\n",
    "    # Create a full spec by starting with defaults and updating with the current spec\n",
    "    full_spec = {\n",
    "        'n_layers': N_LAYER,\n",
    "        'n_heads': N_HEAD,\n",
    "        'd_model': D_MODEL,\n",
    "        'ln': USE_LN,\n",
    "        'bias': USE_BIAS,\n",
    "        'freeze_wv': FREEZE_WV,\n",
    "        'freeze_wo': FREEZE_WO,\n",
    "        'lr': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "    }\n",
    "    full_spec.update(spec) # Overwrite defaults with provided spec values\n",
    "\n",
    "    print(f\"--- Training model: {full_spec['name']} ---\")\n",
    "    model = make_model(\n",
    "        n_layers=full_spec['n_layers'],\n",
    "        n_heads=full_spec['n_heads'],\n",
    "        d_model=full_spec['d_model'], \n",
    "        ln=full_spec['ln'],\n",
    "        use_bias=full_spec['bias'],\n",
    "        freeze_wv=full_spec['freeze_wv'],\n",
    "        freeze_wo=full_spec['freeze_wo'],\n",
    "    )\n",
    "\n",
    "    train(model, max_steps=50_000, lr=full_spec['lr'], weight_decay=full_spec['weight_decay'], verbose=True)\n",
    "    \n",
    "    # Add all spec parameters to the results\n",
    "    result = full_spec.copy()\n",
    "    result['val_acc'] = round(accuracy(model), 4)\n",
    "    rows.append(result)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Move 'name' column to the front for better readability\n",
    "if 'name' in df.columns:\n",
    "    cols = ['name'] + [col for col in df.columns if col != 'name']\n",
    "    df = df[cols]\n",
    "\n",
    "print(df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d7ce1",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "\n",
    "| name                        |   n_layers |   n_heads |   d_model | ln    | use_bias   | freeze_wv   | freeze_wo   |   weight_decay |   val_acc |\n",
    "|:----------------------------|-----------:|----------:|----------:|:------|:-----------|:------------|:------------|---------------:|----------:|\n",
    "| d128_2L_noLN_noBias_uWV_uWO |          2 |         1 |       128 | False | False      | False       | False       |           0.01 |    0.4625 |\n",
    "| d128_2L_noLN_noBias_uWV_fWO |          2 |         1 |       128 | False | False      | False       | True        |           0.01 |    0.4895 |\n",
    "| d128_2L_noLN_noBias_fWV_uWO |          2 |         1 |       128 | False | False      | True        | False       |           0.01 |    0.463  |\n",
    "| d128_2L_noLN_noBias_fWV_fWO |          2 |         1 |       128 | False | False      | True        | True        |           0.01 |    0.9173 |\n",
    "| d128_2L_noLN_Bias_uWV_uWO   |          2 |         1 |       128 | False | True       | False       | False       |           0.01 |    0.868  |\n",
    "| d128_2L_noLN_Bias_uWV_fWO   |          2 |         1 |       128 | False | True       | False       | True        |           0.01 |    0.8945 |\n",
    "| d128_2L_noLN_Bias_fWV_uWO   |          2 |         1 |       128 | False | True       | True        | False       |           0.01 |    0.4645 |\n",
    "| d128_2L_noLN_Bias_fWV_fWO   |          2 |         1 |       128 | False | True       | True        | True        |           0.01 |    0.9183 |\n",
    "| d128_2L_LN_noBias_uWV_uWO   |          2 |         1 |       128 | True  | False      | False       | False       |           0.01 |    0.4743 |\n",
    "| d128_2L_LN_noBias_uWV_fWO   |          2 |         1 |       128 | True  | False      | False       | True        |           0.01 |    0.4607 |\n",
    "| d128_2L_LN_noBias_fWV_uWO   |          2 |         1 |       128 | True  | False      | True        | False       |           0.01 |    0.4632 |\n",
    "| d128_2L_LN_noBias_fWV_fWO   |          2 |         1 |       128 | True  | False      | True        | True        |           0.01 |    0.4485 |\n",
    "| d128_2L_LN_Bias_uWV_uWO     |          2 |         1 |       128 | True  | True       | False       | False       |           0.01 |    0.4733 |\n",
    "| d128_2L_LN_Bias_uWV_fWO     |          2 |         1 |       128 | True  | True       | False       | True        |           0.01 |    0.4647 |\n",
    "| d128_2L_LN_Bias_fWV_uWO     |          2 |         1 |       128 | True  | True       | True        | False       |           0.01 |    0.4755 |\n",
    "| d128_2L_LN_Bias_fWV_fWO     |          2 |         1 |       128 | True  | True       | True        | True        |           0.01 |    0.4602 |\n",
    "\n",
    "| name   |   n_layers |   n_heads |   d_model | ln    | use_bias   | freeze_wv   | freeze_wo   |   weight_decay |   val_acc |\n",
    "|:-------|-----------:|----------:|----------:|:------|:-----------|:------------|:------------|---------------:|----------:|\n",
    "| d256   |          2 |         1 |       256 | False | False  | True        | True        |           0.01 |    0.8697 |\n",
    "| d128   |          2 |         1 |       128 | False | False      | True        | True        |           0.01 |    0.9038 |\n",
    "| d64    |          2 |         1 |        64 | False | False      | True        | True        |           0.01 |    0.6836 |\n",
    "| d32    |          2 |         1 |        32 | False | False      | True        | True        |           0.01 |    0.4278 |\n",
    "| d16    |          2 |         1 |        16 | False | False      | True        | True        |           0.01 |    0.4497 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e08dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c957aa0f71c411aa6fd19b9154655f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500, Loss: 0.6790, Accuracy: 45.02%\n",
      "Step 5000, Loss: 0.6870, Accuracy: 43.33%\n",
      "Step 7500, Loss: 0.6983, Accuracy: 43.80%\n",
      "Step 10000, Loss: 0.6885, Accuracy: 43.65%\n",
      "Step 12500, Loss: 0.6808, Accuracy: 44.47%\n",
      "Step 15000, Loss: 0.6802, Accuracy: 42.02%\n",
      "Step 17500, Loss: 0.6888, Accuracy: 42.25%\n",
      "Step 20000, Loss: 0.6945, Accuracy: 42.93%\n",
      "Step 22500, Loss: 0.6754, Accuracy: 42.70%\n",
      "Step 25000, Loss: 0.6815, Accuracy: 41.60%\n",
      "Step 27500, Loss: 0.6832, Accuracy: 41.93%\n",
      "Step 30000, Loss: 0.3881, Accuracy: 81.27%\n",
      "Step 32500, Loss: 0.2089, Accuracy: 87.98%\n",
      "Step 35000, Loss: 0.1785, Accuracy: 89.42%\n",
      "Step 37500, Loss: 0.1338, Accuracy: 89.95%\n",
      "Step 40000, Loss: 0.1282, Accuracy: 91.12%\n",
      "Step 42500, Loss: 0.1101, Accuracy: 90.88%\n",
      "Step 45000, Loss: 0.1186, Accuracy: 91.40%\n",
      "Step 47500, Loss: 0.0669, Accuracy: 91.33%\n",
      "Step 50000, Loss: 0.1183, Accuracy: 91.45%\n",
      "Final accuracy: 91.45%\n",
      "Model saved to artifacts/2layer_100dig_64d.pt\n"
     ]
    }
   ],
   "source": [
    "# LOAD existing or train and SAVE new model\n",
    "load_existing = True  # Set to False to always train a new model\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and load_existing:\n",
    "    model = load_model(MODEL_PATH, device=DEV)\n",
    "else:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        MODEL_PATH = MODEL_PATH.replace(\".pt\", \"_new.pt\")\n",
    "        print(f\"Model path already exists. Saving new model to {MODEL_PATH}\")\n",
    "    print(\"Training model\")\n",
    "    model = make_model()\n",
    "    train(model, max_steps=MAX_TRAIN_STEPS, checkpoints=USE_CHECKPOINTING)\n",
    "    save_model(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b456419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Overview of Model Parameters ---\n",
      "Parameter Name                           | Shape                | Trainable \n",
      "--------------------------------------------------------------------------------\n",
      "embed.W_E                                | (102, 64)            | Yes       \n",
      "pos_embed.W_pos                          | (5, 64)              | Yes       \n",
      "blocks.0.attn.W_Q                        | (1, 64, 64)          | Yes       \n",
      "blocks.0.attn.W_K                        | (1, 64, 64)          | Yes       \n",
      "blocks.1.attn.W_Q                        | (1, 64, 64)          | Yes       \n",
      "blocks.1.attn.W_K                        | (1, 64, 64)          | Yes       \n",
      "unembed.W_U                              | (64, 102)            | Yes       \n",
      "--------------------------------------------------------------------------------\n",
      "Total parameters: 46758\n",
      "Trainable parameters: 29760\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Model Parameters Overview ---\n",
    "print(\"--- Overview of Model Parameters ---\")   \n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "\n",
    "# Use a formatted string for better alignment\n",
    "print(f\"{'Parameter Name':<40} | {'Shape':<20} | {'Trainable':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    shape_str = str(tuple(param.shape))\n",
    "    is_trainable = \"Yes\" if param.requires_grad else \"No\"\n",
    "    total_params += param.numel()\n",
    "\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    # Print only trainable parameters\n",
    "    print(f\"{name:<40} | {shape_str:<20} | {is_trainable:<10}\")\n",
    "    trainable_params += param.numel()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57f82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
