{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f632e15-3f91-4672-87a4-5ce6c04e218c",
   "metadata": {},
   "source": [
    " ## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b2f55-9c9f-4dab-899e-6912060966d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from datetime import datetime # for unique model naming\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "import einops\n",
    "import pandas as pd, itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n",
    "\n",
    "from model_utils import (\n",
    "    configure_runtime,\n",
    "    build_attention_mask,\n",
    "    save_model,\n",
    "    make_model,\n",
    "    accuracy\n",
    ")\n",
    "from data import get_dataset\n",
    "\n",
    "float_formatter = \"{:.5f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d288046-9564-4394-b4b8-9eecbedaf99f",
   "metadata": {},
   "source": [
    " ## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3798a-67a5-4fdf-b382-8a4bc9aa21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- parameters ----------\n",
    "LIST_LEN = 2 # [d1, d2]\n",
    "SEQ_LEN = LIST_LEN * 2 + 1 # [d1, d2, SEP, o1, o2]\n",
    "\n",
    "N_DIGITS = 100\n",
    "DIGITS = list(range(N_DIGITS)) # 100 digits from 0 to 99\n",
    "MASK = N_DIGITS # special masking token for o1 and o2\n",
    "SEP = N_DIGITS+1 # special seperator token for the model to think about the input (+1 to avoid confusion with the last digit)\n",
    "VOCAB = len(DIGITS) + 2  # + the special tokens\n",
    "\n",
    "D_MODEL = 64\n",
    "N_HEAD = 1\n",
    "N_LAYER = 2\n",
    "USE_LN = False # use layer norm in model\n",
    "USE_BIAS = False # use bias in model\n",
    "FREEZE_WV = True # no value matrix in attn \n",
    "FREEZE_WO = True # no output matrix in attn (i.e. attn head can only copy inputs to outputs)\n",
    "\n",
    "LEARNING_RATE = 1e-3 # default 1e-3\n",
    "WEIGHT_DECAY = 0.01 # default 0.01\n",
    "MAX_TRAIN_STEPS = 50_000 # max training steps\n",
    "USE_CHECKPOINTING = False # whether to use checkpointing for training\n",
    "\n",
    "RUN_TS = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_NAME = f'{N_LAYER}layer_{N_DIGITS}dig_{D_MODEL}d_{RUN_TS}'\n",
    "# MODEL_NAME = \n",
    "MODEL_PATH = \"models/\" + MODEL_NAME + \".pt\"\n",
    "\n",
    "DEV = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Provide runtime config so we don't need to thread constants everywhere\n",
    "configure_runtime(list_len=LIST_LEN, seq_len=SEQ_LEN, vocab=VOCAB, device=DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78a2fe-fb75-46bf-b610-ec4113348ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- mask ----------\n",
    "# attention mask for [d1, d2, SEP, o1, o2] looks like this:\n",
    "# -    d1    d2    SEP    o1    o2   (keys)\n",
    "# d1   0    -inf   -inf  -inf  -inf\n",
    "# d2   0    -inf   -inf  -inf  -inf\n",
    "# SEP  0      0    -inf  -inf  -inf\n",
    "# o1  -inf  -inf    0    -inf   -inf\n",
    "# o2  -inf  -inf    0      0    -inf\n",
    "# (queries)\n",
    "\n",
    "# view mask\n",
    "mask_bias, _ = build_attention_mask()\n",
    "print(mask_bias.cpu()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1f3b8-c08b-4de6-b8cf-94e66e43a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- dataset ----------\n",
    "train_ds, val_ds = get_dataset(\n",
    "    list_len=LIST_LEN, \n",
    "    n_digits=N_DIGITS, \n",
    "    train_split=0.8,\n",
    "    mask_tok=MASK, # use MASK as mask token\n",
    "    sep_tok=SEP, # use SEP as separator token\n",
    "    )\n",
    "\n",
    "train_batch_size = min(128, len(train_ds))\n",
    "val_batch_size = min(256, len(val_ds))\n",
    "train_dl = DataLoader(train_ds, train_batch_size, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, val_batch_size, drop_last=False)\n",
    "\n",
    "print(\"Input:\", train_ds[0][0])\n",
    "print(\"Target:\", train_ds[0][1])\n",
    "print(f\"Train dataset size: {len(train_ds)}, Validation dataset size: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488e68d-8796-41ec-be46-ce701b240216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(m, max_steps=10_000, early_stop_acc=0.999, checkpoints=False, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY):\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr, weight_decay=weight_decay)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "    dl = itertools.cycle(train_dl)  # infinite iterator\n",
    "    pbar = tqdm(range(max_steps), desc=\"Training\")\n",
    "    for step in pbar:\n",
    "        inputs, targets = next(dl)\n",
    "        # get logits/loss for output tokens only\n",
    "        logits = m(inputs.to(DEV))[:, LIST_LEN+1:].reshape(-1, VOCAB) \n",
    "        loss = ce(logits, targets[:, LIST_LEN+1:].reshape(-1).to(DEV))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        if (step + 1) % 100 == 0:\n",
    "            acc = accuracy(m, val_dl)\n",
    "            if acc > early_stop_acc:\n",
    "                print(f\"Early stopping at step {step + 1} with accuracy {acc:.2%} >= {early_stop_acc:.2%}\")\n",
    "                break\n",
    "            # Update tqdm bar w/ metrics\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\": f\"{acc:.2%}\",\n",
    "            })\n",
    "            if checkpoints and (step+1) % 50_000 == 0:\n",
    "                save_model(m, MODEL_PATH)\n",
    "            \n",
    "    print(f\"Final accuracy: {accuracy(m, val_dl):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ffacd-256c-4c7b-9f2a-e68c1dae3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check train set\n",
    "train_ds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34f43b-3c69-4ef7-b80e-b11ee9a5e1eb",
   "metadata": {},
   "source": [
    " **RESULTS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b771f88-27ae-40f3-8bcf-778c354e36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and SAVE new model\n",
    "acc = 0\n",
    "while acc < 0.9:\n",
    "    print(f\"Training {MODEL_NAME}\")\n",
    "    model = make_model(\n",
    "        n_layers=N_LAYER,\n",
    "        n_heads=N_HEAD,\n",
    "        d_model=D_MODEL,\n",
    "        ln=USE_LN,\n",
    "        use_bias=USE_BIAS,\n",
    "        freeze_wv=FREEZE_WV,\n",
    "        freeze_wo=FREEZE_WO,\n",
    "    )\n",
    "    train(model, max_steps=MAX_TRAIN_STEPS, checkpoints=USE_CHECKPOINTING)\n",
    "    acc = accuracy(model, val_dl)\n",
    "    if acc > 0.8:\n",
    "        save_model(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b5a0e-e35c-47bf-a39c-e9b6c53a0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Parameters Overview ---\n",
    "m_for_overview = globals().get('model', None)\n",
    "if m_for_overview is not None:\n",
    "    print(\"--- Overview of Model Parameters ---\")   \n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "\n",
    "    # Use a formatted string for better alignment\n",
    "    print(f\"{'Parameter Name':<40} | {'Shape':<20} | {'Trainable':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for name, param in m_for_overview.named_parameters():\n",
    "        shape_str = str(tuple(param.shape))\n",
    "        is_trainable = \"Yes\" if param.requires_grad else \"No\"\n",
    "        total_params += param.numel()\n",
    "\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        # Print only trainable parameters\n",
    "        print(f\"{name:<40} | {shape_str:<20} | {is_trainable:<10}\")\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
