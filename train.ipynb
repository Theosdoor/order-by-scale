{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52fcc20",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2de5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from datetime import datetime # for unique model naming\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "import einops\n",
    "import pandas as pd, itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n",
    "\n",
    "from model_utils import (\n",
    "    configure_runtime,\n",
    "    build_attention_mask,\n",
    "    attach_custom_mask,\n",
    "    strip_bias,\n",
    "    set_WV_identity_and_freeze,\n",
    "    set_WO_identity_and_freeze,\n",
    "    save_model,\n",
    "    load_model,\n",
    "    make_model,\n",
    "    accuracy\n",
    ")\n",
    "from data import get_dataset\n",
    "\n",
    "float_formatter = \"{:.5f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15f0d1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1437fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- parameters ----------\n",
    "LIST_LEN = 2 # [d1, d2]\n",
    "SEQ_LEN = LIST_LEN * 2 + 1 # [d1, d2, SEP, o1, o2]\n",
    "\n",
    "N_DIGITS = 100\n",
    "DIGITS = list(range(N_DIGITS)) # 100 digits from 0 to 99\n",
    "MASK = N_DIGITS # special masking token for o1 and o2\n",
    "SEP = N_DIGITS+1 # special seperator token for the model to think about the input (+1 to avoid confusion with the last digit)\n",
    "VOCAB = len(DIGITS) + 2  # + the special tokens\n",
    "\n",
    "D_MODEL = 64\n",
    "N_HEAD = 1\n",
    "N_LAYER = 2\n",
    "USE_LN = False # use layer norm in model\n",
    "USE_BIAS = False # use bias in model\n",
    "FREEZE_WV = True # no value matrix in attn \n",
    "FREEZE_WO = True # no output matrix in attn (i.e. attn head can only copy inputs to outputs)\n",
    "\n",
    "LEARNING_RATE = 1e-3 # default 1e-3\n",
    "WEIGHT_DECAY = 0.01 # default 0.01\n",
    "MAX_TRAIN_STEPS = 50_000 # max training steps\n",
    "USE_CHECKPOINTING = False # whether to use checkpointing for training\n",
    "\n",
    "RUN_TS = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_NAME = f'{N_LAYER}layer_{N_DIGITS}dig_{D_MODEL}d_{RUN_TS}'\n",
    "# MODEL_NAME = \n",
    "MODEL_PATH = \"models/\" + MODEL_NAME + \".pt\"\n",
    "\n",
    "# --- dataset --- (not necessary as we fix seed?)\n",
    "# DATASET_NAME = None #Â None ==> generate new one\n",
    "# listlen2_digits10_dupes\n",
    "# listlen2_digits10_nodupes\n",
    "# listlen2_digits100_dupes_traindupesonly\n",
    "# listlen2_digits100_dupes\n",
    "# listlen2_digits100_nodupes\n",
    "\n",
    "DEV = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Provide runtime config so we don't need to thread constants everywhere\n",
    "configure_runtime(list_len=LIST_LEN, seq_len=SEQ_LEN, vocab=VOCAB, device=DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316797fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [-inf, -inf, 0., -inf, -inf],\n",
      "        [-inf, -inf, 0., 0., -inf]])\n"
     ]
    }
   ],
   "source": [
    "# ---------- mask ----------\n",
    "# attention mask for [d1, d2, SEP, o1, o2] looks like this:\n",
    "# -    d1    d2    SEP    o1    o2   (keys)\n",
    "# d1  -inf  -inf   -inf  -inf  -inf\n",
    "# d2   0    -inf   -inf  -inf  -inf\n",
    "# SEP  0      0    -inf  -inf  -inf\n",
    "# o1  -inf  -inf    0    -inf   -inf\n",
    "# o2  -inf  -inf    0      0    -inf\n",
    "# (queries)\n",
    "\n",
    "# view mask\n",
    "mask_bias, _ = build_attention_mask()\n",
    "print(mask_bias.cpu()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60abc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([ 60,  44, 101, 100, 100])\n",
      "Target: tensor([ 60,  44, 101,  60,  44])\n",
      "Train dataset size: 8000, Validation dataset size: 2000\n"
     ]
    }
   ],
   "source": [
    "# ---------- dataset ----------\n",
    "train_ds, val_ds = get_dataset(\n",
    "    list_len=LIST_LEN, \n",
    "    n_digits=N_DIGITS, \n",
    "    train_split=0.8,\n",
    "    mask_tok=MASK, # use MASK as mask token\n",
    "    sep_tok=SEP, # use SEP as separator token\n",
    "    )\n",
    "\n",
    "train_batch_size = min(128, len(train_ds))\n",
    "val_batch_size = min(256, len(val_ds))\n",
    "train_dl = DataLoader(train_ds, train_batch_size, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, val_batch_size, drop_last=False)\n",
    "\n",
    "print(\"Input:\", train_ds[0][0])\n",
    "print(\"Target:\", train_ds[0][1])\n",
    "print(f\"Train dataset size: {len(train_ds)}, Validation dataset size: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95071794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(m, max_steps=10_000, early_stop_acc=0.999, checkpoints=False, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, verbose=True):\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr, weight_decay=weight_decay)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "    dl = itertools.cycle(train_dl)  # infinite iterator\n",
    "    for step in tqdm(range(max_steps), desc=\"Training\"):\n",
    "        inputs, targets = next(dl)\n",
    "        # get logits/loss for output tokens only\n",
    "        logits = m(inputs.to(DEV))[:, LIST_LEN+1:].reshape(-1, VOCAB) \n",
    "        loss = ce(logits, targets[:, LIST_LEN+1:].reshape(-1).to(DEV))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        if (step + 1) % 100 == 0:\n",
    "            acc = accuracy(m, val_dl)\n",
    "            if acc > early_stop_acc:\n",
    "                print(f\"Early stopping at step {step + 1} with accuracy {acc:.2%} >= {early_stop_acc:.2%}\")\n",
    "                break\n",
    "            update_every = max(min(10_000, max_steps//20), 1000)\n",
    "            if verbose and (step+1) % update_every == 0:\n",
    "                print(f\"Step {step + 1}, Loss: {loss.item():.4f}, Accuracy: {acc:.2%}\")\n",
    "            if checkpoints and (step+1) % 50_000 == 0:\n",
    "                save_model(m, MODEL_PATH)\n",
    "            \n",
    "    print(f\"Final accuracy: {accuracy(m, val_dl):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea857e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 60,  44, 101, 100, 100],\n",
       "         [ 28,  90, 101, 100, 100],\n",
       "         [ 93,  99, 101, 100, 100],\n",
       "         [ 19,  17, 101, 100, 100],\n",
       "         [ 49,  19, 101, 100, 100]]),\n",
       " tensor([[ 60,  44, 101,  60,  44],\n",
       "         [ 28,  90, 101,  28,  90],\n",
       "         [ 93,  99, 101,  93,  99],\n",
       "         [ 19,  17, 101,  19,  17],\n",
       "         [ 49,  19, 101,  49,  19]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check train set\n",
    "train_ds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29f099",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/compat/_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:984\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    105\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[cols]\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:2983\u001b[0m, in \u001b[0;36mDataFrame.to_markdown\u001b[0;34m(self, buf, mode, index, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2981\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtablefmt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2982\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshowindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, index)\n\u001b[0;32m-> 2983\u001b[0m tabulate \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtabulate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m result \u001b[38;5;241m=\u001b[39m tabulate\u001b[38;5;241m.\u001b[39mtabulate(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/compat/_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "# ---------- experiment grid ----------\n",
    "def make_name(d_model, n_layers, ln, use_bias, freeze_wv, freeze_wo):\n",
    "    parts = [\n",
    "        f\"d{d_model}\",\n",
    "        f\"{n_layers}L\",\n",
    "        (\"LN\" if ln else \"noLN\"),\n",
    "        (\"Bias\" if use_bias else \"noBias\"),\n",
    "        (\"fWV\" if freeze_wv else \"uWV\"), # freeze / unfreeze\n",
    "        (\"fWO\" if freeze_wo else \"uWO\"),\n",
    "    ]\n",
    "    return \"_\".join(parts)\n",
    "\n",
    "specs = [\n",
    "    # {'name': 'd256', 'd_model': 256},\n",
    "    # {'name': 'd128', 'd_model': 128, 'weight_decay': 1.0},\n",
    "    # {'name': 'd64', 'd_model': 64},\n",
    "    \n",
    "    # {'name': 'd32', 'd_model': 32},\n",
    "    # {'name': 'd32_ln_bias', 'd_model': 32, 'ln': True, 'use_bias': True},\n",
    "    # {'name': 'd32_noLN', 'd_model': 32, 'ln': False, 'use_bias': True},\n",
    "    # {'name': 'd32_noBias', 'd_model': 32, 'ln': True, 'use_bias': False},\n",
    "    # {'name': 'd32_noLNnoBias', 'd_model': 32, 'ln': False, 'use_bias': False},\n",
    "    # {'name': 'd32_fwo', 'd_model': 32, 'freeze_wo': True},\n",
    "    # {'name': 'd32_unfwo', 'd_model': 32, 'freeze_wo': False},\n",
    "\n",
    "    # {'name': 'd16', 'd_model': 16},\n",
    "    # {'name': 'd16_ln_bias', 'd_model': 16, 'ln': True, 'use_bias': True},\n",
    "    # {'name': 'd16_noLN', 'd_model': 16, 'ln': False, 'use_bias': True},\n",
    "    # {'name': 'd16_noBias', 'd_model': 16, 'ln': True, 'use_bias': False},\n",
    "    # {'name': 'd16_noLNnoBias', 'd_model': 16, 'ln': False, 'use_bias': False},\n",
    "    # {'name': 'd16_fwo', 'd_model': 16, 'freeze_wo': True},\n",
    "    # {'name': 'd16_unfwo', 'd_model': 16, 'freeze_wo': False},\n",
    "\n",
    "    # {'name': 'd8', 'd_model': 8},\n",
    "    # {'name': 'd8_ln_bias', 'd_model': 8, 'ln': True, 'use_bias': True},\n",
    "    # {'name': 'd8_noLN', 'd_model': 8, 'ln': False, 'use_bias': True},\n",
    "    # {'name': 'd8_noBias', 'd_model': 8, 'ln': True, 'use_bias': False},\n",
    "    # {'name': 'd8_noLNnoBias', 'd_model': 8, 'ln': False, 'use_bias': False},\n",
    "    # {'name': 'd8_fwo', 'd_model': 8, 'freeze_wo': True},\n",
    "    # {'name': 'd8_unfwo', 'd_model': 8, 'freeze_wo': False},\n",
    "\n",
    "    # {'name': 'd4', 'd_model': 4},\n",
    "]\n",
    "\n",
    "from itertools import product\n",
    "# specs = []\n",
    "# d_model = 128\n",
    "# for n_layers, ln, use_bias, freeze_wv, freeze_wo in product(\n",
    "#     [2, 3],            # layers\n",
    "#     [False, True],     # ln\n",
    "#     [False, True],     # use_bias\n",
    "#     [False, True],     # freeze_wv\n",
    "#     [False, True],     # freeze_wo\n",
    "# ):\n",
    "#     specs.append({\n",
    "#         \"name\": make_name(d_model, n_layers, ln, use_bias, freeze_wv, freeze_wo),\n",
    "#         \"d_model\": d_model,\n",
    "#         \"n_layers\": n_layers,\n",
    "#         \"ln\": ln,\n",
    "#         \"use_bias\": use_bias,\n",
    "#         \"freeze_wv\": freeze_wv,\n",
    "#         \"freeze_wo\": freeze_wo,\n",
    "#     })\n",
    "\n",
    "# -----------------------\n",
    "rows = []\n",
    "for spec in specs:\n",
    "    # Create a full spec by starting with defaults and updating with the current spec\n",
    "    full_spec = {\n",
    "        'n_layers': N_LAYER,\n",
    "        'n_heads': N_HEAD,\n",
    "        'd_model': D_MODEL,\n",
    "        'ln': USE_LN,\n",
    "        'bias': USE_BIAS,\n",
    "        'freeze_wv': FREEZE_WV,\n",
    "        'freeze_wo': FREEZE_WO,\n",
    "        'lr': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "    }\n",
    "    full_spec.update(spec) # Overwrite defaults with provided spec values\n",
    "\n",
    "    print(f\"--- Training model: {full_spec['name']} ---\")\n",
    "    model = make_model(\n",
    "        n_layers=full_spec['n_layers'],\n",
    "        n_heads=full_spec['n_heads'],\n",
    "        d_model=full_spec['d_model'],\n",
    "        ln=full_spec['ln'],\n",
    "        use_bias=full_spec['bias'],\n",
    "        freeze_wv=full_spec['freeze_wv'],\n",
    "        freeze_wo=full_spec['freeze_wo'],\n",
    "    )\n",
    "\n",
    "    train(model, max_steps=50_000, lr=full_spec['lr'], weight_decay=full_spec['weight_decay'], verbose=True)\n",
    "    \n",
    "    # Add all spec parameters to the results\n",
    "    result = full_spec.copy()\n",
    "    result['val_acc'] = round(accuracy(model, val_dl), 4)\n",
    "    rows.append(result)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Move 'name' column to the front for better readability\n",
    "if 'name' in df.columns:\n",
    "    cols = ['name'] + [col for col in df.columns if col != 'name']\n",
    "    df = df[cols]\n",
    "\n",
    "print(df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d7ce1",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "\n",
    "| name                        |   n_layers |   n_heads |   d_model | ln    | use_bias   | freeze_wv   | freeze_wo   |   weight_decay |   val_acc |\n",
    "|:----------------------------|-----------:|----------:|----------:|:------|:-----------|:------------|:------------|---------------:|----------:|\n",
    "| d128_2L_noLN_noBias_uWV_uWO |          2 |         1 |       128 | False | False      | False       | False       |           0.01 |    0.4625 |\n",
    "| d128_2L_noLN_noBias_uWV_fWO |          2 |         1 |       128 | False | False      | False       | True        |           0.01 |    0.4895 |\n",
    "| d128_2L_noLN_noBias_fWV_uWO |          2 |         1 |       128 | False | False      | True        | False       |           0.01 |    0.463  |\n",
    "| d128_2L_noLN_noBias_fWV_fWO |          2 |         1 |       128 | False | False      | True        | True        |           0.01 |    0.9173 |\n",
    "| d128_2L_noLN_Bias_uWV_uWO   |          2 |         1 |       128 | False | True       | False       | False       |           0.01 |    0.868  |\n",
    "| d128_2L_noLN_Bias_uWV_fWO   |          2 |         1 |       128 | False | True       | False       | True        |           0.01 |    0.8945 |\n",
    "| d128_2L_noLN_Bias_fWV_uWO   |          2 |         1 |       128 | False | True       | True        | False       |           0.01 |    0.4645 |\n",
    "| d128_2L_noLN_Bias_fWV_fWO   |          2 |         1 |       128 | False | True       | True        | True        |           0.01 |    0.9183 |\n",
    "| d128_2L_LN_noBias_uWV_uWO   |          2 |         1 |       128 | True  | False      | False       | False       |           0.01 |    0.4743 |\n",
    "| d128_2L_LN_noBias_uWV_fWO   |          2 |         1 |       128 | True  | False      | False       | True        |           0.01 |    0.4607 |\n",
    "| d128_2L_LN_noBias_fWV_uWO   |          2 |         1 |       128 | True  | False      | True        | False       |           0.01 |    0.4632 |\n",
    "| d128_2L_LN_noBias_fWV_fWO   |          2 |         1 |       128 | True  | False      | True        | True        |           0.01 |    0.4485 |\n",
    "| d128_2L_LN_Bias_uWV_uWO     |          2 |         1 |       128 | True  | True       | False       | False       |           0.01 |    0.4733 |\n",
    "| d128_2L_LN_Bias_uWV_fWO     |          2 |         1 |       128 | True  | True       | False       | True        |           0.01 |    0.4647 |\n",
    "| d128_2L_LN_Bias_fWV_uWO     |          2 |         1 |       128 | True  | True       | True        | False       |           0.01 |    0.4755 |\n",
    "| d128_2L_LN_Bias_fWV_fWO     |          2 |         1 |       128 | True  | True       | True        | True        |           0.01 |    0.4602 |\n",
    "\n",
    "| name   |   n_layers |   n_heads |   d_model | ln    | use_bias   | freeze_wv   | freeze_wo   |   weight_decay |   val_acc |\n",
    "|:-------|-----------:|----------:|----------:|:------|:-----------|:------------|:------------|---------------:|----------:|\n",
    "| d256   |          2 |         1 |       256 | False | False  | True        | True        |           0.01 |    0.8697 |\n",
    "| d128   |          2 |         1 |       128 | False | False      | True        | True        |           0.01 |    0.9038 |\n",
    "| d64    |          2 |         1 |        64 | False | False      | True        | True        |           0.01 |    0.6836 |\n",
    "| d32    |          2 |         1 |        32 | False | False      | True        | True        |           0.01 |    0.4278 |\n",
    "| d16    |          2 |         1 |        16 | False | False      | True        | True        |           0.01 |    0.4497 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e08dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280ab8acd6be441d9944ecd0a5a9337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m model = make_model(\n\u001b[32m     21\u001b[39m     n_layers=N_LAYER,\n\u001b[32m     22\u001b[39m     n_heads=N_HEAD,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     freeze_wo=FREEZE_WO,\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_TRAIN_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_CHECKPOINTING\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m save_model(model, MODEL_PATH)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(m, max_steps, early_stop_acc, checkpoints, lr, weight_decay, verbose)\u001b[39m\n\u001b[32m      6\u001b[39m inputs, targets = \u001b[38;5;28mnext\u001b[39m(dl)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# get logits/loss for output tokens only\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m logits = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[:, LIST_LEN+\u001b[32m1\u001b[39m:].reshape(-\u001b[32m1\u001b[39m, VOCAB) \n\u001b[32m      9\u001b[39m loss = ce(logits, targets[:, LIST_LEN+\u001b[32m1\u001b[39m:].reshape(-\u001b[32m1\u001b[39m).to(DEV))\n\u001b[32m     10\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:638\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munembed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.output_logits_soft_cap > \u001b[32m0.0\u001b[39m:\n\u001b[32m    640\u001b[39m         logits = \u001b[38;5;28mself\u001b[39m.cfg.output_logits_soft_cap * F.tanh(\n\u001b[32m    641\u001b[39m             logits / \u001b[38;5;28mself\u001b[39m.cfg.output_logits_soft_cap\n\u001b[32m    642\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/transformer_lens/components/unembed.py:31\u001b[39m, in \u001b[36mUnembed.forward\u001b[39m\u001b[34m(self, residual)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m, residual: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch pos d_model\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     30\u001b[39m ) -> Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch pos d_vocab_out\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbatch_addmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mb_U\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_U\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/transformer_lens/utilities/addmm.py:33\u001b[39m, in \u001b[36mbatch_addmm\u001b[39m\u001b[34m(bias, weight, x)\u001b[39m\n\u001b[32m     31\u001b[39m n_output_features = weight.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m     32\u001b[39m size_out = x.size()[:-\u001b[32m1\u001b[39m] + (n_output_features,)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m x = \u001b[43mvanilla_addmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m x = x.view(size_out)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/transformer_lens/utilities/addmm.py:18\u001b[39m, in \u001b[36mvanilla_addmm\u001b[39m\u001b[34m(input, mat1, mat2)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvanilla_addmm\u001b[39m(\n\u001b[32m     10\u001b[39m     \u001b[38;5;28minput\u001b[39m: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33m... #o\u001b[39m\u001b[33m\"\u001b[39m],  \u001b[38;5;66;03m# Must be broadcastable to \"m o\"\u001b[39;00m\n\u001b[32m     11\u001b[39m     mat1: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mm n\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     12\u001b[39m     mat2: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mn o\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     13\u001b[39m ) -> Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mm o\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Typechecked version of torch.addmm.\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[33;03m    Note that both mat1 and mat2 *must* be 2d matrices.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# LOAD existing or train and SAVE new model\n",
    "load_existing = True  # Set to False to always train a new model\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and load_existing:\n",
    "    model = load_model(\n",
    "        MODEL_PATH,\n",
    "        n_layers=N_LAYER,\n",
    "        n_heads=N_HEAD,\n",
    "        d_model=D_MODEL,\n",
    "        ln=USE_LN,\n",
    "        use_bias=USE_BIAS,\n",
    "        freeze_wv=FREEZE_WV,\n",
    "        freeze_wo=FREEZE_WO,\n",
    "    )\n",
    "else:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        MODEL_PATH = MODEL_PATH.replace(\".pt\", \"_new.pt\")\n",
    "        print(f\"Model path already exists. Saving new model to {MODEL_PATH}\")\n",
    "    print(\"Training model\")\n",
    "    model = make_model(\n",
    "        n_layers=N_LAYER,\n",
    "        n_heads=N_HEAD,\n",
    "        d_model=D_MODEL,\n",
    "        ln=USE_LN,\n",
    "        use_bias=USE_BIAS,\n",
    "        freeze_wv=FREEZE_WV,\n",
    "        freeze_wo=FREEZE_WO,\n",
    "    )\n",
    "    train(model, max_steps=MAX_TRAIN_STEPS, checkpoints=USE_CHECKPOINTING)\n",
    "    save_model(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Overview of Model Parameters ---\n",
      "Parameter Name                           | Shape                | Trainable \n",
      "--------------------------------------------------------------------------------\n",
      "embed.W_E                                | (102, 64)            | Yes       \n",
      "pos_embed.W_pos                          | (5, 64)              | Yes       \n",
      "blocks.0.attn.W_Q                        | (1, 64, 64)          | Yes       \n",
      "blocks.0.attn.W_K                        | (1, 64, 64)          | Yes       \n",
      "blocks.1.attn.W_Q                        | (1, 64, 64)          | Yes       \n",
      "blocks.1.attn.W_K                        | (1, 64, 64)          | Yes       \n",
      "unembed.W_U                              | (64, 102)            | Yes       \n",
      "--------------------------------------------------------------------------------\n",
      "Total parameters: 46758\n",
      "Trainable parameters: 29760\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Model Parameters Overview ---\n",
    "print(\"--- Overview of Model Parameters ---\")   \n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "\n",
    "# Use a formatted string for better alignment\n",
    "print(f\"{'Parameter Name':<40} | {'Shape':<20} | {'Trainable':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    shape_str = str(tuple(param.shape))\n",
    "    is_trainable = \"Yes\" if param.requires_grad else \"No\"\n",
    "    total_params += param.numel()\n",
    "\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    # Print only trainable parameters\n",
    "    print(f\"{name:<40} | {shape_str:<20} | {is_trainable:<10}\")\n",
    "    trainable_params += param.numel()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57f82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
