{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "h3H1R__HCc54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch, random\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "# from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Configure plotly to use static rendering if widgets fail\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"notebook\"\n",
        "\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "float_formatter = \"{:.5f}\".format\n",
        "np.set_printoptions(formatter={'float_kind':float_formatter})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdUT-1P-_Gsu"
      },
      "source": [
        "How does this model work?\n",
        "\n",
        "super basic two layer transformer with no MLP or even a value matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "1f65592add8048388731ac71645b9a22"
          ]
        },
        "id": "rlhNtRpvCh8v",
        "outputId": "ff423a77-9474-4960-d3e6-7a7e9c5e07c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from artifacts/len_2_v1.pt\n",
            "Moving model to device:  cpu\n"
          ]
        }
      ],
      "source": [
        "# DIGITS = list(range(10))\n",
        "# LIST_LEN = 2\n",
        "# SPECIAL = 10\n",
        "# VOCAB = 11\n",
        "# SEQ_LEN = LIST_LEN * 2 + 1\n",
        "# D_MODEL = 16\n",
        "# N_HEAD = 1\n",
        "# N_LAYER = 2 # 2 layers each with single attn head\n",
        "# FREEZE_WV = True # no value matrix in attn (i.e. attn head can only copy inputs to outputs)\n",
        "# MODEL_PATH = \"artifacts/len_2_v1.pt\"\n",
        "\n",
        "# class DigitDataset(Dataset):\n",
        "#     def __init__(self, n):\n",
        "#         self.data = [[random.randint(0, 9) for _ in range(LIST_LEN)] for _ in range(n)]\n",
        "#         # come up with 'size' lots of sequences of random digits (each seq len 2)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # [d1, d2, 10, d1, d2], where 10 is a special separator token\n",
        "#         seq = self.data[idx]\n",
        "#         tok = seq + [SPECIAL] + seq\n",
        "#         return torch.tensor(tok, dtype=torch.long)\n",
        "\n",
        "# def build_mask(n: int, lead_diag=1) -> torch.Tensor:\n",
        "#     # create attention pattern for a sequence of length n\n",
        "#     # rows are queries, columns are keys\n",
        "#     # float(-inf) means \"ignore this position\" (i.e. becomes 0 in later softmax - see 3b1b video)\n",
        "#     m = torch.triu(torch.ones(n, n) * float(\"-inf\"), diagonal=lead_diag) # prevents attending to future tokens NOTE\n",
        "#     m[LIST_LEN+1:, :LIST_LEN] = float(\"-inf\")\n",
        "#     # m[LIST_LEN+1:] = all tokens after the special token (i.e. queries of output tokens)\n",
        "#     # m[LIST_LEN+1:, :LIST_LEN] = refers to all keys of tokens before special token\n",
        "#     #  ==> this line explicitly forbids the output tokens (when they are the query) from attending to the input tokens (when they are the key).\n",
        "\n",
        "#     # attention mask for [d1, d2, SEP, o1, o2] looks like this (query rows are horizontal, key columns are vertical):\n",
        "#     # -    d1    d2    SEP    o1    o2   (keys)\n",
        "#     # d1   0    -inf   -inf  -inf  -inf\n",
        "#     # d2   0      0    -inf  -inf  -inf\n",
        "#     # SEP  0      0     0    -inf  -inf\n",
        "#     # o1  -inf  -inf    0      0   -inf\n",
        "#     # o2  -inf  -inf    0      0     0\n",
        "#     # (queries)\n",
        "\n",
        "\n",
        "#     # -    SEP   d1    d2     o1    o2   (keys)\n",
        "#     # d1   0    -inf   -inf  -inf  -inf\n",
        "#     # d2   0      0    -inf  -inf  -inf\n",
        "#     # SEP  0      0     0    -inf  -inf\n",
        "#     # o1  -inf  -inf    0      0   -inf\n",
        "#     # o2  -inf  -inf    0      0     0\n",
        "#     # (queries)\n",
        "#     return m\n",
        "\n",
        "\n",
        "# def make_model(device: str = \"cuda\") -> \"HookedTransformer\":\n",
        "#     cfg = HookedTransformerConfig(\n",
        "#         d_model=D_MODEL,\n",
        "#         d_head=D_MODEL // N_HEAD,\n",
        "#         n_layers=N_LAYER,\n",
        "#         n_heads=N_HEAD,\n",
        "#         n_ctx=SEQ_LEN,\n",
        "#         d_vocab=VOCAB,\n",
        "#         d_vocab_out=VOCAB,\n",
        "#         attn_only=True, # no MLP!\n",
        "#     )\n",
        "#     model = HookedTransformer(cfg).to(device)\n",
        "#     if FREEZE_WV:\n",
        "#         set_WV_identity_and_freeze(model)\n",
        "#     return model\n",
        "\n",
        "\n",
        "# def attach_custom_mask(model: \"HookedTransformer\") -> None:\n",
        "#     def _mask(scores, hook=None):\n",
        "#         t = scores.size(-1)\n",
        "#         scores += build_mask(t).to(scores.device)\n",
        "#         return scores\n",
        "    \n",
        "#     # def _mask2(scores, hook=None):\n",
        "#     #     t = scores.size(-1)\n",
        "#     #     scores += build_mask(t, 0).to(scores.device)\n",
        "#     #     return scores\n",
        "#     # model.blocks[0].attn.hook_attn_scores.add_perma_hook(_mask)\n",
        "#     # model.blocks[1].attn.hook_attn_scores.add_perma_hook(_mask2)\n",
        "\n",
        "#     for block in model.blocks:\n",
        "#         block.attn.hook_attn_scores.add_perma_hook(_mask)\n",
        "\n",
        "\n",
        "\n",
        "# def set_WV_identity_and_freeze(model: \"HookedTransformer\") -> None:\n",
        "#     with torch.no_grad():\n",
        "#         eye = torch.eye(D_MODEL).unsqueeze(0)  # add head dim\n",
        "#         for block in model.blocks:\n",
        "#             block.attn.W_V.copy_(eye)\n",
        "#             block.attn.W_V.requires_grad = False\n",
        "\n",
        "# def train(\n",
        "#     epochs: int = 10,\n",
        "#     batch: int = 1024,\n",
        "#     size: int = 50000,\n",
        "#     val: int = 1000,\n",
        "#     device=\"cuda\",\n",
        "# ) -> HookedTransformer:\n",
        "    \n",
        "#     ds = DigitDataset(size)\n",
        "#     dl = DataLoader(ds, batch, shuffle=True)\n",
        "#     model = make_model(device)\n",
        "#     attach_custom_mask(model)\n",
        "#     opt = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "#     loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#     for epoch in tqdm(range(epochs)):\n",
        "#         epoch_loss = 0\n",
        "#         num_batches = 0\n",
        "#         for seq in dl:\n",
        "#             seq = seq.to(device)\n",
        "            \n",
        "#             # FIXED: Only compute loss on the output tokens (positions 3 and 4)\n",
        "#             # Input sequence: [d1, d2, SEP, d1, d2]\n",
        "#             # We want to predict positions 3,4 from positions 0,1,2,3\n",
        "#             logits = model(seq)  # Shape: [batch, seq_len, vocab]\n",
        "            \n",
        "#             # Only compute loss on the output positions (last LIST_LEN tokens)\n",
        "#             output_logits = logits[:, -LIST_LEN:, :]  # [batch, LIST_LEN, vocab]\n",
        "#             output_targets = seq[:, -LIST_LEN:]       # [batch, LIST_LEN]\n",
        "            \n",
        "#             loss = loss_fn(output_logits.reshape(-1, VOCAB), output_targets.reshape(-1))\n",
        "            \n",
        "#             loss.backward()\n",
        "#             opt.step()\n",
        "#             opt.zero_grad()\n",
        "            \n",
        "#             epoch_loss += loss.item()\n",
        "#             num_batches += 1\n",
        "            \n",
        "#         avg_loss = epoch_loss / num_batches\n",
        "        \n",
        "#         # Validation accuracy\n",
        "#         corr = 0\n",
        "#         for _ in range(val):\n",
        "#             d = [random.randint(0, 9) for _ in range(LIST_LEN)]\n",
        "#             corr += generate(model, d) == d\n",
        "#         acc = corr / val\n",
        "#         print(f\"Epoch {epoch+1}: Loss {avg_loss:.4f}, Acc {acc:.2%}\")\n",
        "#     return model\n",
        "\n",
        "# # def train(\n",
        "# #     epochs: int = 10,\n",
        "# #     batch: int = 1024,\n",
        "# #     size: int = 50000,\n",
        "# #     val: int = 1000,\n",
        "# #     device=\"cuda\",\n",
        "# # ) -> HookedTransformer:\n",
        "\n",
        "# #     ds = DigitDataset(size)\n",
        "# #     dl = DataLoader(ds, batch, shuffle=True)\n",
        "# #     model = make_model(device)\n",
        "# #     attach_custom_mask(model)\n",
        "# #     opt = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "# #     loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# #     for _ in tqdm(range(epochs)):\n",
        "# #         for seq in dl:\n",
        "# #             seq = seq.to(device)\n",
        "# #             # OLD\n",
        "# #             # logits = model(seq[:, :-1])  # predict next token, so exclude last token\n",
        "# #             # loss = loss_fn(logits.reshape(-1, VOCAB), seq[:, 1:].reshape(-1))\n",
        "\n",
        "# #             # NEW\n",
        "# #             logits = model(seq) \n",
        "# #             loss = loss_fn(logits.reshape(-1, VOCAB), seq.reshape(-1))\n",
        "\n",
        "# #             loss.backward()\n",
        "# #             opt.step()\n",
        "# #             opt.zero_grad()\n",
        "# #         corr = 0\n",
        "# #         for _ in range(val):\n",
        "# #             d = [random.randint(0, 9) for _ in range(LIST_LEN)]\n",
        "# #             corr += generate(model, d) == d\n",
        "# #         print(f\"acc {corr / val:.2%}\")\n",
        "# #     return model\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def generate(model: HookedTransformer, digits: list[int]) -> list[int]:\n",
        "#     seq = digits + [SPECIAL]\n",
        "#     out: list[int] = []\n",
        "#     for _ in range(LIST_LEN):\n",
        "#         x = torch.tensor(seq + out, device=next(model.parameters()).device).unsqueeze(0)\n",
        "#         nxt = model(x)[:, -1].argmax(-1).item()\n",
        "#         out.append(nxt)\n",
        "#     return out\n",
        "\n",
        "\n",
        "# def save_model(model: HookedTransformer, path: str = MODEL_PATH):\n",
        "#     os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "#     torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "# def load_model(\n",
        "#     path: str = MODEL_PATH, device: str = \"cuda\"\n",
        "# ) -> HookedTransformer:\n",
        "#     model = make_model(device)\n",
        "#     model.load_state_dict(\n",
        "#         torch.load(path, map_location=device)\n",
        "#     )  # map weights to target device\n",
        "#     attach_custom_mask(model)\n",
        "#     model.eval()\n",
        "#     return model\n",
        "\n",
        "# USAGE\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# path = MODEL_PATH\n",
        "# if os.path.exists(path):\n",
        "#     print(\"Loading model from\", path)\n",
        "#     model = load_model(path, device)\n",
        "# else:\n",
        "#     print(\"Training model\")\n",
        "#     model = train(epochs=20, device=device)\n",
        "#     save_model(model, path)\n",
        "\n",
        "\n",
        "\n",
        "# notes\n",
        "#  - try sep as cls token (at start?) - see bert. at start so always unmasked, and can chec if its a good summary of list\n",
        "#  - try making SEP always unmasked ==> like BERT, means it provides better context. Also experiment with in middle vs start?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### New model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, pandas as pd, itertools\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- constants ----------\n",
        "VOCAB, THINK, SEQ = 101, 100, 5  # vocab size, think token, sequence length\n",
        "DEV = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        ")\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# mask: True = allowed, converted to big negative bias later\n",
        "mask_bool = torch.tril(torch.ones(SEQ, SEQ)).bool()\n",
        "mask_bool[3:, :2] = False\n",
        "mask_bool.fill_diagonal_(False)\n",
        "mask_bias = (~mask_bool).float() * -1e5  # additive bias, shape (T,T)\n",
        "mask_bias = mask_bias.unsqueeze(0).unsqueeze(0)  # (1,1,T,T) broadcastable\n",
        "\n",
        "\n",
        "# ---------- data ----------\n",
        "def make_split(n):\n",
        "    xy = torch.randint(0, 100, (n, 2)) \n",
        "    seq = torch.full((n, SEQ), THINK)\n",
        "    seq[:, :2], seq[:, 3:] = xy, xy\n",
        "    return seq\n",
        "\n",
        "\n",
        "train_ds = TensorDataset(make_split(10_000))\n",
        "val_ds = TensorDataset(make_split(2_000))\n",
        "train_dl = DataLoader(train_ds, 128, shuffle=True, drop_last=True)\n",
        "val_dl = DataLoader(val_ds, 256, drop_last=False)\n",
        "\n",
        "# print(train_ds[0][0])  # Example sequence: [d1, d2, THINK, d1, d2]\n",
        "\n",
        "\n",
        "# ---------- config helper ----------\n",
        "def mk_cfg(d_model, ln):\n",
        "    return HookedTransformerConfig(\n",
        "        n_layers=2,\n",
        "        n_heads=1,\n",
        "        d_model=d_model,\n",
        "        d_head=d_model,\n",
        "        n_ctx=SEQ,\n",
        "        d_vocab=VOCAB,\n",
        "        attn_only=True,\n",
        "        act_fn=None,\n",
        "        normalization_type=(\"LN\" if ln else None),\n",
        "    )\n",
        "\n",
        "\n",
        "# ---------- utilities ----------\n",
        "def apply_mask_to_scores(scores, hook):\n",
        "    # scores: (batch, heads, Q, K)\n",
        "    return scores + mask_bias.to(scores.device)\n",
        "\n",
        "\n",
        "def strip_bias(m):\n",
        "    for mod in m.modules():\n",
        "        if hasattr(mod, \"bias\") and mod.bias is not None:\n",
        "            mod.bias.requires_grad_(False)\n",
        "            torch.nn.init.zeros_(mod.bias)\n",
        "\n",
        "\n",
        "def accuracy(m):\n",
        "    m.eval()\n",
        "    hits = tots = 0\n",
        "    with torch.no_grad():\n",
        "        for (seq,) in val_dl:\n",
        "            logits = m(seq.to(DEV))[:, 3:5]  # (batch, 2, vocab)\n",
        "            preds = logits.argmax(-1)\n",
        "            hits += (preds == seq[:, 3:5].to(DEV)).sum().item()\n",
        "            tots += preds.numel()\n",
        "    return hits / tots\n",
        "\n",
        "\n",
        "def train(m, max_steps=3000):\n",
        "    opt, ce = torch.optim.AdamW(m.parameters(), 1e-3), torch.nn.CrossEntropyLoss()\n",
        "    dl = itertools.cycle(train_dl)  # infinite iterator\n",
        "    for step in trange(max_steps, desc=\"Training\"):\n",
        "        (seq,) = next(dl)\n",
        "        logits = m(seq.to(DEV))[:, 3:5].reshape(-1, VOCAB)\n",
        "        loss = ce(logits, seq[:, 3:5].reshape(-1).to(DEV))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        # if (step + 1) % 100 == 0 and accuracy(m) > 0.999:\n",
        "        #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving model to device:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|█         | 308/3000 [00:01<00:16, 164.88it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m keep_bias:\n\u001b[32m     22\u001b[39m         strip_bias(model)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     rows.append(\n\u001b[32m     26\u001b[39m         \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     27\u001b[39m             model=name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m         )\n\u001b[32m     33\u001b[39m     )\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(pd.DataFrame(rows).to_markdown(index=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(m, max_steps)\u001b[39m\n\u001b[32m     80\u001b[39m loss = ce(logits, seq[:, \u001b[32m3\u001b[39m:\u001b[32m5\u001b[39m].reshape(-\u001b[32m1\u001b[39m).to(DEV))\n\u001b[32m     81\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m opt.zero_grad()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/torch/optim/adam.py:405\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight_decay != \u001b[32m0\u001b[39m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoupled_weight_decay:\n\u001b[32m    404\u001b[39m         \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m         \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    407\u001b[39m         \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weight_decay, Tensor):\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# # ---------- experiment grid ----------\n",
        "# specs = [\n",
        "#     (\"d32_ln_bias\", 32, True, True),\n",
        "#     (\"d32_noLN\", 32, False, True),\n",
        "#     (\"d32_noBias\", 32, True, False),\n",
        "#     (\"d16\", 16, True, True),\n",
        "#     (\"d8\", 8, True, True),\n",
        "#     (\"d4\", 4, True, True),\n",
        "# ]\n",
        "\n",
        "# rows = []\n",
        "# for name, d, ln, keep_bias in specs:\n",
        "#     model = HookedTransformer(mk_cfg(d, ln)).to(DEV)\n",
        "\n",
        "#     # register the same mask hook on every layer\n",
        "#     for l in range(model.cfg.n_layers):\n",
        "#         model.blocks[l].attn.hook_attn_scores.add_hook(\n",
        "#             apply_mask_to_scores, dir=\"fwd\"\n",
        "#         )\n",
        "\n",
        "#     if not keep_bias:\n",
        "#         strip_bias(model)\n",
        "\n",
        "#     train(model)\n",
        "#     rows.append(\n",
        "#         dict(\n",
        "#             model=name,\n",
        "#             d_model=d,\n",
        "#             use_ln=ln,\n",
        "#             bias=keep_bias,\n",
        "#             val_acc=round(accuracy(model), 4),\n",
        "#         )\n",
        "#     )\n",
        "\n",
        "# print(pd.DataFrame(rows).to_markdown(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving model to device:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 3000/3000 [00:16<00:00, 184.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final accuracy: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# The smallest size that gets perfect accuracy is width = 8, and we don't need\n",
        "# the MLP layers.\n",
        "\n",
        "model = HookedTransformer(mk_cfg(8, True)).to(DEV)\n",
        "for l in range(model.cfg.n_layers):\n",
        "    model.blocks[l].attn.hook_attn_scores.add_hook(apply_mask_to_scores, dir=\"fwd\")\n",
        "train(model)\n",
        "print(\"Final accuracy:\", accuracy(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ no attention leakage onto x₁/x₂\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# We confirm below that the model does not leak attention onto the first two\n",
        "# tokens, which are the inputs to the task. The model should only attend to the\n",
        "# first two tokens when predicting the third token, and not attend to them at all\n",
        "# when predicting the fourth and fifth tokens.\n",
        "\n",
        "def check_attention(m, dataloader, eps=1e-3):\n",
        "    for (seq,) in dataloader:\n",
        "        with torch.no_grad():\n",
        "            _, cache = m.run_with_cache(seq.to(DEV))\n",
        "        for l in range(m.cfg.n_layers):\n",
        "            pat = cache[\"pattern\", l][:, 0]  # (batch, Q, K)\n",
        "            leak = pat[:, 3:, :2].sum(dim=-1)  # mass on forbidden keys\n",
        "            if (leak > eps).any():\n",
        "                raise ValueError(\n",
        "                    f\"Layer {l}: output tokens attend to x₁/x₂ by >{eps:.0e}\"\n",
        "                )\n",
        "    print(\"✓ no attention leakage onto x₁/x₂\")\n",
        "\n",
        "\n",
        "sample = torch.tensor([[12, 34, THINK, 12, 34]], device=DEV)\n",
        "_, cache = model.run_with_cache(sample)\n",
        "\n",
        "fig, axes = plt.subplots(1, model.cfg.n_layers, figsize=(6, 3))\n",
        "if model.cfg.n_layers == 1:\n",
        "    axes = [axes]\n",
        "for l in range(model.cfg.n_layers):\n",
        "    pat = cache[\"pattern\", l][0, 0].cpu()  # (5,5)\n",
        "    ax = axes[l]\n",
        "    im = ax.imshow(pat, cmap=\"viridis\", vmin=0, vmax=1)\n",
        "    ax.set_title(f\"Layer {l} Attention Pattern\")\n",
        "    ax.set_xlabel(\"Key Position\")\n",
        "    ax.set_ylabel(\"Query Position\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "check_attention(model, val_dl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0: all attention patterns identical? False\n",
            "Layer 1: all attention patterns identical? False\n",
            "Accuracy with avg-attn: 0.9995\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# The attention patterns are not the same across inputs. However, we can replace\n",
        "# the attention scores with their average and still get almost perfect\n",
        "# performance.\n",
        "\n",
        "# %%\n",
        "\n",
        "all_pats = [[] for _ in range(model.cfg.n_layers)]\n",
        "for (seq,) in val_dl:\n",
        "    with torch.no_grad():\n",
        "        _, cache = model.run_with_cache(seq.to(DEV))\n",
        "    for l in range(model.cfg.n_layers):\n",
        "        pat = cache[\"pattern\", l][:, 0]  # (batch, Q, K)\n",
        "        all_pats[l].append(pat)\n",
        "all_pats = [torch.cat(pats, dim=0) for pats in all_pats]\n",
        "\n",
        "for l, pats in enumerate(all_pats):\n",
        "    identical = torch.allclose(pats, pats[0].expand_as(pats))\n",
        "    print(f\"Layer {l}: all attention patterns identical? {identical}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    avg_pats = [\n",
        "        torch.zeros(SEQ, SEQ, device=DEV) for _ in range(model.cfg.n_layers)\n",
        "    ]\n",
        "    n = 0\n",
        "    for (seq,) in val_dl:\n",
        "        _, cache = model.run_with_cache(seq.to(DEV))\n",
        "        for l in range(model.cfg.n_layers):\n",
        "            avg_pats[l] += cache[\"pattern\", l][:, 0].sum(0)\n",
        "        n += seq.shape[0]\n",
        "    avg_pats = [p / n for p in avg_pats]\n",
        "\n",
        "\n",
        "def mk_hook(avg):\n",
        "    logits = (avg + 1e-12).log()  # log-prob so softmax≈avg, ε avoids -∞\n",
        "\n",
        "    def f(scores, hook):\n",
        "        return logits.unsqueeze(0).unsqueeze(0).expand_as(scores)\n",
        "\n",
        "    return f\n",
        "\n",
        "for l in range(model.cfg.n_layers):\n",
        "    model.blocks[l].attn.hook_attn_scores.add_hook(\n",
        "        mk_hook(avg_pats[l]), dir=\"fwd\"\n",
        "    )\n",
        "\n",
        "print(\"Accuracy with avg-attn:\", accuracy(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "SPECIAL = THINK\n",
        "device = DEV\n",
        "SEQ_LEN = SEQ\n",
        "\n",
        "sample_list = [8, 3] # Define a sample input list to visualize\n",
        "\n",
        "# The original code used a sample token for ablation. We'll use it again for comparison.\n",
        "full_sequence = sample_list + [SPECIAL] + sample_list\n",
        "sample_tokens = torch.tensor(full_sequence, device=device).unsqueeze(0) # Add batch dim\n",
        "\n",
        "head_index_to_ablate = 0 # fixed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHGmCN_7Ug-_"
      },
      "outputs": [],
      "source": [
        "# embedding and unembedding matrix w/ pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "2s3bU9nBiHYa",
        "outputId": "ba19d926-2819-456f-accb-4218b4f6216c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformer_lens.ActivationCache.ActivationCache'>\n",
            "ActivationCache with keys ['blocks.1.attn.hook_pattern']\n",
            "torch.Size([1, 5, 5])\n",
            "[[[1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            "  [1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            "  [0.53436 0.46564 0.00000 0.00000 0.00000]\n",
            "  [0.00000 0.00000 1.00000 0.00000 0.00000]\n",
            "  [0.00000 0.00000 0.73838 0.26162 0.00000]]]\n",
            "Layer 1 Head Attention Patterns:\n",
            "Generating attention heatmap...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                    <div id=\"a24e44cd-a92f-45e9-87c8-9a8dd56c958c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"a24e44cd-a92f-45e9-87c8-9a8dd56c958c\")) {                    Plotly.newPlot(                        \"a24e44cd-a92f-45e9-87c8-9a8dd56c958c\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"d1(8)\",\"d2(3)\",\"SEP\",\"o1(8)\",\"o2(3)\"],\"y\":[\"d1(8)\",\"d2(3)\",\"SEP\",\"o1(8)\",\"o2(3)\"],\"z\":{\"dtype\":\"f4\",\"bdata\":\"AACAP8i8jCvIvIwryLyMK8i8jCsAAIA\\u002fyLyMK8i8jCvIvIwryLyMK6PLCD+5aO4+wryMK8K8jCvCvIwryLyMK8i8jCsAAIA\\u002fyLyMK8i8jCvDvIwrw7yMKyYGPT+084U+w7yMKw==\",\"shape\":\"5, 5\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Key (Memory): %{x}\\u003cbr\\u003eQuery (Current Token): %{y}\\u003cbr\\u003eAttention Weight: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Key (Memory)\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Query (Current Token)\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Attention Weight\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"title\":{\"text\":\"Attention Pattern for Layer 1\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a24e44cd-a92f-45e9-87c8-9a8dd56c958c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };            </script>        </div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# attn pattern = after softmax\n",
        "\n",
        "token_labels = [f\"d{i+1}({d})\" for i, d in enumerate(sample_list)] + \\\n",
        "               [\"SEP\"] + \\\n",
        "               [f\"o{i+1}({d})\" for i, d in enumerate(sample_list)]\n",
        "\n",
        "attn_layer = 1\n",
        "attn_hook_name = \"blocks.\"+str(attn_layer)+\".attn.hook_pattern\"\n",
        "logits, attn_cache = model.run_with_cache(sample_tokens, remove_batch_dim=True, stop_at_layer=attn_layer+1, names_filter=[attn_hook_name])\n",
        "attn = attn_cache[attn_hook_name]\n",
        "\n",
        "print(type(attn_cache))\n",
        "print(attn_cache)\n",
        "attention_pattern = attn_cache[\"pattern\", attn_layer, \"attn\"]\n",
        "print(attention_pattern.shape)\n",
        "print(attention_pattern.cpu().numpy())\n",
        "\n",
        "print(\"Layer \"+ str(attn_layer) + \" Head Attention Patterns:\")\n",
        "#  Remove the batch and head dimensions to get a 2D matrix for plotting.\n",
        "attention_pattern_2d = attention_pattern.squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "print(\"Generating attention heatmap...\")\n",
        "fig = px.imshow(\n",
        "    attention_pattern_2d,\n",
        "    x=token_labels,\n",
        "    y=token_labels,\n",
        "    color_continuous_scale='Viridis',\n",
        "    labels=dict(x=\"Key (Memory)\", y=\"Query (Current Token)\", color=\"Attention Weight\"),\n",
        "    title=f\"Attention Pattern for Layer {attn_layer}\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "eecb365d597549e88e049765d0ff072e"
          ]
        },
        "id": "S4WGF_StLmde",
        "outputId": "63c26772-33f2-48ad-ba9f-b4078f8f8b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating the mean attention pattern for L0H0...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e55808ef4d848ccbefba759bb637849",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'to'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sample_tokens_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader_for_avg):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         sample_tokens_batch = \u001b[43msample_tokens_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m     22\u001b[39m         \u001b[38;5;66;03m# Run the model and cache the attention patterns\u001b[39;00m\n\u001b[32m     23\u001b[39m         _, cache = model.run_with_cache(\n\u001b[32m     24\u001b[39m             sample_tokens_batch,\n\u001b[32m     25\u001b[39m             names_filter=[attn_pattern_hook_name]\n\u001b[32m     26\u001b[39m         )\n",
            "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- Step 1: Calculate the Mean Attention Pattern ---\n",
        "\n",
        "layer_to_ablate = 0\n",
        "\n",
        "# We need a dataset to calculate the average over\n",
        "dataset_for_avg = DigitDataset(2000) # Use a few thousand samples for a stable average\n",
        "dataloader_for_avg = DataLoader(dataset_for_avg, batch_size=128)\n",
        "\n",
        "# Get the correct hook name for the attention pattern\n",
        "attn_pattern_hook_name = utils.get_act_name(\"pattern\", layer_to_ablate)\n",
        "\n",
        "# Initialize a tensor to store the sum of attention patterns\n",
        "# Shape: [query_pos, key_pos]\n",
        "accumulated_patterns = torch.zeros((SEQ_LEN, SEQ_LEN), device=device)\n",
        "num_samples = 0\n",
        "\n",
        "print(\"Calculating the mean attention pattern for L0H0...\")\n",
        "# Loop over the dataset without tracking gradients\n",
        "with torch.no_grad():\n",
        "    for sample_tokens_batch in tqdm(dataloader_for_avg):\n",
        "        sample_tokens_batch = sample_tokens_batch.to(device)\n",
        "        # Run the model and cache the attention patterns\n",
        "        _, cache = model.run_with_cache(\n",
        "            sample_tokens_batch,\n",
        "            names_filter=[attn_pattern_hook_name]\n",
        "        )\n",
        "        # Get the patterns for the specific head\n",
        "        # Shape: [batch, head_index, query_pos, key_pos]\n",
        "        patterns_batch = cache[attn_pattern_hook_name][:, head_index_to_ablate, :, :]\n",
        "        \n",
        "        # Add the patterns to our accumulator\n",
        "        accumulated_patterns += patterns_batch.sum(dim=0)\n",
        "        num_samples += len(sample_tokens_batch)\n",
        "\n",
        "# Calculate the mean by dividing by the total number of samples\n",
        "mean_pattern = accumulated_patterns / num_samples\n",
        "\n",
        "print(f\"Mean pattern calculated: \\n{mean_pattern.numpy()}\\n\")\n",
        "print(f\"Original pattern: \\n{attention_pattern_2d}\\n\")\n",
        "\n",
        "# --- Step 2: Define the Hook and Run the Mean Ablation Experiment ---\n",
        "\n",
        "# This new hook replaces the current attention pattern with the mean pattern\n",
        "def mean_ablation_hook(\n",
        "    pattern, # Float[torch.Tensor, \"batch head_index query_pos key_pos\"]\n",
        "    hook,\n",
        "):\n",
        "    # Replace the pattern for the target head with our pre-calculated mean pattern\n",
        "    # The mean_pattern is [query_pos, key_pos], we broadcast it across the batch dimension\n",
        "    pattern[:, head_index_to_ablate, :, :] = mean_pattern \n",
        "    return pattern\n",
        "\n",
        "# Get original loss (no ablation)\n",
        "original_loss = model(sample_tokens, return_type=\"loss\")\n",
        "\n",
        "# Run the model with the new mean ablation hook\n",
        "mean_ablated_loss = model.run_with_hooks(\n",
        "    sample_tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[(\n",
        "        attn_pattern_hook_name, # Hook the attention PATTERN this time\n",
        "        mean_ablation_hook\n",
        "    )]\n",
        ")\n",
        "\n",
        "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
        "print(f\"Ablated Loss (Using Mean Attention Pattern): {mean_ablated_loss.item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss is similar when using the mean attention pattern (over 2000 diff random 2-lists) on the sample list. This suggests that the individual digits themselves don't matter for accuracy!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Invalid return_type passed in: accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1 Ablation\n",
            "BEFORE: \n",
            "[[[1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            "  [1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            "  [0.58187 0.41813 0.00000 0.00000 0.00000]\n",
            "  [0.00000 0.00000 1.00000 0.00000 0.00000]\n",
            "  [0.00000 0.00000 0.91226 0.08774 0.00000]]]\n",
            "AFTER: \n",
            "[[[0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            "  [0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            "  [0.58187 0.00000 0.00000 0.00000 0.00000]\n",
            "  [0.00000 0.00000 1.00000 0.00000 0.00000]\n",
            "  [0.00000 0.00000 0.00000 0.00000 0.00000]]]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'item'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     30\u001b[39m original_loss = model(sample_tokens, return_type=\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m ablated_loss = model.run_with_hooks(\n\u001b[32m     32\u001b[39m     sample_tokens,\n\u001b[32m     33\u001b[39m     return_type=\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     )]\n\u001b[32m     38\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moriginal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAblated Loss (zeroing attention): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mablated_loss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'item'"
          ]
        }
      ],
      "source": [
        "# try setting specific attention positions to zero\n",
        "layer_to_ablate = 1 # output digits do nothijg in layer 0\n",
        "print(f\"Layer {layer_to_ablate} Ablation\")\n",
        "# Define which specific attention position you want to zero out\n",
        "query_pos_to_zero, key_pos_to_zero = [4,2] # (top left = [0,0]. query is the row, key is the column)\n",
        "\n",
        "def specific_attention_ablation_hook(\n",
        "    pattern, # Shape: [batch, head_index, query_pos, key_pos]\n",
        "    hook\n",
        "):\n",
        "    # print(f\"Original attention at [{query_pos_to_zero}, {key_pos_to_zero}]: {pattern[0, head_index_to_ablate, query_pos_to_zero, key_pos_to_zero].item():.4f}\")\n",
        "    \n",
        "    # Set specific attention weight to 0\n",
        "    with torch.no_grad():\n",
        "        print(f'BEFORE: \\n{pattern[:, head_index_to_ablate, :, :].cpu().numpy()}')\n",
        "\n",
        "        pattern[:, head_index_to_ablate, query_pos_to_zero, :] = 0.0\n",
        "        pattern[:, head_index_to_ablate, 3, 3] = 0.0\n",
        "        pattern[:, head_index_to_ablate, 0:2, :] = 0.0\n",
        "        pattern[:, head_index_to_ablate, 2, 1:] = 0.0\n",
        "        \n",
        "        print(f'AFTER: \\n{pattern[:, head_index_to_ablate, :, :].cpu().numpy()}')\n",
        "    \n",
        "    return pattern\n",
        "\n",
        "# Get the attention pattern hook name\n",
        "attn_pattern_hook_name = utils.get_act_name(\"pattern\", layer_to_ablate)\n",
        "\n",
        "# Run with the specific position ablated\n",
        "original_loss = model(sample_tokens, return_type=\"loss\")\n",
        "ablated_loss = model.run_with_hooks(\n",
        "    sample_tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[(\n",
        "        attn_pattern_hook_name,\n",
        "        specific_attention_ablation_hook\n",
        "    )]\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
        "print(f\"Ablated Loss (zeroing attention): {ablated_loss.item():.3f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using hook names: blocks.0.hook_resid_post and blocks.1.hook_resid_post\n",
            "8\n",
            "47\n",
            "45\n",
            "8\n",
            "3\n",
            "--- Verifying Prediction from Intermediate Residual Streams ---\n",
            "Sample list: [8, 3], Target final token: 3\n",
            "--------------------------------------------------\n",
            "Prediction using resid_post_L0 @ W_U: 33\n",
            "Is it correct? ❌\n",
            "--------------------------------------------------\n",
            "Prediction using resid_post_L1 @ W_U (final output): 3\n",
            "Is it correct? ✅ (obviously)\n"
          ]
        }
      ],
      "source": [
        "# \"resid after layer 0” @ W_U and see if that gives you the last token\n",
        "\n",
        "# The hook name for the residual stream after layer 0\n",
        "resid_l0_hook_name = utils.get_act_name(\"resid_post\", 0)\n",
        "# The hook name for the residual stream after layer 1 (the final one)\n",
        "resid_l1_hook_name = utils.get_act_name(\"resid_post\", 1)\n",
        "print(f\"Using hook names: {resid_l0_hook_name} and {resid_l1_hook_name}\")\n",
        "\n",
        "# The position from which the final prediction is made is the last one in sample_tokens[:, :-1]\n",
        "prediction_pos = -1\n",
        "# The actual target token\n",
        "target_token = sample_tokens[0, -1].item() # 3\n",
        "\n",
        "# Run the model and cache the activations from both layers\n",
        "with torch.no_grad():\n",
        "    _, cache = model.run_with_cache(\n",
        "        sample_tokens,\n",
        "        names_filter=[\n",
        "            resid_l0_hook_name, \n",
        "            resid_l1_hook_name\n",
        "            ]\n",
        "    )\n",
        "    \n",
        "    # Get the residual stream at the prediction position from after L0\n",
        "    resid_l0 = cache[resid_l0_hook_name][0, prediction_pos, :]\n",
        "    # Get the residual stream at the prediction position from after L1\n",
        "    resid_l1 = cache[resid_l1_hook_name][0, prediction_pos, :]\n",
        "\n",
        "    # Get the unembedding matrix\n",
        "    W_U = model.W_U\n",
        "\n",
        "    all_resid = cache[resid_l1_hook_name][0]\n",
        "    for e in all_resid:\n",
        "        print((e @ W_U).argmax().item())\n",
        "\n",
        "    # Manually calculate logits by multiplying the residual stream by the unembedding matrix\n",
        "    logits_from_l0 = resid_l0 @ W_U\n",
        "    logits_from_l1 = resid_l1 @ W_U\n",
        "\n",
        "    # print(f\"Logits from L0: {logits_from_l0.cpu().numpy()}\")\n",
        "    # print(f\"Logits from L1: {logits_from_l1.cpu().numpy()}\") # len 11 because vocab size is 11 (0-9 & special token)\n",
        "\n",
        "    # Get the predicted token index from the logits\n",
        "    prediction_from_l0 = logits_from_l0.argmax().item()\n",
        "    prediction_from_l1 = logits_from_l1.argmax().item()\n",
        "\n",
        "\n",
        "print(f\"--- Verifying Prediction from Intermediate Residual Streams ---\")\n",
        "print(f\"Sample list: {sample_list}, Target final token: {target_token}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Prediction using resid_post_L0 @ W_U: {prediction_from_l0}\")\n",
        "print(f\"Is it correct? {'✅' if prediction_from_l0 == target_token else '❌'}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Prediction using resid_post_L1 @ W_U (final output): {prediction_from_l1}\")\n",
        "print(f\"Is it correct? {'✅ (obviously)' if prediction_from_l1 == target_token else '❌'}\")\n",
        "\n",
        "\n",
        "# so the first layer isnt enough to get final token\n",
        "# but the 2nd layer doesn't need the last attn row\n",
        "# ==> how can it predict the final token without the last attention row?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJUVmN31U9Z-"
      },
      "outputs": [],
      "source": [
        "# neels grokking paper - formula from model weights that represents\n",
        "# but actual d_i are arbitrary\n",
        "\n",
        "# TODO\n",
        "# attn pattern (try and break it - means, rand values), neels paper - formula ideas, embed/unembed matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHamZW-IComj",
        "outputId": "4b178d3b-af70-4cae-f9b1-948004f49122"
      },
      "outputs": [],
      "source": [
        "# # The position of the separator token, which acts as our compression point\n",
        "# COMPRESSION_POS = LIST_LEN\n",
        "\n",
        "# # The name of the hook point for the residual stream after Layer 1\n",
        "# COMPRESSION_HOOK_NAME = utils.get_act_name(\"resid_post\", 1)\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def get_compressed_representation(model, digits_list):\n",
        "#     \"\"\"\n",
        "#     Runs the model on a list of digits and returns the activation\n",
        "#     at the compression point.\n",
        "#     \"\"\"\n",
        "#     # The input to the model should not contain the second list of digits\n",
        "#     # as the compression happens before generation.\n",
        "#     tokens = torch.tensor([digits_list + [SPECIAL]], device=device)\n",
        "#     _, cache = model.run_with_cache(tokens, names_filter=[COMPRESSION_HOOK_NAME])\n",
        "#     # Get the activation: [batch, position, d_model] -> [d_model]\n",
        "#     compressed_vector = cache[COMPRESSION_HOOK_NAME][0, COMPRESSION_POS].cpu()\n",
        "#     return compressed_vector\n",
        "\n",
        "# # Generate a dataset of compressed representations\n",
        "# num_samples = 2000\n",
        "# compressed_vectors = []\n",
        "# all_digit_lists = [] # Store original lists for hover text\n",
        "# labels = [] # Will store the sum of digits\n",
        "# print(f\"Generating {num_samples} samples...\")\n",
        "# for _ in range(num_samples):\n",
        "#     digit_list = [random.randint(0, 9) for _ in range(LIST_LEN)]\n",
        "#     all_digit_lists.append(digit_list)\n",
        "#     vec = get_compressed_representation(model, digit_list)\n",
        "#     compressed_vectors.append(vec.numpy())\n",
        "#     labels.append(sum(digit_list)) # Label is the sum of the digits\n",
        "\n",
        "# compressed_vectors = np.array(compressed_vectors)\n",
        "\n",
        "# # Use PCA to find the 3 most important dimensions\n",
        "# print(\"Running PCA...\")\n",
        "# pca = PCA(n_components=3)\n",
        "# compressed_pca = pca.fit_transform(compressed_vectors)\n",
        "\n",
        "# # --- 4. VISUALIZATION ---\n",
        "\n",
        "# print(\"Generating plot...\")\n",
        "\n",
        "# # Create a figure for the 3D plot\n",
        "# fig = go.Figure()\n",
        "\n",
        "# # Add the scatter plot of the compressed list vectors\n",
        "# fig.add_trace(go.Scatter3d(\n",
        "#     x=compressed_pca[:, 0],\n",
        "#     y=compressed_pca[:, 1],\n",
        "#     z=compressed_pca[:, 2],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=3,\n",
        "#         color=labels, # Color by the sum of digits\n",
        "#         colorscale='Turbo', # A nice rainbow colorscale\n",
        "#         opacity=0.7,\n",
        "#         colorbar=dict(title='Sum of Digits in List'),\n",
        "#     ),\n",
        "#     name='Compressed Lists',\n",
        "#     # Add informative hover text\n",
        "#     hovertext=[f'List: {dl}<br>Sum: {s}' for dl, s in zip(all_digit_lists, labels)],\n",
        "#     hoverinfo='text'\n",
        "# ))\n",
        "\n",
        "# # Update layout for clarity\n",
        "# fig.update_layout(\n",
        "#     title='Structure of the Compressed Representation (Colored by Sum)',\n",
        "#     scene=dict(\n",
        "#         xaxis_title='Principal Component 1',\n",
        "#         yaxis_title='Principal Component 2',\n",
        "#         zaxis_title='Principal Component 3'\n",
        "#     ),\n",
        "#     margin=dict(r=20, b=10, l=10, t=40)\n",
        "# )\n",
        "\n",
        "# fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
