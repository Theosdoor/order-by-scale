{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Theosdoor/list-comp/blob/main/list_comp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path_prefix = '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "id": "tc28huVLY_Jy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "9c6fe1c4-4ea9-4c7d-dc2c-2cd8d3dba214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1721290347.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msave_path_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "collapsed": true,
        "id": "bMlWaAkH_Gsn"
      },
      "outputs": [],
      "source": [
        "# Install if in Colab\n",
        "%pip install transformer_lens --upgrade\n",
        "%pip install circuitsvis\n",
        "# Install a faster Node version\n",
        "!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3H1R__HCc54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch, random\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "# from mpl_toolkits.mplot3d import Axes3D\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# added by gemini\n",
        "from transformer_lens import utils\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdUT-1P-_Gsu"
      },
      "source": [
        "How does this model work?\n",
        "\n",
        "super basic two layer transformer with no MLP or even a value matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlhNtRpvCh8v"
      },
      "outputs": [],
      "source": [
        "DIGITS = list(range(10))\n",
        "LIST_LEN = 2\n",
        "SPECIAL = 10\n",
        "VOCAB = 11\n",
        "SEQ_LEN = LIST_LEN * 2 + 1\n",
        "D_MODEL = 16\n",
        "N_HEAD = 1\n",
        "N_LAYER = 2 # 2 layers each with single attn head\n",
        "FREEZE_WV = True # no value matrix in attn (i.e. attn head can only copy inputs to outputs)\n",
        "MODEL_PATH = \"artifacts/len_2.pt\"\n",
        "\n",
        "class DigitDataset(Dataset):\n",
        "    def __init__(self, n):\n",
        "        self.data = [[random.randint(0, 9) for _ in range(LIST_LEN)] for _ in range(n)]\n",
        "        # come up with 'size' lots of sequences of random digits (each seq len 5)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # [d1, d2, d3, d4, d5, 10, d1, d2, d3, d4, d5], where 10 is a special separator token\n",
        "        seq = self.data[idx]\n",
        "        tok = seq + [SPECIAL] + seq\n",
        "        return torch.tensor(tok, dtype=torch.long)\n",
        "\n",
        "def build_mask(n: int) -> torch.Tensor:\n",
        "    # create attention pattern for a sequence of length n\n",
        "    # rows are queries, columns are keys\n",
        "    # float(-inf) means \"ignore this position\" (i.e. becomes 0 in later softmax - see 3b1b video)\n",
        "    m = torch.triu(torch.ones(n, n) * float(\"-inf\"), 1) # prevents attending to future tokens\n",
        "    m[LIST_LEN+1:, :LIST_LEN] = float(\"-inf\")\n",
        "    # m[LIST_LEN+1:] = all tokens after the special token (i.e. queries of output tokens)\n",
        "    # m[LIST_LEN+1:, :LIST_LEN] = refers to all keys of tokens before special token\n",
        "    #  ==> this line explicitly forbids the output tokens (when they are the query) from attending to the input tokens (when they are the key).\n",
        "\n",
        "    # attention mask for [d1, d2, d3, d4, d5, 10, d1, d2, d3, d4, d5] looks like this (query rows are horizontal, key columns are vertical):\n",
        "    # -   d1    d2    d3    d4    d5    10    d1    d2    d3    d4    d5  (keys)\n",
        "    # d1  0    -inf   -inf  -inf  -inf  -inf  -inf  -inf  -inf  -inf  -inf\n",
        "    # d2  0      0    -inf  -inf  -inf  -inf  -inf  -inf  -inf  -inf  -inf\n",
        "    # d3  0      0     0    -inf  -inf  -inf  -inf  -inf  -inf  -inf  -inf\n",
        "    # d4  0      0     0      0   -inf  -inf  -inf  -inf  -inf  -inf  -inf\n",
        "    # d5  0      0     0      0     0   -inf  -inf  -inf  -inf  -inf  -inf\n",
        "    # 10  0      0     0      0     0     0   -inf  -inf  -inf  -inf  -inf\n",
        "    # d1  -inf   -inf  -inf  -inf  -inf   0     0   -inf  -inf  -inf  -inf\n",
        "    # d2  -inf   -inf  -inf  -inf  -inf   0     0     0   -inf  -inf  -inf\n",
        "    # d3  -inf   -inf  -inf  -inf  -inf   0     0     0     0   -inf  -inf\n",
        "    # d4  -inf   -inf  -inf  -inf  -inf   0     0     0     0     0   -inf\n",
        "    # d5  -inf   -inf  -inf  -inf  -inf   0     0     0     0     0     0\n",
        "    # (queries)\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def make_model(device: str = \"cuda\") -> \"HookedTransformer\":\n",
        "    cfg = HookedTransformerConfig(\n",
        "        d_model=D_MODEL,\n",
        "        d_head=D_MODEL // N_HEAD,\n",
        "        n_layers=N_LAYER,\n",
        "        n_heads=N_HEAD,\n",
        "        n_ctx=SEQ_LEN,\n",
        "        d_vocab=VOCAB,\n",
        "        d_vocab_out=VOCAB,\n",
        "        attn_only=True, # no MLP!\n",
        "    )\n",
        "    model = HookedTransformer(cfg).to(device)\n",
        "    if FREEZE_WV:\n",
        "        set_WV_identity_and_freeze(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "def attach_custom_mask(model: \"HookedTransformer\") -> None:\n",
        "    def _mask(scores, hook=None):\n",
        "        t = scores.size(-1)\n",
        "        scores += build_mask(t).to(scores.device)\n",
        "        return scores\n",
        "\n",
        "    for block in model.blocks:\n",
        "        block.attn.hook_attn_scores.add_perma_hook(_mask)\n",
        "\n",
        "\n",
        "def set_WV_identity_and_freeze(model: \"HookedTransformer\") -> None:\n",
        "    with torch.no_grad():\n",
        "        eye = torch.eye(D_MODEL).unsqueeze(0)  # add head dim\n",
        "        for block in model.blocks:\n",
        "            block.attn.W_V.copy_(eye)\n",
        "            block.attn.W_V.requires_grad = False\n",
        "\n",
        "\n",
        "def train(\n",
        "    epochs: int = 10,\n",
        "    batch: int = 1024,\n",
        "    size: int = 50000,\n",
        "    val: int = 1000,\n",
        "    device=\"cuda\",\n",
        ") -> HookedTransformer:\n",
        "    ds = DigitDataset(size)\n",
        "    dl = DataLoader(ds, batch, shuffle=True)\n",
        "    model = make_model(device)\n",
        "    attach_custom_mask(model)\n",
        "    opt = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    for _ in tqdm(range(epochs)):\n",
        "        for seq in dl:\n",
        "            seq = seq.to(device)\n",
        "            logits = model(seq[:, :-1])\n",
        "            loss = loss_fn(logits.reshape(-1, VOCAB), seq[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "        corr = 0\n",
        "        for _ in range(val):\n",
        "            d = [random.randint(0, 9) for _ in range(LIST_LEN)]\n",
        "            corr += generate(model, d) == d\n",
        "        print(f\"acc {corr / val:.2%}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model: HookedTransformer, digits: list[int]) -> list[int]:\n",
        "    seq = digits + [SPECIAL]\n",
        "    out: list[int] = []\n",
        "    for _ in range(LIST_LEN):\n",
        "        x = torch.tensor(seq + out, device=next(model.parameters()).device).unsqueeze(0)\n",
        "        nxt = model(x)[:, -1].argmax(-1).item()\n",
        "        out.append(nxt)\n",
        "    return out\n",
        "\n",
        "\n",
        "def save_model(model: HookedTransformer, path: str = MODEL_PATH):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "def load_model(\n",
        "    path: str = MODEL_PATH, device: str = \"cuda\"\n",
        ") -> HookedTransformer:\n",
        "    model = make_model(device)\n",
        "    model.load_state_dict(\n",
        "        torch.load(path, map_location=device)\n",
        "    )  # map weights to target device\n",
        "    attach_custom_mask(model)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# USAGE\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "path = save_path_prefix + MODEL_PATH\n",
        "if os.path.exists(path):\n",
        "    print(\"Loading model from\", path)\n",
        "    model = load_model(path, device)\n",
        "else:\n",
        "    print(\"Training model\")\n",
        "    model = train(epochs=20, device=device)\n",
        "    save_model(model, path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding and unembedding matrix w/ pca"
      ],
      "metadata": {
        "id": "oHGmCN_7Ug-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attn pattern = after softmax\n",
        "\n",
        "# Define a sample input list to visualize\n",
        "sample_list = [8, 3]\n",
        "full_sequence = sample_list + [SPECIAL] + sample_list\n",
        "tokens = torch.tensor(full_sequence, device=device).unsqueeze(0) # Add batch dim\n",
        "token_labels = [f\"d{i+1}({d})\" for i, d in enumerate(sample_list)] + \\\n",
        "               [\"SEP\"] + \\\n",
        "               [f\"o{i+1}({d})\" for i, d in enumerate(sample_list)]\n",
        "\n",
        "attn_layer = 0\n",
        "attn_hook_name = \"blocks.\"+str(attn_layer)+\".attn.hook_pattern\"\n",
        "logits, attn_cache = model.run_with_cache(tokens, remove_batch_dim=True, stop_at_layer=attn_layer+1, names_filter=[attn_hook_name])\n",
        "attn = attn_cache[attn_hook_name]\n",
        "\n",
        "print(type(attn_cache))\n",
        "print(attn_cache)\n",
        "attention_pattern = attn_cache[\"pattern\", attn_layer, \"attn\"]\n",
        "print(attention_pattern.shape)\n",
        "print(attention_pattern)\n",
        "\n",
        "print(\"Layer \"+ str(attn_layer) + \" Head Attention Patterns:\")\n",
        "#  Remove the batch and head dimensions to get a 2D matrix for plotting.\n",
        "attention_pattern_2d = attention_pattern.squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "print(\"Generating attention heatmap...\")\n",
        "fig = px.imshow(\n",
        "    attention_pattern_2d,\n",
        "    x=token_labels,\n",
        "    y=token_labels,\n",
        "    color_continuous_scale='Viridis',\n",
        "    labels=dict(x=\"Key (Memory)\", y=\"Query (Current Token)\", color=\"Attention Weight\"),\n",
        "    title=f\"Attention Pattern for Layer {attn_layer}\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2s3bU9nBiHYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ablation\n",
        "\n",
        "layer_to_ablate = 0\n",
        "head_index_to_ablate = 0 # fixed\n",
        "\n",
        "# We need a dataset to calculate the average over\n",
        "dataset_for_avg = DigitDataset(2000) # Use a few thousand samples for a stable average\n",
        "dataloader_for_avg = DataLoader(dataset_for_avg, batch_size=128)\n",
        "\n",
        "# Get the correct hook name for the attention pattern\n",
        "attn_pattern_hook_name = \"blocks.0.attn.hook_pattern\"\n",
        "\n",
        "def uniform_ablation_hook(\n",
        "    pattern, # Float[torch.Tensor, \"batch head_index query_pos key_pos\"]\n",
        "    hook,\n",
        "):\n",
        "    \"\"\"\n",
        "    Replaces the attention pattern of a target head with a uniform distribution\n",
        "    over the allowed (unmasked) keys.\n",
        "    \"\"\"\n",
        "    # Get the sub-tensor for the head we want to ablate\n",
        "    # Shape: [batch, query_pos, key_pos]\n",
        "    target_head_pattern = pattern[:, head_index_to_ablate, :, :]\n",
        "\n",
        "    # A key is valid if its original attention > 0 (i.e., it wasn't masked out by softmax)\n",
        "    # This creates a boolean mask of the valid connections\n",
        "    valid_keys_mask = target_head_pattern > 0\n",
        "\n",
        "    # For each query, count how many keys it can attend to.\n",
        "    # Add a small epsilon to prevent division by zero for queries that can't attend anywhere.\n",
        "    # Shape: [batch, query_pos, 1] (keepdim=True for broadcasting)\n",
        "    num_valid_keys = valid_keys_mask.sum(dim=-1, keepdim=True) + 1e-9\n",
        "\n",
        "    # Create the uniform pattern by dividing 1 by the number of valid keys.\n",
        "    # The mask (now float) ensures we only place values in allowed positions.\n",
        "    uniform_pattern = valid_keys_mask.float() / num_valid_keys\n",
        "\n",
        "    # Replace the original pattern for the target head with the new uniform one\n",
        "    print(\"Uniformly distributing attention for L0H0...\")\n",
        "    print(\"BEFORE\", pattern[:,  head_index_to_ablate,:, :])\n",
        "    pattern[:, head_index_to_ablate, :, :] = uniform_pattern\n",
        "    print(\"AFTER\", pattern[:, head_index_to_ablate, :, :])\n",
        "\n",
        "    return pattern\n",
        "\n",
        "# We define a head ablation hook\n",
        "# The type annotations are NOT necessary, they're just a useful guide to the reader\n",
        "\n",
        "def head_ablation_hook(\n",
        "    value, # Float[torch.Tensor, \"batch pos head_index d_head\"]\n",
        "    hook,\n",
        "): # -> Float[torch.Tensor, \"batch pos head_index d_head\"]\n",
        "    print(f\"Shape of the value tensor: {value.shape}\")\n",
        "    # print(\"BEFORE\", value[:, :, head_index_to_ablate, :])\n",
        "    value[:, :, head_index_to_ablate, :] = 0.\n",
        "    # print(\"AFTER\", value[:, :, head_index_to_ablate, :])\n",
        "    return value\n",
        "\n",
        "original_loss = model(tokens, return_type=\"loss\")\n",
        "# 2. Run the model with the new uniform ablation hook\n",
        "uniform_ablated_loss = model.run_with_hooks(\n",
        "    tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[(\n",
        "        attn_pattern_hook_name,\n",
        "        uniform_ablation_hook\n",
        "    )]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Ablation Results ---\")\n",
        "print(f\"✅ Original Loss: {original_loss.item():.3f}\")\n",
        "print(f\"➡️  Ablated Loss (Uniform Attention Pattern): {uniform_ablated_loss.item():.3f}\")"
      ],
      "metadata": {
        "id": "inq_xdytn24M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try ablating:\n",
        "# * set attn pattern to average attn pattern\n",
        "# * remove residual?\n",
        "# * keys & queries\n",
        "\n",
        "# neels grokking paper - formula from model weights that represents\n",
        "# but actual d_i are arbitrary\n",
        "\n",
        "\n",
        "# TODO\n",
        "# attn pattern (try and break it - means, rand values), neels paper - formula ideas, embed/unembed matrix"
      ],
      "metadata": {
        "id": "MJUVmN31U9Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "avt7BnLgUgiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHamZW-IComj"
      },
      "outputs": [],
      "source": [
        "# --- 3. CORE ANALYSIS: EXTRACTING & VISUALIZING THE REPRESENTATION ---\n",
        "\n",
        "# The position of the separator token, which acts as our compression point\n",
        "COMPRESSION_POS = LIST_LEN\n",
        "\n",
        "# The name of the hook point for the residual stream after Layer 1\n",
        "COMPRESSION_HOOK_NAME = utils.get_act_name(\"resid_post\", 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_compressed_representation(model, digits_list):\n",
        "    \"\"\"\n",
        "    Runs the model on a list of digits and returns the activation\n",
        "    at the compression point.\n",
        "    \"\"\"\n",
        "    # The input to the model should not contain the second list of digits\n",
        "    # as the compression happens before generation.\n",
        "    tokens = torch.tensor([digits_list + [SPECIAL]], device=device)\n",
        "    _, cache = model.run_with_cache(tokens, names_filter=[COMPRESSION_HOOK_NAME])\n",
        "    # Get the activation: [batch, position, d_model] -> [d_model]\n",
        "    compressed_vector = cache[COMPRESSION_HOOK_NAME][0, COMPRESSION_POS].cpu()\n",
        "    return compressed_vector\n",
        "\n",
        "# Generate a dataset of compressed representations\n",
        "num_samples = 2000\n",
        "compressed_vectors = []\n",
        "all_digit_lists = [] # Store original lists for hover text\n",
        "labels = [] # Will store the sum of digits\n",
        "print(f\"Generating {num_samples} samples...\")\n",
        "for _ in range(num_samples):\n",
        "    digit_list = [random.randint(0, 9) for _ in range(LIST_LEN)]\n",
        "    all_digit_lists.append(digit_list)\n",
        "    vec = get_compressed_representation(model, digit_list)\n",
        "    compressed_vectors.append(vec.numpy())\n",
        "    labels.append(sum(digit_list)) # Label is the sum of the digits\n",
        "\n",
        "compressed_vectors = np.array(compressed_vectors)\n",
        "\n",
        "# Use PCA to find the 3 most important dimensions\n",
        "print(\"Running PCA...\")\n",
        "pca = PCA(n_components=3)\n",
        "compressed_pca = pca.fit_transform(compressed_vectors)\n",
        "\n",
        "# --- 4. VISUALIZATION ---\n",
        "\n",
        "print(\"Generating plot...\")\n",
        "\n",
        "# Create a figure for the 3D plot\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the scatter plot of the compressed list vectors\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=compressed_pca[:, 0],\n",
        "    y=compressed_pca[:, 1],\n",
        "    z=compressed_pca[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=3,\n",
        "        color=labels, # Color by the sum of digits\n",
        "        colorscale='Turbo', # A nice rainbow colorscale\n",
        "        opacity=0.7,\n",
        "        colorbar=dict(title='Sum of Digits in List'),\n",
        "    ),\n",
        "    name='Compressed Lists',\n",
        "    # Add informative hover text\n",
        "    hovertext=[f'List: {dl}<br>Sum: {s}' for dl, s in zip(all_digit_lists, labels)],\n",
        "    hoverinfo='text'\n",
        "))\n",
        "\n",
        "# Update layout for clarity\n",
        "fig.update_layout(\n",
        "    title='Structure of the Compressed Representation (Colored by Sum)',\n",
        "    scene=dict(\n",
        "        xaxis_title='Principal Component 1',\n",
        "        yaxis_title='Principal Component 2',\n",
        "        zaxis_title='Principal Component 3'\n",
        "    ),\n",
        "    margin=dict(r=20, b=10, l=10, t=40)\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}