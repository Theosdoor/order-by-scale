{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "h3H1R__HCc54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.decomposition import PCA\n",
        "import umap\n",
        "\n",
        "import einops\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd, itertools\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n",
        "\n",
        "# Configure plotly to use static rendering if widgets fail\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"notebook\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "float_formatter = \"{:.5f}\".format\n",
        "np.set_printoptions(formatter={'float_kind':float_formatter})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf],\n",
            "        [0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf],\n",
            "        [-inf, -inf, 0., -inf, -inf],\n",
            "        [-inf, -inf, 0., 0., -inf]])\n"
          ]
        }
      ],
      "source": [
        "# ---------- constants ----------\n",
        "LIST_LEN = 2 # [d1, d2]\n",
        "SEQ_LEN = LIST_LEN * 2 + 1 # [d1, d2, SEP, o1, o2]\n",
        "\n",
        "N_DIGITS = 100\n",
        "DIGITS = list(range(N_DIGITS)) # 100 digits from 0 to 99\n",
        "SEP = DIGITS[-1] + 1 # special seperator token for the model to think about the input (+1 to avoid confusion with the last digit)\n",
        "VOCAB = len(DIGITS) + 1  # +1 for the special token\n",
        "\n",
        "D_MODEL = 32\n",
        "N_HEAD = 1\n",
        "N_LAYER = 2 # 2 layers each with single attn head\n",
        "USE_LN = True # use layer norm in model\n",
        "USE_BIAS = False # use bias in model\n",
        "FREEZE_WV = True # no value matrix in attn \n",
        "FREEZE_WO = True # no output matrix in attn (i.e. attn head can only copy inputs to outputs)\n",
        "\n",
        "TRAIN_SPLIT = 0.8 # 80% train, 20% test\n",
        "MAX_TRAIN_STEPS = 400_000 # max training steps\n",
        "\n",
        "# model name for saving and loading\n",
        "# MODEL_NAME = f'{N_DIGITS}dig_{D_MODEL}d'\n",
        "MODEL_NAME = f'v5_100dig_32d_FreezeWO_LNNoBias'\n",
        "MODEL_PATH = \"models/\" + MODEL_NAME + \".pt\"\n",
        "\n",
        "USE_CHECKPOINTING = True # whether to use checkpointing for training\n",
        "\n",
        "DEV = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        ")\n",
        "device = DEV\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# ---------- mask ----------\n",
        "# attention mask for [d1, d2, SEP, o1, o2] looks like this (query rows are horizontal, key columns are vertical):\n",
        "# -    d1    d2    SEP    o1    o2   (keys)\n",
        "# d1  -inf  -inf   -inf  -inf  -inf\n",
        "# d2   0    -inf   -inf  -inf  -inf\n",
        "# SEP  0      0    -inf  -inf  -inf\n",
        "# o1  -inf  -inf    0    -inf   -inf\n",
        "# o2  -inf  -inf    0      0    -inf\n",
        "# (queries)\n",
        "\n",
        "mask_bias = torch.triu(torch.ones(SEQ_LEN, SEQ_LEN) * float(\"-inf\")) # upper triangular bias mask (lead_diag & above = -inf, rest = 0)\n",
        "mask_bias[0, 0] = 0. # don't want a full row of -inf! otherwise we get nan erros & training breaks\n",
        "mask_bias[LIST_LEN+1:, :LIST_LEN] = float(\"-inf\") # stop output tokens from attending to input tokens\n",
        "mask_bias = mask_bias.unsqueeze(0).unsqueeze(0) # (1,1,T,T) broadcastable across batch and heads\n",
        "\n",
        "print(mask_bias.cpu()[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: tensor([ 60,  44, 100, 100, 100])\n",
            "Target: tensor([ 60,  44, 100,  60,  44])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(8000, 2000)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ---------- data ----------\n",
        "# Create all possible combinations of digits\n",
        "all_data = list(itertools.product(DIGITS, repeat=LIST_LEN))\n",
        "n_data = len(all_data)\n",
        "all_data = torch.tensor(all_data, dtype=torch.int64)\n",
        "\n",
        "# Create sequences of the form [d1, d2, SEP, d1, d2]\n",
        "all_targets = torch.full((n_data, SEQ_LEN), SEP)\n",
        "all_targets[:, :LIST_LEN] = all_data\n",
        "all_targets[:, LIST_LEN+1:] = all_data\n",
        "\n",
        "# Create input sequences of the form [d1, d2, SEP, SEP, SEP]\n",
        "all_inputs = all_targets.clone()\n",
        "all_inputs[:, LIST_LEN+1:] = SEP\n",
        "\n",
        "# Shuffle the dataset (inputs and targets together)\n",
        "perm = torch.randperm(n_data)\n",
        "all_inputs = all_inputs[perm]\n",
        "all_targets = all_targets[perm]\n",
        "\n",
        "train_ds = TensorDataset(all_inputs[:int(TRAIN_SPLIT*n_data)], all_targets[:int(TRAIN_SPLIT*n_data)])  # 80% for training\n",
        "val_ds = TensorDataset(all_inputs[int(TRAIN_SPLIT*n_data):], all_targets[int(TRAIN_SPLIT*n_data):])  # 20% for validation\n",
        "train_batch_size = min(128, len(train_ds))  # Use a batch size of 128 or less if dataset is smaller\n",
        "val_batch_size = min(256, len(val_ds))  # Use a batch size of 256 or less if dataset is smaller\n",
        "train_dl = DataLoader(train_ds, train_batch_size, shuffle=True, drop_last=True)\n",
        "val_dl = DataLoader(val_ds, val_batch_size, drop_last=False)\n",
        "\n",
        "print(\"Input:\", train_ds[0][0])  # Example input: [d1, d2, SEP, SEP, SEP]\n",
        "print(\"Target:\", train_ds[0][1]) # Example target: [d1, d2, SEP, d1, d2]\n",
        "len(train_ds), len(val_ds)  # Should be 80% for train and 20% for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- config helper ----------\n",
        "def attach_custom_mask(model):\n",
        "    def _mask(scores, hook=None):\n",
        "        # scores: (batch, heads, Q, K)\n",
        "        return scores + mask_bias.to(scores.device)\n",
        "    \n",
        "    # register the same mask hook on every layer\n",
        "    for block in model.blocks:\n",
        "        block.attn.hook_attn_scores.add_perma_hook(_mask, dir=\"fwd\")\n",
        "        # NOTE - new model origianlly had 'add_hook' instead of 'add_perma_hook'. I've changed it in line with the original model\n",
        "\n",
        "\n",
        "def strip_bias(m):\n",
        "    for mod in m.modules():\n",
        "        if hasattr(mod, \"bias\") and mod.bias is not None:\n",
        "            mod.bias.requires_grad_(False)\n",
        "            torch.nn.init.zeros_(mod.bias)\n",
        "            print(mod)\n",
        "\n",
        "    # remove biases from attention layers\n",
        "    attn_biases = ['b_Q', 'b_K', 'b_V', 'b_O']\n",
        "    for block in m.blocks:\n",
        "        for b in attn_biases:\n",
        "            mod = getattr(block.attn, b, None)\n",
        "            if mod is not None:\n",
        "                mod.requires_grad_(False)\n",
        "                torch.nn.init.zeros_(mod)\n",
        "\n",
        "    # remove unembed bias\n",
        "    if hasattr(m, \"unembed\") and m.b_U is not None:\n",
        "        m.unembed.b_U.requires_grad_(False)\n",
        "        torch.nn.init.zeros_(m.unembed.b_U)\n",
        "\n",
        "\n",
        "def set_WV_identity_and_freeze(model, d_model):\n",
        "    with torch.no_grad():\n",
        "        eye = torch.eye(d_model).unsqueeze(0)  # add head dim\n",
        "        for block in model.blocks:\n",
        "            block.attn.W_V.copy_(eye)\n",
        "            block.attn.W_V.requires_grad = False\n",
        "\n",
        "def set_WO_identity_and_freeze(model, d_model):\n",
        "    with torch.no_grad():\n",
        "        eye = torch.eye(d_model).unsqueeze(0)  # add head dim\n",
        "        for block in model.blocks:\n",
        "            block.attn.W_O.copy_(eye)\n",
        "            block.attn.W_O.requires_grad = False\n",
        "\n",
        "\n",
        "def make_model(d_model=D_MODEL, ln=USE_LN, use_bias=USE_BIAS, freeze_wv=FREEZE_WV, freeze_wo=FREEZE_WO):\n",
        "    cfg = HookedTransformerConfig(\n",
        "        n_layers=N_LAYER,\n",
        "        n_heads=N_HEAD,\n",
        "        d_model=d_model,\n",
        "        d_head=d_model // N_HEAD,\n",
        "        n_ctx=SEQ_LEN,\n",
        "        d_vocab=VOCAB,\n",
        "        attn_only=True, # no MLP!\n",
        "        normalization_type=(\"LN\" if ln else None),\n",
        "    )\n",
        "    model = HookedTransformer(cfg).to(DEV)\n",
        "    if freeze_wv:\n",
        "        set_WV_identity_and_freeze(model, d_model)\n",
        "    if freeze_wo:\n",
        "        set_WO_identity_and_freeze(model, d_model)\n",
        "    if not use_bias:\n",
        "        strip_bias(model)\n",
        "    \n",
        "    attach_custom_mask(model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- Model saving / loading helpers ------\n",
        "def save_model(model, path = MODEL_PATH):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(path = MODEL_PATH, device = DEV):\n",
        "    print(\"Loading model from\", path)\n",
        "    model = make_model()\n",
        "    model.load_state_dict(\n",
        "        torch.load(path, map_location=device)\n",
        "    )  # map weights to target device\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- utilities ----------\n",
        "def accuracy(m):\n",
        "    m.eval()\n",
        "    hits = tots = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_dl:\n",
        "            logits = m(inputs.to(DEV))[:, LIST_LEN+1:]  # (batch, 2, vocab)\n",
        "            preds = logits.argmax(-1)\n",
        "            hits += (preds == targets[:, LIST_LEN+1:].to(DEV)).sum().item()\n",
        "            tots += preds.numel()\n",
        "    return hits / tots\n",
        "\n",
        "\n",
        "def train(m, max_steps=10000, early_stop_acc=0.999, checkpoints=False):\n",
        "    opt = torch.optim.AdamW(m.parameters(), 1e-3)\n",
        "    ce = torch.nn.CrossEntropyLoss()\n",
        "    dl = itertools.cycle(train_dl)  # infinite iterator\n",
        "    for step in tqdm(range(max_steps), desc=\"Training\"):\n",
        "        inputs, targets = next(dl)\n",
        "        # get logits/loss for output tokens only\n",
        "        logits = m(inputs.to(DEV))[:, LIST_LEN+1:].reshape(-1, VOCAB) \n",
        "        loss = ce(logits, targets[:, LIST_LEN+1:].reshape(-1).to(DEV))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        if (step + 1) % 100 == 0:\n",
        "            acc = accuracy(m)\n",
        "            if acc >= early_stop_acc:\n",
        "                print(f\"Early stopping at step {step + 1} with accuracy {acc:.2%} >= {early_stop_acc:.2%}\")\n",
        "                break\n",
        "            update_every = max(min(10000, 0.05*max_steps), 1000)\n",
        "            if (step+1) % update_every == 0:\n",
        "                print(f\"Step {step + 1}, Loss: {loss.item():.4f}, Accuracy: {acc:.2%}\")\n",
        "            if checkpoints and (step+1) % 50000 == 0:\n",
        "                save_model(m, MODEL_PATH)\n",
        "            \n",
        "    print(f\"Final accuracy: {accuracy(m):.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 60,  44, 100, 100, 100],\n",
              "         [ 28,  90, 100, 100, 100],\n",
              "         [ 93,  99, 100, 100, 100],\n",
              "         [ 19,  17, 100, 100, 100],\n",
              "         [ 49,  19, 100, 100, 100]]),\n",
              " tensor([[ 60,  44, 100,  60,  44],\n",
              "         [ 28,  90, 100,  28,  90],\n",
              "         [ 93,  99, 100,  93,  99],\n",
              "         [ 19,  17, 100,  19,  17],\n",
              "         [ 49,  19, 100,  49,  19]]))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check train set\n",
        "train_ds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ---------- experiment grid ----------\n",
        "\n",
        "specs = [\n",
        "    # {'name': 'd32_ln_bias', 'd_model': 32, 'ln': True, 'use_bias': True},\n",
        "    # {'name': 'd32_noLN', 'd_model': 32, 'ln': False, 'use_bias': True},\n",
        "    # {'name': 'd32_noBias', 'd_model': 32, 'ln': True, 'use_bias': False},\n",
        "    # {'name': 'd32_noLNnoBias', 'd_model': 32, 'ln': False, 'use_bias': False},\n",
        "    # {'name': 'd32_fwo', 'd_model': 32, 'freeze_wo': True},\n",
        "    # {'name': 'd32_unfwo', 'd_model': 32, 'freeze_wo': False},\n",
        "\n",
        "    # {'name': 'd16_ln_bias', 'd_model': 16, 'ln': True, 'use_bias': True},\n",
        "    # {'name': 'd16_noLN', 'd_model': 16, 'ln': False, 'use_bias': True},\n",
        "    # {'name': 'd16_noBias', 'd_model': 16, 'ln': True, 'use_bias': False},\n",
        "    # {'name': 'd16_noLNnoBias', 'd_model': 16, 'ln': False, 'use_bias': False},\n",
        "    # {'name': 'd16_fwo', 'd_model': 16, 'freeze_wo': True},\n",
        "    # {'name': 'd16_unfwo', 'd_model': 16, 'freeze_wo': False},\n",
        "\n",
        "    # {'name': 'd8_ln_bias', 'd_model': 8, 'ln': True, 'use_bias': True},\n",
        "    # {'name': 'd8_noLN', 'd_model': 8, 'ln': False, 'use_bias': True},\n",
        "    # {'name': 'd8_noBias', 'd_model': 8, 'ln': True, 'use_bias': False},\n",
        "    # {'name': 'd8_noLNnoBias', 'd_model': 8, 'ln': False, 'use_bias': False},\n",
        "    # {'name': 'd8_fwo', 'd_model': 8, 'freeze_wo': True},\n",
        "    # {'name': 'd8_unfwo', 'd_model': 8, 'freeze_wo': False},\n",
        "\n",
        "    # {'name': 'd4_ln_bias', 'd_model': 4, 'ln': True, 'use_bias': True},\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for spec in specs:\n",
        "    # Create a full spec by starting with defaults and updating with the current spec\n",
        "    full_spec = {\n",
        "        'd_model': D_MODEL,\n",
        "        'ln': USE_LN,\n",
        "        'use_bias': USE_BIAS,\n",
        "        'freeze_wv': FREEZE_WV,\n",
        "        'freeze_wo': FREEZE_WO,\n",
        "    }\n",
        "    full_spec.update(spec) # Overwrite defaults with provided spec values\n",
        "\n",
        "    print(f\"--- Training model: {full_spec['name']} ---\")\n",
        "    model = make_model(\n",
        "        d_model=full_spec['d_model'], \n",
        "        ln=full_spec['ln'],\n",
        "        use_bias=full_spec['use_bias'],\n",
        "        freeze_wv=full_spec['freeze_wv'],\n",
        "        freeze_wo=full_spec['freeze_wo'],\n",
        "    )\n",
        "\n",
        "    train(model, max_steps=10)\n",
        "    \n",
        "    # Add all spec parameters to the results\n",
        "    result = full_spec.copy()\n",
        "    result['val_acc'] = round(accuracy(model), 4)\n",
        "    rows.append(result)\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Move 'name' column to the front for better readability\n",
        "if 'name' in df.columns:\n",
        "    cols = ['name'] + [col for col in df.columns if col != 'name']\n",
        "    df = df[cols]\n",
        "\n",
        "print(df.to_markdown(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| model       |   d_model | use_ln   | bias   |   val_acc |\n",
        "|:------------|----------:|:---------|:-------|----------:|\n",
        "| d32_ln_bias |        32 | True     | True   |     0.7   |\n",
        "| d32_noLN    |        32 | False    | True   |     0.425 |\n",
        "| d32_noBias  |        32 | True     | False  |     0.775 |\n",
        "| d16         |        16 | True     | True   |     0.575 |\n",
        "| d8          |         8 | True     | True   |     0.875 |\n",
        "| d4          |         4 | True     | True   |     0.5   |\n",
        "\n",
        "(for 10 digits)\n",
        "\n",
        "Bias not needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model\n",
            "Moving model to device:  cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40c0b8a54fe34a7ba148777fa272f32b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/400000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 10000, Loss: 0.1154, Accuracy: 90.80%\n"
          ]
        }
      ],
      "source": [
        "# LOAD existing or train and SAVE new model\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model = load_model(MODEL_PATH, device=DEV)\n",
        "else:\n",
        "    print(\"Training model\")\n",
        "    model = make_model(D_MODEL, USE_LN)\n",
        "    train(model, max_steps=MAX_TRAIN_STEPS, early_stop_acc=0.999, checkpoints=USE_CHECKPOINTING)\n",
        "    save_model(model, MODEL_PATH)\n",
        "\n",
        "# from torchinfo import summary\n",
        "# summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Overview of Model Parameters ---\n",
            "Parameter Name                           | Shape                | Trainable \n",
            "--------------------------------------------------------------------------------\n",
            "embed.W_E                                | (101, 32)            | Yes       \n",
            "pos_embed.W_pos                          | (5, 32)              | Yes       \n",
            "blocks.0.ln1.w                           | (32,)                | Yes       \n",
            "blocks.0.ln1.b                           | (32,)                | Yes       \n",
            "blocks.0.attn.W_Q                        | (1, 32, 32)          | Yes       \n",
            "blocks.0.attn.W_O                        | (1, 32, 32)          | Yes       \n",
            "blocks.0.attn.b_Q                        | (1, 32)              | Yes       \n",
            "blocks.0.attn.b_O                        | (32,)                | Yes       \n",
            "blocks.0.attn.W_K                        | (1, 32, 32)          | Yes       \n",
            "blocks.0.attn.W_V                        | (1, 32, 32)          | No        \n",
            "blocks.0.attn.b_K                        | (1, 32)              | Yes       \n",
            "blocks.0.attn.b_V                        | (1, 32)              | Yes       \n",
            "blocks.1.ln1.w                           | (32,)                | Yes       \n",
            "blocks.1.ln1.b                           | (32,)                | Yes       \n",
            "blocks.1.attn.W_Q                        | (1, 32, 32)          | Yes       \n",
            "blocks.1.attn.W_O                        | (1, 32, 32)          | Yes       \n",
            "blocks.1.attn.b_Q                        | (1, 32)              | Yes       \n",
            "blocks.1.attn.b_O                        | (32,)                | Yes       \n",
            "blocks.1.attn.W_K                        | (1, 32, 32)          | Yes       \n",
            "blocks.1.attn.W_V                        | (1, 32, 32)          | No        \n",
            "blocks.1.attn.b_K                        | (1, 32)              | Yes       \n",
            "blocks.1.attn.b_V                        | (1, 32)              | Yes       \n",
            "ln_final.w                               | (32,)                | Yes       \n",
            "ln_final.b                               | (32,)                | Yes       \n",
            "unembed.W_U                              | (32, 101)            | Yes       \n",
            "unembed.b_U                              | (101,)               | Yes       \n",
            "--------------------------------------------------------------------------------\n",
            "Total parameters: 15365\n",
            "Trainable parameters: 13317\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- Model Parameters Overview ---\n",
        "\n",
        "print(\"--- Overview of Model Parameters ---\")\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "\n",
        "# Use a formatted string for better alignment\n",
        "print(f\"{'Parameter Name':<40} | {'Shape':<20} | {'Trainable':<10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    shape_str = str(tuple(param.shape))\n",
        "    is_trainable = \"Yes\" if param.requires_grad else \"No\"\n",
        "    print(f\"{name:<40} | {shape_str:<20} | {is_trainable:<10}\")\n",
        "    \n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W_K   (1, 32, 32)\n",
            "W_Q   (1, 32, 32)\n",
            "W_pos (5, 32)\n",
            "W_E   (100, 32)\n",
            "W_U   (32, 100)\n"
          ]
        }
      ],
      "source": [
        "# Helper variables\n",
        "# W_O_l0, W_O_l1 = model.W_O # frozen as I\n",
        "W_K_l0, W_K_l1 = model.W_K\n",
        "W_Q_l0, W_Q_l1 = model.W_Q\n",
        "# W_V_l0, W_V_l1 = model.W_V # frozen as I\n",
        "W_pos = model.W_pos\n",
        "W_E = model.W_E[:-1] # exclude SEP embedding (last token in W_E vocab)\n",
        "final_pos_resid_initial = model.W_E[-1] + W_pos[LIST_LEN+1]  # W_pos[2] is the SEP token position, which is at index LIST_LEN+1\n",
        "W_U = model.W_U[:, :-1] # exclude SEP\n",
        "\n",
        "# print('W_O  ', tuple(W_O_l0.shape))\n",
        "print('W_K  ', tuple(W_K_l0.shape))\n",
        "print('W_Q  ', tuple(W_Q_l0.shape))\n",
        "# print('W_V  ', tuple(W_V_l0.shape))\n",
        "print('W_pos', tuple(W_pos.shape))\n",
        "print('W_E  ', tuple(W_E.shape))\n",
        "print('W_U  ', tuple(W_U.shape)) # (d_model, vocab-1) - excludes SEP token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Get the first three positional embedding vectors\n",
        "# W_pos_d1, W_pos_d2, W_pos_think, W_pos_o1, W_pos_o2 = W_pos\n",
        "\n",
        "# # Look at the difference between positional embeddings; show they are symmetric\n",
        "# def compare_tensors(v, w):\n",
        "#     return ((v-w).pow(2).sum()/v.pow(2).sum().sqrt()/w.pow(2).sum().sqrt()).item()\n",
        "# print('Difference in position embeddings', compare_tensors(W_pos_d1, W_pos_d2))\n",
        "# print('Cosine similarity of position embeddings', torch.cosine_similarity(W_pos_d1, W_pos_d2, dim=0).item())\n",
        "\n",
        "# # Compare N(x, y) and N(y, x)\n",
        "# neuron_acts_square = neuron_acts_post.reshape(p, p, D_MLP)\n",
        "# diff = compare_tensors(\n",
        "#     neuron_acts_square,\n",
        "#     einops.rearrange(neuron_acts_square, \"x y d_mlp -> y x d_mlp\")\n",
        "# )\n",
        "# print(f'Difference in neuron activations for (x,y) and (y,x): {diff:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We confirm below that the model does not leak attention onto the first two tokens, which are the inputs to the task. The model should only attend to the first two tokens when predicting the third token, and not attend to them at all when predicting the fourth and fifth tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def check_attention(m, dataloader, eps=1e-3):\n",
        "#     for (seq,) in dataloader:\n",
        "#         with torch.no_grad():\n",
        "#             _, cache = m.run_with_cache(seq.to(DEV))\n",
        "#         for l in range(m.cfg.n_layers):\n",
        "#             pat = cache[\"pattern\", l][:, 0]  # (batch, Q, K)\n",
        "#             leak = pat[:, LIST_LEN+1:, :LIST_LEN].sum(dim=-1)  # mass on forbidden keys\n",
        "#             if (leak > eps).any():\n",
        "#                 raise ValueError(f\"❌ Layer {l}: output tokens attend to x₁/x₂ by >{eps:.0e}\")\n",
        "#     print(\"✅ no attention leakage onto x₁/x₂\")\n",
        "\n",
        "\n",
        "# sample = val_ds[0][0] # Example input sequence\n",
        "# print(f\"Sample sequence: {sample.cpu().numpy()}\")  # Print the sample sequence for reference\n",
        "# _, cache = model.run_with_cache(sample)\n",
        "\n",
        "# fig, axes = plt.subplots(1, model.cfg.n_layers, figsize=(6, 3))\n",
        "# if model.cfg.n_layers == 1:\n",
        "#     axes = [axes]\n",
        "# for l in range(model.cfg.n_layers):\n",
        "#     pat = cache[\"pattern\", l][0, 0].cpu()  # (5,5)\n",
        "#     ax = axes[l]\n",
        "#     im = ax.imshow(pat, cmap=\"viridis\", vmin=0, vmax=1)\n",
        "#     ax.set_title(f\"Layer {l} Attention Pattern\")\n",
        "#     ax.set_xlabel(\"Key Position\")\n",
        "#     ax.set_ylabel(\"Query Position\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# check_attention(model, val_dl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample sequence: [ 80  52 100 100 100]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                    <div id=\"007eb21a-2db0-4331-b6ca-36617c94d218\" class=\"plotly-graph-div\" style=\"height:450px; width:850px;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"007eb21a-2db0-4331-b6ca-36617c94d218\")) {                    Plotly.newPlot(                        \"007eb21a-2db0-4331-b6ca-36617c94d218\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":false,\"x\":[\"d1\",\"d2\",\"SEP\",\"o1\",\"o2\"],\"y\":[\"d1\",\"d2\",\"SEP\",\"o1\",\"o2\"],\"z\":{\"dtype\":\"f4\",\"bdata\":\"AACAPwAAAAAAAAAAAAAAAAAAAAAAAIA\\u002fAAAAAAAAAAAAAAAAAAAAAHy92D5BoRM\\u002fAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA\\u002fAAAAAAAAAAAAAAAAAAAAAAFx3j5\\u002fxxA\\u002fAAAAAA==\",\"shape\":\"5, 5\"},\"zmax\":1,\"zmin\":0,\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":true,\"x\":[\"d1\",\"d2\",\"SEP\",\"o1\",\"o2\"],\"y\":[\"d1\",\"d2\",\"SEP\",\"o1\",\"o2\"],\"z\":{\"dtype\":\"f4\",\"bdata\":\"AACAPwAAAAAAAAAAAAAAAAAAAAAAAIA\\u002fAAAAAAAAAAAAAAAAAAAAALxp9T4hSwU\\u002fAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA\\u002fAAAAAAAAAAAAAAAAAAAAAP9khz6ATTw\\u002fAAAAAA==\",\"shape\":\"5, 5\"},\"zmax\":1,\"zmin\":0,\"type\":\"heatmap\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scattermap\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermap\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.46],\"title\":{\"text\":\"Key Position\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Query Position\"},\"autorange\":\"reversed\"},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.54,1.0],\"title\":{\"text\":\"Key Position\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Query Position\"},\"autorange\":\"reversed\"},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Layer 0 Attention Pattern\",\"x\":0.23,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Layer 1 Attention Pattern\",\"x\":0.77,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Attention Patterns for a Sample Sequence\"},\"width\":850,\"height\":450},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('007eb21a-2db0-4331-b6ca-36617c94d218');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };            </script>        </div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ no attention leakage onto x₁/x₂\n"
          ]
        }
      ],
      "source": [
        "# --- Using Plotly for visualization ---\n",
        "\n",
        "def check_attention(m, dataloader, eps=1e-3):\n",
        "    for inputs, _ in dataloader:\n",
        "        with torch.no_grad():\n",
        "            _, cache = m.run_with_cache(inputs.to(DEV))\n",
        "        for l in range(m.cfg.n_layers):\n",
        "            pat = cache[\"pattern\", l][:, 0]  # (batch, Q, K)\n",
        "            leak = pat[:, LIST_LEN+1:, :LIST_LEN].sum(dim=-1)  # mass on forbidden keys\n",
        "            if (leak > eps).any():\n",
        "                raise ValueError(f\"❌ Layer {l}: output tokens attend to x₁/x₂ by >{eps:.0e}\")\n",
        "    print(\"✅ no attention leakage onto x₁/x₂\")\n",
        "\n",
        "\n",
        "sample = val_ds[0][0] # Example input sequence\n",
        "print(f\"Sample sequence: {sample.cpu().numpy()}\")  # Print the sample sequence for reference\n",
        "_, cache = model.run_with_cache(sample.unsqueeze(0).to(DEV))\n",
        "\n",
        "# --- Create Plotly visualization ---\n",
        "token_labels = [f'd{i+1}' for i in range(LIST_LEN)] + ['SEP'] + [f'o{i+1}' for i in range(LIST_LEN)]\n",
        "subplot_titles = [f\"Layer {l} Attention Pattern\" for l in range(model.cfg.n_layers)]\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, \n",
        "    cols=model.cfg.n_layers, \n",
        "    subplot_titles=subplot_titles,\n",
        "    horizontal_spacing=0.08 # Add spacing between plots\n",
        ")\n",
        "\n",
        "for l in range(model.cfg.n_layers):\n",
        "    pat = cache[\"pattern\", l][0, 0].cpu().numpy()\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Heatmap(\n",
        "            z=pat,\n",
        "            x=token_labels,\n",
        "            y=token_labels,\n",
        "            colorscale=\"Viridis\",\n",
        "            zmin=0,\n",
        "            zmax=1,\n",
        "            showscale=(l == model.cfg.n_layers - 1) # Show colorbar only for the last plot\n",
        "        ),\n",
        "        row=1, col=l+1\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Attention Patterns for a Sample Sequence\",\n",
        "    width=850,\n",
        "    height=450,\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "\n",
        "# Apply settings to all axes\n",
        "fig.update_xaxes(title_text=\"Key Position\")\n",
        "fig.update_yaxes(title_text=\"Query Position\", autorange='reversed')\n",
        "\n",
        "fig.show()\n",
        "\n",
        "check_attention(model, val_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0: all attention patterns identical? ❌\n",
            "Layer 1: all attention patterns identical? ❌\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                    <div id=\"85ddca46-1dbe-49c5-bb42-4c2305881cf4\" class=\"plotly-graph-div\" style=\"height:450px; width:850px;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"85ddca46-1dbe-49c5-bb42-4c2305881cf4\")) {                    Plotly.newPlot(                        \"85ddca46-1dbe-49c5-bb42-4c2305881cf4\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":false,\"x\":[\"d1\",\"d2\",\"SEP\",\"o1\",\"o2\"],\"y\":[\"d1\",\"d2\",\"SEP\",\"o1\",\"o2\"],\"z\":{\"dtype\":\"f4\",\"bdata\":\"AACAPwAAAAAAAAAAAAAAAAAAAAAAAIA\\u002fAAAAAAAAAAAAAAAAAAAAAKRM\\u002fz6vWQA\\u002fAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA\\u002fAAAAAAAAAAAAAAAAAAAAAAFx3j5\\u002fxxA\\u002fAAAAAA==\",\"shape\":\"5, 5\"},\"zmax\":1,\"zmin\":0,\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":true,\"x\":[\"d1\",\"d2\",\"SEP\",\"o1\",\"o2\"],\"y\":[\"d1\",\"d2\",\"SEP\",\"o1\",\"o2\"],\"z\":{\"dtype\":\"f4\",\"bdata\":\"AACAPwAAAAAAAAAAAAAAAAAAAAAAAIA\\u002fAAAAAAAAAAAAAAAAAAAAAERK9T7fWgU\\u002fAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA\\u002fAAAAAAAAAAAAAAAAAAAAAE\\u002fHEj9jcdo+AAAAAA==\",\"shape\":\"5, 5\"},\"zmax\":1,\"zmin\":0,\"type\":\"heatmap\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scattermap\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermap\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.46],\"title\":{\"text\":\"Key Position\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Query Position\"},\"autorange\":\"reversed\"},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.54,1.0],\"title\":{\"text\":\"Key Position\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Query Position\"},\"autorange\":\"reversed\"},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Layer 0 Average Attention\",\"x\":0.23,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Layer 1 Average Attention\",\"x\":0.77,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Average Attention Patterns Across Validation Set\"},\"width\":850,\"height\":450},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('85ddca46-1dbe-49c5-bb42-4c2305881cf4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };            </script>        </div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with avg-attn: 0.5755\n"
          ]
        }
      ],
      "source": [
        "# --- Mean Attention Patterns ---\n",
        "\n",
        "all_pats = [[] for _ in range(model.cfg.n_layers)]\n",
        "for inputs, _ in val_dl:\n",
        "    with torch.no_grad():\n",
        "        _, cache = model.run_with_cache(inputs.to(DEV))\n",
        "    for l in range(model.cfg.n_layers):\n",
        "        pat = cache[\"pattern\", l][:, 0]  # (batch, Q, K)\n",
        "        all_pats[l].append(pat)\n",
        "all_pats = [torch.cat(pats, dim=0) for pats in all_pats]\n",
        "\n",
        "for l, pats in enumerate(all_pats):\n",
        "    identical = torch.allclose(pats, pats[0].expand_as(pats))\n",
        "    print(f\"Layer {l}: all attention patterns identical? {'✅' if identical else '❌'}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    avg_pats = [\n",
        "        torch.zeros(SEQ_LEN, SEQ_LEN, device=DEV) for _ in range(model.cfg.n_layers)\n",
        "    ]\n",
        "    n = 0\n",
        "    for inputs, _ in val_dl:\n",
        "        _, cache = model.run_with_cache(inputs.to(DEV))\n",
        "        for l in range(model.cfg.n_layers):\n",
        "            avg_pats[l] += cache[\"pattern\", l][:, 0].sum(0)\n",
        "        n += inputs.shape[0]\n",
        "    avg_pats = [p / n for p in avg_pats]\n",
        "\n",
        "# --- Visualize Average Attention Patterns ---\n",
        "token_labels = [f'd{i+1}' for i in range(LIST_LEN)] + ['SEP'] + [f'o{i+1}' for i in range(LIST_LEN)]\n",
        "subplot_titles = [f\"Layer {l} Average Attention\" for l in range(model.cfg.n_layers)]\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, \n",
        "    cols=model.cfg.n_layers, \n",
        "    subplot_titles=subplot_titles,\n",
        "    horizontal_spacing=0.08\n",
        ")\n",
        "\n",
        "for l in range(model.cfg.n_layers):\n",
        "    avg_pat_np = avg_pats[l].cpu().numpy()\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Heatmap(\n",
        "            z=avg_pat_np,\n",
        "            x=token_labels,\n",
        "            y=token_labels,\n",
        "            colorscale=\"Viridis\",\n",
        "            zmin=0,\n",
        "            zmax=1,\n",
        "            showscale=(l == model.cfg.n_layers - 1) # Show colorbar only for the last plot\n",
        "        ),\n",
        "        row=1, col=l+1\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Average Attention Patterns Across Validation Set\",\n",
        "    width=850,\n",
        "    height=450,\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig.update_xaxes(title_text=\"Key Position\")\n",
        "fig.update_yaxes(title_text=\"Query Position\", autorange='reversed')\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# Create a deep copy of the model to avoid modifying the original\n",
        "model_with_avg_attn = copy.deepcopy(model)\n",
        "\n",
        "def mk_hook(avg):\n",
        "    logits = (avg + 1e-12).log()  # log-prob so softmax≈avg, ε avoids -∞\n",
        "\n",
        "    def f(scores, hook):\n",
        "        return logits.unsqueeze(0).unsqueeze(0).expand_as(scores)\n",
        "\n",
        "    return f\n",
        "\n",
        "for l in range(model_with_avg_attn.cfg.n_layers):\n",
        "    model_with_avg_attn.blocks[l].attn.hook_attn_scores.add_hook(\n",
        "        mk_hook(avg_pats[l]), dir=\"fwd\"\n",
        "    )\n",
        "\n",
        "print(\"Accuracy with avg-attn:\", accuracy(model_with_avg_attn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The attention patterns are not the same across inputs. However, we can replace the attention scores with their average and still get almost perfect performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original loss: 0.23425647616386414\n",
            "Original accuracy: 0.9110000133514404\n",
            "Sample sequence 0: [ 80  52 100 100 100]\n"
          ]
        }
      ],
      "source": [
        "# --- Setup ---\n",
        "head_index_to_ablate = 0 # fixed\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Check loss on validation set\n",
        "val_inputs = val_ds.tensors[0].to(DEV)\n",
        "val_targets = val_ds.tensors[1].to(DEV)\n",
        "sample_idx = 0  # Use the xth sample in the validation set for comparing predictions\n",
        "sample_list = val_inputs[sample_idx].cpu().numpy()\n",
        "\n",
        "# --- Calculate Original Loss on last 2 digits ---\n",
        "with torch.no_grad():\n",
        "    original_logits, cache = model.run_with_cache(val_inputs, return_type=\"logits\")\n",
        "    output_logits = original_logits[:, LIST_LEN+1:] # Slice to get logits for the last two positions\n",
        "    output_targets = val_targets[:, LIST_LEN+1:] # Slice to get the target tokens\n",
        "    \n",
        "    original_loss = loss_fn(output_logits.reshape(-1, VOCAB), output_targets.reshape(-1)) # Calculate the loss\n",
        "    # Calculate accuracy\n",
        "    original_predictions = original_logits.argmax(dim=-1) \n",
        "    original_output_predictions = original_predictions[:, LIST_LEN+1:]\n",
        "    original_accuracy = (original_output_predictions == output_targets).float().mean()\n",
        "\n",
        "print(f\"Original loss: {original_loss.item()}\")\n",
        "print(f\"Original accuracy: {original_accuracy.item()}\")\n",
        "print(f\"Sample sequence {sample_idx}: {sample_list}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Positional Encoding Ablation Results ---\n",
            "Original Loss: 0.2343, Original Accuracy: 0.9110\n",
            "Original prediction (sample 0): [52 52 53 80 52]\n",
            "------------------------------------------------------------\n",
            "Ablating Positional Encodings Before Layer 0:\n",
            "  Ablated Loss: 28.4467\n",
            "  Ablated Accuracy: 0.2870\n",
            "  Ablated prediction (sample 0):  [12 12 80 52 52]\n",
            "------------------------------------------------------------\n",
            "Ablating Positional Encodings Between Layer 0 and 1:\n",
            "  Ablated Loss: 0.8277\n",
            "  Ablated Accuracy: 0.7753\n",
            "  Ablated prediction (sample 0):  [56 52 53 37 52]\n",
            "------------------------------------------------------------\n",
            "Ablating Positional Encodings After Layer 1 (pre W_U):\n",
            "  Ablated Loss: 0.8849\n",
            "  Ablated Accuracy: 0.7598\n",
            "  Ablated prediction (sample 0):  [52 52 53 37 52]\n",
            "------------------------------------------------------------\n",
            "Ablating Positional Encodings Before Layer 0:\n",
            "  Ablated Loss: 28.4467\n",
            "  Ablated Accuracy: 0.2870\n",
            "  Ablated prediction (sample 0):  [12 12 80 52 52]\n",
            "------------------------------------------------------------\n",
            "Ablating Positional Encodings Between Layer 0 and 1:\n",
            "  Ablated Loss: 0.8277\n",
            "  Ablated Accuracy: 0.7753\n",
            "  Ablated prediction (sample 0):  [56 52 53 37 52]\n",
            "------------------------------------------------------------\n",
            "Ablating Positional Encodings After Layer 1 (pre W_U):\n",
            "  Ablated Loss: 0.8849\n",
            "  Ablated Accuracy: 0.7598\n",
            "  Ablated prediction (sample 0):  [52 52 53 37 52]\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- Positional Encoding Ablation ---\n",
        "\n",
        "print(\"--- Positional Encoding Ablation Results ---\")\n",
        "print(f\"Original Loss: {original_loss.item():.4f}, Original Accuracy: {original_accuracy.item():.4f}\")\n",
        "print(f\"Original prediction (sample {sample_idx}): {original_predictions[sample_idx].cpu().numpy()}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Hook to subtract positional encodings from the residual stream\n",
        "def ablate_pos_encoding_hook(resid, hook):\n",
        "    # resid shape: [batch, seq_pos, d_model]\n",
        "    # W_pos shape: [seq_pos, d_model]\n",
        "    # We subtract the positional embeddings from the residual stream.\n",
        "    # W_pos is automatically broadcast across the batch dimension.\n",
        "    result = resid - model.W_pos\n",
        "    # Restore some positions to their original values\n",
        "    idx = 2\n",
        "    result[:, idx] = resid[:, idx]\n",
        "    return result\n",
        "\n",
        "# Define the ablation experiments: a description and the hook point\n",
        "ablation_points = [\n",
        "    (\"Before Layer 0\", \"blocks.0.hook_resid_pre\"),\n",
        "    (\"Between Layer 0 and 1\", \"blocks.0.hook_resid_post\"), # i.e. blocks.1.hook_resid_pre\n",
        "    (\"After Layer 1 (pre W_U)\", \"blocks.1.hook_resid_post\"), # Before Unembed\n",
        "]\n",
        "\n",
        "# --- Perform Ablation for Each Case ---\n",
        "for description, hook_name in ablation_points:\n",
        "    with torch.no_grad():\n",
        "        # Run the model with the ablation hook\n",
        "        ablated_logits = model.run_with_hooks(\n",
        "            val_inputs,\n",
        "            return_type=\"logits\",\n",
        "            fwd_hooks=[(hook_name, ablate_pos_encoding_hook)]\n",
        "        )\n",
        "    \n",
        "    # Calculate ablated loss on the output tokens\n",
        "    output_logits_ablated = ablated_logits[:, LIST_LEN+1:]\n",
        "    ablated_loss = loss_fn(output_logits_ablated.reshape(-1, VOCAB), output_targets.reshape(-1))\n",
        "    \n",
        "    # Calculate ablated accuracy on the output tokens\n",
        "    ablated_predictions = ablated_logits.argmax(dim=-1)\n",
        "    ablated_output_predictions = ablated_predictions[:, LIST_LEN+1:]\n",
        "    ablated_accuracy = (ablated_output_predictions == output_targets).float().mean()\n",
        "    \n",
        "    print(f\"Ablating Positional Encodings {description}:\")\n",
        "    print(f\"  Ablated Loss: {ablated_loss.item():.4f}\")\n",
        "    print(f\"  Ablated Accuracy: {ablated_accuracy.item():.4f}\")\n",
        "    print(f\"  Ablated prediction (sample {sample_idx}):  {ablated_predictions[sample_idx].cpu().numpy()}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residual stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Attention Skip Connection Ablation Results ---\n",
            "Sample sequence: [ 80  52 100 100 100]\n",
            "Original Loss: 0.2343\n",
            "Original Accuracy: 0.9110\n",
            "Original predictions: [52 52 53 80 52]\n",
            "--------------------------------------------------\n",
            "Ablating Skip Connection at Layer 0:\n",
            "  Ablated Loss: 8.8041\n",
            "  Loss Increase: 8.5699\n",
            "  Ablated Accuracy: 0.4265\n",
            "  Ablated predictions: [56 56 53 52 15]\n",
            "--------------------------------------------------\n",
            "Ablating Skip Connection at Layer 1:\n",
            "  Ablated Loss: 25.2989\n",
            "  Loss Increase: 25.0646\n",
            "  Ablated Accuracy: 0.1147\n",
            "  Ablated predictions: [52 52 52 36 52]\n",
            "--------------------------------------------------\n",
            "Ablating Skip Connection at Layer 0:\n",
            "  Ablated Loss: 8.8041\n",
            "  Loss Increase: 8.5699\n",
            "  Ablated Accuracy: 0.4265\n",
            "  Ablated predictions: [56 56 53 52 15]\n",
            "--------------------------------------------------\n",
            "Ablating Skip Connection at Layer 1:\n",
            "  Ablated Loss: 25.2989\n",
            "  Loss Increase: 25.0646\n",
            "  Ablated Accuracy: 0.1147\n",
            "  Ablated predictions: [52 52 52 36 52]\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def ablate_skip_connection(layer_to_ablate):\n",
        "    \"\"\"\n",
        "    Ablates the skip connection over the attention block for a specific layer.\n",
        "    \"\"\"\n",
        "    \n",
        "    # This dictionary will store the input residual stream for the layer\n",
        "    captured_resid_pre = {}\n",
        "\n",
        "    # Hook to capture the input to the attention block\n",
        "    def capture_resid_pre_hook(resid, hook):\n",
        "        captured_resid_pre['value'] = resid\n",
        "        return resid\n",
        "\n",
        "    # Hook to ablate the skip connection by subtracting the captured input\n",
        "    def ablate_skip_hook(resid, hook):\n",
        "        # resid here is resid_pre + attn_out\n",
        "        # We subtract resid_pre to leave only attn_out\n",
        "        result = resid - captured_resid_pre['value']\n",
        "        # result[:, 3:] = resid[:, 3:]\n",
        "        return result\n",
        "\n",
        "    resid_pre_hook_name = f\"blocks.{layer_to_ablate}.hook_resid_pre\"\n",
        "    hook_attn_out_name = f\"blocks.{layer_to_ablate}.hook_attn_out\"\n",
        "    resid_post_hook_name = f\"blocks.{layer_to_ablate}.hook_resid_post\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ablated_logits = model.run_with_hooks(\n",
        "            val_inputs,\n",
        "            return_type=\"logits\",\n",
        "            fwd_hooks=[\n",
        "                (resid_pre_hook_name, capture_resid_pre_hook),\n",
        "                (resid_post_hook_name, ablate_skip_hook)\n",
        "            ]\n",
        "        )\n",
        "    return ablated_logits\n",
        "\n",
        "print(f\"--- Attention Skip Connection Ablation Results ---\")\n",
        "print(f\"Sample sequence: {val_inputs[sample_idx].cpu().numpy()}\") # last sample in validation set\n",
        "print(f\"Original Loss: {original_loss.item():.4f}\")\n",
        "print(f\"Original Accuracy: {original_accuracy.item():.4f}\")\n",
        "print(f\"Original predictions: {original_predictions[sample_idx].cpu().numpy()}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Perform Ablation for Each Layer ---\n",
        "for l in range(N_LAYER):\n",
        "    ablated_logits = ablate_skip_connection(l)\n",
        "    output_logits_ablated = ablated_logits[:, LIST_LEN+1:]\n",
        "    ablated_loss = loss_fn(output_logits_ablated.reshape(-1, VOCAB), output_targets.reshape(-1))\n",
        "    ablated_predictions = ablated_logits.argmax(dim=-1)\n",
        "    \n",
        "    # --- Calculate Ablated Accuracy ---\n",
        "    ablated_output_predictions = ablated_predictions[:, LIST_LEN+1:]\n",
        "    ablated_accuracy = (ablated_output_predictions == output_targets).float().mean()\n",
        "\n",
        "    print(f\"Ablating Skip Connection at Layer {l}:\")\n",
        "    print(f\"  Ablated Loss: {ablated_loss.item():.4f}\")\n",
        "    print(f\"  Loss Increase: {(ablated_loss - original_loss).item():.4f}\")\n",
        "    print(f\"  Ablated Accuracy: {ablated_accuracy.item():.4f}\")\n",
        "    print(f\"  Ablated predictions: {ablated_predictions[sample_idx].cpu().numpy()}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Ablating Both Skip Connections (Layers 0 & 1) ---\n",
            "Validation set size: 2000 samples\n",
            "--------------------------------------------------\n",
            "Metric       | Original   | Ablated   \n",
            "--------------------------------------------------\n",
            "Loss         | 0.2343     | 30.1144   \n",
            "Accuracy     | 0.9110     | 0.1107    \n",
            "--------------------------------------------------\n",
            "Example from 0th validation sample:\n",
            "  Sample sequence:      [ 80  52 100 100 100]\n",
            "  Original predictions: [52 52 53 80 52]\n",
            "  Ablated predictions:  [52 52 52 36 15]\n"
          ]
        }
      ],
      "source": [
        "# --- Ablate Skip Connections for Layer 0 and 1 Simultaneously ---\n",
        "\n",
        "# This dictionary will store the input residual streams for each layer\n",
        "captured_resid_pres = {}\n",
        "\n",
        "# Hook to capture the input to an attention block\n",
        "def capture_resid_pre_hook(resid, hook):\n",
        "    layer_idx = hook.layer()\n",
        "    captured_resid_pres[layer_idx] = resid\n",
        "    return resid\n",
        "\n",
        "# Hook to ablate the skip connection by subtracting the captured input\n",
        "def ablate_skip_hook(resid, hook):\n",
        "    layer_idx = hook.layer()\n",
        "    # Subtract the captured resid_pre for the corresponding layer\n",
        "    result = resid - captured_resid_pres[layer_idx]\n",
        "    idx = -2\n",
        "    # result[:, -2:] = resid[:, -2:] # keep some tokens intact (last 2)\n",
        "    return result\n",
        "\n",
        "# Define the hooks for both layers\n",
        "fwd_hooks = []\n",
        "for l in range(model.cfg.n_layers):\n",
        "    resid_pre_hook_name = f\"blocks.{l}.hook_resid_pre\"\n",
        "    resid_post_hook_name = f\"blocks.{l}.hook_resid_post\"\n",
        "    fwd_hooks.extend([\n",
        "        (resid_pre_hook_name, capture_resid_pre_hook),\n",
        "        (resid_post_hook_name, ablate_skip_hook)\n",
        "    ])\n",
        "\n",
        "# Run the model with both skip connections ablated\n",
        "with torch.no_grad():\n",
        "    ablated_logits = model.run_with_hooks(\n",
        "        val_inputs,\n",
        "        return_type=\"logits\",\n",
        "        fwd_hooks=fwd_hooks\n",
        "    )\n",
        "    output_logits_ablated = ablated_logits[:, LIST_LEN+1:]\n",
        "    ablated_loss = loss_fn(output_logits_ablated.reshape(-1, VOCAB), output_targets.reshape(-1))\n",
        "    ablated_predictions = ablated_logits.argmax(dim=-1)\n",
        "    \n",
        "    # --- Calculate Ablated Accuracy ---\n",
        "    ablated_output_predictions = ablated_predictions[:, LIST_LEN+1:]\n",
        "    ablated_accuracy = (ablated_output_predictions == output_targets).float().mean()\n",
        "\n",
        "\n",
        "print(f\"--- Ablating Both Skip Connections (Layers 0 & 1) ---\")\n",
        "print(f\"Validation set size: {len(val_inputs)} samples\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Metric':<12} | {'Original':<10} | {'Ablated':<10}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Loss':<12} | {original_loss.item():<10.4f} | {ablated_loss.item():<10.4f}\")\n",
        "print(f\"{'Accuracy':<12} | {original_accuracy.item():<10.4f} | {ablated_accuracy.item():<10.4f}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Example from {sample_idx}th validation sample:\")\n",
        "print(f\"  Sample sequence:      {val_inputs[sample_idx].cpu().numpy()}\")\n",
        "print(f\"  Original predictions: {original_predictions[sample_idx].cpu().numpy()}\")\n",
        "print(f\"  Ablated predictions:  {ablated_predictions[sample_idx].cpu().numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Skip Connection Mean Ablation Metrics ---\n",
            "Original -> Loss: 0.2343, Accuracy: 91.10%\n",
            "--------------------------------------------------\n",
            "Ablating Layer 0 Skip -> Loss: 6.9966, Accuracy: 43.58%\n",
            "Ablating Layer 1 Skip -> Loss: 27.1300, Accuracy: 9.57%\n",
            "Ablating Both Skips -> Loss: 29.4209, Accuracy: 12.73%\n",
            "--------------------------------------------------\n",
            "Ablating Both Skips -> Loss: 29.4209, Accuracy: 12.73%\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- Mean Ablation of Skip Connections ---\n",
        "\n",
        "# 1. Cache the 'resid_pre' activations for each layer across the validation set\n",
        "resid_pre_hook_names = [f\"blocks.{l}.hook_resid_pre\" for l in range(model.cfg.n_layers)]\n",
        "with torch.no_grad():\n",
        "    _, cache = model.run_with_cache(val_inputs, names_filter=lambda name: name in resid_pre_hook_names)\n",
        "\n",
        "# 2. Calculate the mean of these activations\n",
        "mean_resid_pres = {}\n",
        "for l in range(model.cfg.n_layers):\n",
        "    mean_resid_pres[l] = cache[resid_pre_hook_names[l]].mean(dim=(0, 1))\n",
        "\n",
        "# --- Define hooks for mean ablation ---\n",
        "captured_resid_pre = {}\n",
        "\n",
        "def capture_resid_pre_hook(resid, hook):\n",
        "    \"\"\"Saves the current resid_pre to be subtracted in the next hook.\"\"\"\n",
        "    captured_resid_pre[hook.layer()] = resid\n",
        "    return resid\n",
        "\n",
        "def mean_ablate_skip_hook(resid, hook):\n",
        "    \"\"\"Replaces the skip connection with its mean value.\"\"\"\n",
        "    layer_idx = hook.layer()\n",
        "    # resid_post = resid_pre + block_output\n",
        "    # We want: mean_resid_pre + block_output\n",
        "    # So we calculate: resid_post - resid_pre + mean_resid_pre\n",
        "    return resid - captured_resid_pre[layer_idx] + mean_resid_pres[layer_idx]\n",
        "\n",
        "# --- Function to calculate loss and accuracy ---\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def calculate_metrics(logits, targets):\n",
        "    \"\"\"Calculates loss and accuracy for the output tokens.\"\"\"\n",
        "    output_logits = logits[:, LIST_LEN+1:]\n",
        "    output_targets = targets[:, LIST_LEN+1:]\n",
        "    \n",
        "    loss = loss_fn(output_logits.reshape(-1, VOCAB), output_targets.reshape(-1)).item()\n",
        "    \n",
        "    predictions = output_logits.argmax(dim=-1)\n",
        "    accuracy = (predictions == output_targets).float().mean().item()\n",
        "    \n",
        "    return loss, accuracy\n",
        "\n",
        "# --- Evaluate metrics for each ablation case ---\n",
        "\n",
        "print(\"--- Skip Connection Mean Ablation Metrics ---\")\n",
        "\n",
        "# Original metrics\n",
        "with torch.no_grad():\n",
        "    original_logits = model(val_inputs)\n",
        "    og_loss, original_acc = calculate_metrics(original_logits, val_targets)\n",
        "print(f\"Original -> Loss: {og_loss:.4f}, Accuracy: {original_acc:.2%}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Ablate each layer individually\n",
        "for l in range(model.cfg.n_layers):\n",
        "    fwd_hooks = [\n",
        "        (f\"blocks.{l}.hook_resid_pre\", capture_resid_pre_hook),\n",
        "        (f\"blocks.{l}.hook_resid_post\", mean_ablate_skip_hook)\n",
        "    ]\n",
        "    with torch.no_grad():\n",
        "        ablated_logits = model.run_with_hooks(val_inputs, fwd_hooks=fwd_hooks)\n",
        "        ablated_loss, ablated_acc = calculate_metrics(ablated_logits, val_targets)\n",
        "    print(f\"Ablating Layer {l} Skip -> Loss: {ablated_loss:.4f}, Accuracy: {ablated_acc:.2%}\")\n",
        "\n",
        "# Ablate both layers simultaneously\n",
        "fwd_hooks = []\n",
        "for l in range(model.cfg.n_layers):\n",
        "    fwd_hooks.extend([\n",
        "        (f\"blocks.{l}.hook_resid_pre\", capture_resid_pre_hook),\n",
        "        (f\"blocks.{l}.hook_resid_post\", mean_ablate_skip_hook)\n",
        "    ])\n",
        "with torch.no_grad():\n",
        "    ablated_logits = model.run_with_hooks(val_inputs, fwd_hooks=fwd_hooks)\n",
        "    ablated_loss, ablated_acc = calculate_metrics(ablated_logits, val_targets)\n",
        "print(f\"Ablating Both Skips -> Loss: {ablated_loss:.4f}, Accuracy: {ablated_acc:.2%}\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### W_E and W_U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting 2D UMAP visualization  for W_E and W_U... \n",
            "(n_neighbors=5, min_dist=0.1, metric=euclidean)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/Mechinterp/list-comp/.venv/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning:\n",
            "\n",
            "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                    <div id=\"ae010b64-324f-4288-bbe4-1c1c3d743dc5\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"ae010b64-324f-4288-bbe4-1c1c3d743dc5\")) {                    Plotly.newPlot(                        \"ae010b64-324f-4288-bbe4-1c1c3d743dc5\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[\"Token: 0\\u003cbr\\u003ex: 3.046, y: 15.514\",\"Token: 1\\u003cbr\\u003ex: 2.768, y: 14.570\",\"Token: 2\\u003cbr\\u003ex: 2.624, y: 14.685\",\"Token: 3\\u003cbr\\u003ex: 2.229, y: 14.881\",\"Token: 4\\u003cbr\\u003ex: 2.228, y: 17.038\",\"Token: 5\\u003cbr\\u003ex: 5.391, y: 13.387\",\"Token: 6\\u003cbr\\u003ex: 1.369, y: 15.552\",\"Token: 7\\u003cbr\\u003ex: 4.937, y: 14.666\",\"Token: 8\\u003cbr\\u003ex: 4.315, y: 17.060\",\"Token: 9\\u003cbr\\u003ex: 2.370, y: 15.326\",\"Token: 10\\u003cbr\\u003ex: 4.168, y: 14.899\",\"Token: 11\\u003cbr\\u003ex: 4.840, y: 16.512\",\"Token: 12\\u003cbr\\u003ex: 2.579, y: 16.354\",\"Token: 13\\u003cbr\\u003ex: 6.097, y: 13.251\",\"Token: 14\\u003cbr\\u003ex: 4.446, y: 15.080\",\"Token: 15\\u003cbr\\u003ex: 2.940, y: 16.164\",\"Token: 16\\u003cbr\\u003ex: 3.652, y: 15.459\",\"Token: 17\\u003cbr\\u003ex: 2.109, y: 15.487\",\"Token: 18\\u003cbr\\u003ex: 4.231, y: 17.088\",\"Token: 19\\u003cbr\\u003ex: 1.714, y: 14.609\",\"Token: 20\\u003cbr\\u003ex: 2.535, y: 17.023\",\"Token: 21\\u003cbr\\u003ex: 3.229, y: 13.926\",\"Token: 22\\u003cbr\\u003ex: 5.793, y: 15.313\",\"Token: 23\\u003cbr\\u003ex: 4.202, y: 13.009\",\"Token: 24\\u003cbr\\u003ex: 3.860, y: 15.667\",\"Token: 25\\u003cbr\\u003ex: 4.656, y: 15.084\",\"Token: 26\\u003cbr\\u003ex: 1.297, y: 15.030\",\"Token: 27\\u003cbr\\u003ex: 3.880, y: 14.314\",\"Token: 28\\u003cbr\\u003ex: 4.538, y: 16.679\",\"Token: 29\\u003cbr\\u003ex: 3.197, y: 13.731\",\"Token: 30\\u003cbr\\u003ex: 2.169, y: 16.566\",\"Token: 31\\u003cbr\\u003ex: 2.763, y: 15.238\",\"Token: 32\\u003cbr\\u003ex: 4.179, y: 15.299\",\"Token: 33\\u003cbr\\u003ex: 3.179, y: 15.631\",\"Token: 34\\u003cbr\\u003ex: 4.512, y: 13.680\",\"Token: 35\\u003cbr\\u003ex: 4.342, y: 14.638\",\"Token: 36\\u003cbr\\u003ex: 6.174, y: 13.142\",\"Token: 37\\u003cbr\\u003ex: 4.513, y: 15.682\",\"Token: 38\\u003cbr\\u003ex: 2.987, y: 13.550\",\"Token: 39\\u003cbr\\u003ex: 4.214, y: 15.445\",\"Token: 40\\u003cbr\\u003ex: 5.004, y: 15.900\",\"Token: 41\\u003cbr\\u003ex: 3.216, y: 17.073\",\"Token: 42\\u003cbr\\u003ex: 3.059, y: 14.188\",\"Token: 43\\u003cbr\\u003ex: 5.547, y: 14.953\",\"Token: 44\\u003cbr\\u003ex: 5.785, y: 13.513\",\"Token: 45\\u003cbr\\u003ex: 1.783, y: 15.912\",\"Token: 46\\u003cbr\\u003ex: 5.567, y: 13.977\",\"Token: 47\\u003cbr\\u003ex: 5.934, y: 13.655\",\"Token: 48\\u003cbr\\u003ex: 2.693, y: 15.048\",\"Token: 49\\u003cbr\\u003ex: 2.740, y: 15.868\",\"Token: 50\\u003cbr\\u003ex: 4.137, y: 14.155\",\"Token: 51\\u003cbr\\u003ex: 3.616, y: 16.251\",\"Token: 52\\u003cbr\\u003ex: 2.289, y: 16.176\",\"Token: 53\\u003cbr\\u003ex: 1.695, y: 15.154\",\"Token: 54\\u003cbr\\u003ex: 3.703, y: 15.005\",\"Token: 55\\u003cbr\\u003ex: 4.081, y: 12.779\",\"Token: 56\\u003cbr\\u003ex: 1.828, y: 15.324\",\"Token: 57\\u003cbr\\u003ex: 2.935, y: 16.161\",\"Token: 58\\u003cbr\\u003ex: 3.264, y: 14.641\",\"Token: 59\\u003cbr\\u003ex: 4.424, y: 13.672\",\"Token: 60\\u003cbr\\u003ex: 4.538, y: 14.315\",\"Token: 61\\u003cbr\\u003ex: 4.802, y: 14.976\",\"Token: 62\\u003cbr\\u003ex: 5.674, y: 15.562\",\"Token: 63\\u003cbr\\u003ex: 3.402, y: 16.525\",\"Token: 64\\u003cbr\\u003ex: 5.373, y: 13.408\",\"Token: 65\\u003cbr\\u003ex: 3.255, y: 14.421\",\"Token: 66\\u003cbr\\u003ex: 4.083, y: 17.171\",\"Token: 67\\u003cbr\\u003ex: 3.352, y: 13.703\",\"Token: 68\\u003cbr\\u003ex: 4.255, y: 14.707\",\"Token: 69\\u003cbr\\u003ex: 1.856, y: 15.537\",\"Token: 70\\u003cbr\\u003ex: 3.592, y: 13.002\",\"Token: 71\\u003cbr\\u003ex: 2.770, y: 14.451\",\"Token: 72\\u003cbr\\u003ex: 3.363, y: 15.227\",\"Token: 73\\u003cbr\\u003ex: 3.742, y: 14.329\",\"Token: 74\\u003cbr\\u003ex: 3.642, y: 14.735\",\"Token: 75\\u003cbr\\u003ex: 5.601, y: 14.053\",\"Token: 76\\u003cbr\\u003ex: 1.621, y: 15.769\",\"Token: 77\\u003cbr\\u003ex: 6.215, y: 13.463\",\"Token: 78\\u003cbr\\u003ex: 4.305, y: 15.738\",\"Token: 79\\u003cbr\\u003ex: 2.831, y: 16.527\",\"Token: 80\\u003cbr\\u003ex: 3.261, y: 14.911\",\"Token: 81\\u003cbr\\u003ex: 2.638, y: 14.030\",\"Token: 82\\u003cbr\\u003ex: 2.762, y: 15.733\",\"Token: 83\\u003cbr\\u003ex: 3.207, y: 14.915\",\"Token: 84\\u003cbr\\u003ex: 5.224, y: 14.666\",\"Token: 85\\u003cbr\\u003ex: 1.391, y: 15.150\",\"Token: 86\\u003cbr\\u003ex: 4.990, y: 13.531\",\"Token: 87\\u003cbr\\u003ex: 2.596, y: 17.079\",\"Token: 88\\u003cbr\\u003ex: 5.023, y: 15.488\",\"Token: 89\\u003cbr\\u003ex: 2.500, y: 15.753\",\"Token: 90\\u003cbr\\u003ex: 2.562, y: 15.158\",\"Token: 91\\u003cbr\\u003ex: 4.560, y: 15.927\",\"Token: 92\\u003cbr\\u003ex: 2.911, y: 16.582\",\"Token: 93\\u003cbr\\u003ex: 5.193, y: 14.671\",\"Token: 94\\u003cbr\\u003ex: 4.035, y: 13.944\",\"Token: 95\\u003cbr\\u003ex: 4.309, y: 13.164\",\"Token: 96\\u003cbr\\u003ex: 3.343, y: 13.218\",\"Token: 97\\u003cbr\\u003ex: 2.667, y: 16.477\",\"Token: 98\\u003cbr\\u003ex: 2.162, y: 16.124\",\"Token: 99\\u003cbr\\u003ex: 5.208, y: 13.862\",\"Token: SEP\\u003cbr\\u003ex: 3.611, y: 14.618\"],\"marker\":{\"color\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"size\":10},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"SEP\"],\"textposition\":\"top center\",\"x\":{\"dtype\":\"f4\",\"bdata\":\"B\\u002fdCQNsmMUDD9idAIKwOQKaTDkDUhqxA5kqvP+j8nUB4EYpAArUXQLZhhUAR5JpAqg4lQG4Zw0DmQY5AHis8QHu+aUDo9wZAhmKHQDZ02z\\u002fDPyJAfKpOQGRcuUBqc4ZAkwl3QHn7lEC9EqY\\u002fclN4QNQ5kUDknUxAKMwKQKrSMEBVvYVAm21LQBljkECi74pADJXFQORqkEAmLT9AjtmGQAcdoEA9zk1AtcNDQDeDsUCnHblAezzkP5MnskC15r1AGlUsQDhgL0AIYoRA5nNnQJd3EkCj8Ng\\u002frPZsQIGTgkB7AOo\\u002fHdI7QNzkUECIlI1ARzqRQJaomUAhj7VA9LhZQDfxq0B6VlBAfqSCQCiFVkAXJYhArJftP+beZUACQzFAFkNXQNx3b0BWHWlAUT6zQKCKzz\\u002fz4MZARsGJQDA0NUCWrFBAWdgoQMbJMEBjPU1ApSenQMULsj9AsZ9A+CQmQFG+oECZACBAJPAjQHPukUCxTTpA0SmmQM0fgUCP4IlAu+5VQICtKkCoYQpAsaqmQFAiZ0A=\"},\"y\":{\"dtype\":\"f4\",\"bdata\":\"Ezl4QSMfaUHh82pBSRluQRZNiEGBL1ZBp9N4Qe+makHPe4hBRTl1QXBjbkFoGIRBH9SCQR4CVEFbR3FBplCBQXhWd0HDy3dB6rOIQYi+aUF3LohBLc9eQZ4BdUEHJFBBlax6QZ1YcUEoeXBBfwRlQeNthUH4s1tBrIaEQVDPc0GcyXRBWBd6QWriWkGmNmpBRUdSQd7oekFbzFhBYR93QVNofkGRlIhBcgBjQQc\\u002fb0HtNlhBkZZ+QUGiX0H4e1pBlsZwQdzjfUH3emJBDwKCQZpogUFGd3JB7hNwQTt4TEFoLXVB+0mBQWxBakHEwFpBiQplQRadb0Hb\\u002fHhBDDSEQWOGVkE3u2ZBsF2JQZA+W0EKTmtBNJl4QUIGUEEFNmdBpaFzQSpFZUH9w2tBaNhgQZtOfEFjZ1dBEtB7QSY3hEFUlG5BdnlgQQ+6e0Gaom5Bj6lqQTtockFLfVhBbqKIQYXPd0FrC3xBb4VyQbDWfkFpp4RB9L1qQY4aX0FjoVJBGHtTQerRg0Fv\\u002foBBbsldQWHjaUE=\"},\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Token: 0\\u003cbr\\u003ex: -3.620, y: 1.590\",\"Token: 1\\u003cbr\\u003ex: -2.509, y: 2.693\",\"Token: 2\\u003cbr\\u003ex: -3.553, y: 3.675\",\"Token: 3\\u003cbr\\u003ex: -2.949, y: 3.614\",\"Token: 4\\u003cbr\\u003ex: -2.087, y: 2.726\",\"Token: 5\\u003cbr\\u003ex: 1.264, y: 1.938\",\"Token: 6\\u003cbr\\u003ex: -4.942, y: 5.460\",\"Token: 7\\u003cbr\\u003ex: -0.783, y: 2.426\",\"Token: 8\\u003cbr\\u003ex: -0.070, y: 2.593\",\"Token: 9\\u003cbr\\u003ex: -4.056, y: 2.874\",\"Token: 10\\u003cbr\\u003ex: -3.642, y: 2.587\",\"Token: 11\\u003cbr\\u003ex: 0.146, y: 2.468\",\"Token: 12\\u003cbr\\u003ex: -5.035, y: 5.980\",\"Token: 13\\u003cbr\\u003ex: 3.130, y: 1.669\",\"Token: 14\\u003cbr\\u003ex: -2.159, y: 0.698\",\"Token: 15\\u003cbr\\u003ex: -5.129, y: 5.641\",\"Token: 16\\u003cbr\\u003ex: -0.794, y: 1.546\",\"Token: 17\\u003cbr\\u003ex: -4.508, y: 6.103\",\"Token: 18\\u003cbr\\u003ex: -3.842, y: 2.139\",\"Token: 19\\u003cbr\\u003ex: -3.412, y: 0.835\",\"Token: 20\\u003cbr\\u003ex: -2.411, y: 1.971\",\"Token: 21\\u003cbr\\u003ex: -2.245, y: 1.582\",\"Token: 22\\u003cbr\\u003ex: 2.498, y: 2.055\",\"Token: 23\\u003cbr\\u003ex: -2.560, y: 1.224\",\"Token: 24\\u003cbr\\u003ex: -2.495, y: 2.951\",\"Token: 25\\u003cbr\\u003ex: -2.811, y: 1.757\",\"Token: 26\\u003cbr\\u003ex: -3.631, y: 1.033\",\"Token: 27\\u003cbr\\u003ex: -1.937, y: 1.625\",\"Token: 28\\u003cbr\\u003ex: -3.028, y: 2.448\",\"Token: 29\\u003cbr\\u003ex: -2.028, y: 2.392\",\"Token: 30\\u003cbr\\u003ex: -3.221, y: 3.816\",\"Token: 31\\u003cbr\\u003ex: -3.806, y: 4.563\",\"Token: 32\\u003cbr\\u003ex: -1.897, y: 1.025\",\"Token: 33\\u003cbr\\u003ex: -3.270, y: 0.724\",\"Token: 34\\u003cbr\\u003ex: 2.646, y: 1.869\",\"Token: 35\\u003cbr\\u003ex: -1.005, y: 1.822\",\"Token: 36\\u003cbr\\u003ex: 3.567, y: 1.686\",\"Token: 37\\u003cbr\\u003ex: -0.678, y: 1.086\",\"Token: 38\\u003cbr\\u003ex: 0.079, y: 1.336\",\"Token: 39\\u003cbr\\u003ex: -2.075, y: 2.452\",\"Token: 40\\u003cbr\\u003ex: 0.354, y: 2.419\",\"Token: 41\\u003cbr\\u003ex: -0.275, y: 2.403\",\"Token: 42\\u003cbr\\u003ex: -2.961, y: 2.642\",\"Token: 43\\u003cbr\\u003ex: -0.626, y: 2.064\",\"Token: 44\\u003cbr\\u003ex: 3.341, y: 1.943\",\"Token: 45\\u003cbr\\u003ex: -4.557, y: 4.388\",\"Token: 46\\u003cbr\\u003ex: -1.308, y: 2.151\",\"Token: 47\\u003cbr\\u003ex: 2.178, y: 1.605\",\"Token: 48\\u003cbr\\u003ex: -2.544, y: 0.953\",\"Token: 49\\u003cbr\\u003ex: -5.051, y: 4.895\",\"Token: 50\\u003cbr\\u003ex: -2.246, y: 1.624\",\"Token: 51\\u003cbr\\u003ex: 0.641, y: 1.133\",\"Token: 52\\u003cbr\\u003ex: -4.699, y: 6.360\",\"Token: 53\\u003cbr\\u003ex: -4.727, y: 6.391\",\"Token: 54\\u003cbr\\u003ex: -1.955, y: 2.120\",\"Token: 55\\u003cbr\\u003ex: 1.771, y: 1.393\",\"Token: 56\\u003cbr\\u003ex: -4.341, y: 6.175\",\"Token: 57\\u003cbr\\u003ex: -4.338, y: 4.528\",\"Token: 58\\u003cbr\\u003ex: -3.780, y: 1.333\",\"Token: 59\\u003cbr\\u003ex: 0.326, y: 1.154\",\"Token: 60\\u003cbr\\u003ex: -1.229, y: 2.255\",\"Token: 61\\u003cbr\\u003ex: -3.865, y: 2.772\",\"Token: 62\\u003cbr\\u003ex: 3.433, y: 1.703\",\"Token: 63\\u003cbr\\u003ex: -4.213, y: 5.103\",\"Token: 64\\u003cbr\\u003ex: 2.698, y: 1.918\",\"Token: 65\\u003cbr\\u003ex: 0.841, y: 2.185\",\"Token: 66\\u003cbr\\u003ex: -2.467, y: 1.651\",\"Token: 67\\u003cbr\\u003ex: -5.248, y: 5.257\",\"Token: 68\\u003cbr\\u003ex: -2.132, y: 0.656\",\"Token: 69\\u003cbr\\u003ex: -3.401, y: 2.911\",\"Token: 70\\u003cbr\\u003ex: 0.176, y: 1.248\",\"Token: 71\\u003cbr\\u003ex: -4.492, y: 4.724\",\"Token: 72\\u003cbr\\u003ex: -2.543, y: 3.246\",\"Token: 73\\u003cbr\\u003ex: -4.465, y: 3.824\",\"Token: 74\\u003cbr\\u003ex: -0.185, y: 1.282\",\"Token: 75\\u003cbr\\u003ex: 3.012, y: 1.734\",\"Token: 76\\u003cbr\\u003ex: -1.892, y: 1.216\",\"Token: 77\\u003cbr\\u003ex: 3.262, y: 2.011\",\"Token: 78\\u003cbr\\u003ex: -1.422, y: 2.039\",\"Token: 79\\u003cbr\\u003ex: -4.215, y: 5.957\",\"Token: 80\\u003cbr\\u003ex: -2.813, y: 3.465\",\"Token: 81\\u003cbr\\u003ex: -2.744, y: 2.182\",\"Token: 82\\u003cbr\\u003ex: -3.629, y: 1.841\",\"Token: 83\\u003cbr\\u003ex: -3.968, y: 3.338\",\"Token: 84\\u003cbr\\u003ex: 2.526, y: 1.681\",\"Token: 85\\u003cbr\\u003ex: -2.387, y: 2.930\",\"Token: 86\\u003cbr\\u003ex: -3.247, y: 2.003\",\"Token: 87\\u003cbr\\u003ex: -4.466, y: 5.233\",\"Token: 88\\u003cbr\\u003ex: -2.351, y: 1.050\",\"Token: 89\\u003cbr\\u003ex: -4.454, y: 5.778\",\"Token: 90\\u003cbr\\u003ex: -4.909, y: 5.538\",\"Token: 91\\u003cbr\\u003ex: -3.091, y: 2.055\",\"Token: 92\\u003cbr\\u003ex: -3.016, y: 2.663\",\"Token: 93\\u003cbr\\u003ex: 1.536, y: 1.285\",\"Token: 94\\u003cbr\\u003ex: 1.660, y: 1.604\",\"Token: 95\\u003cbr\\u003ex: 1.659, y: 1.463\",\"Token: 96\\u003cbr\\u003ex: -1.989, y: 1.876\",\"Token: 97\\u003cbr\\u003ex: -5.155, y: 4.996\",\"Token: 98\\u003cbr\\u003ex: -3.329, y: 3.457\",\"Token: 99\\u003cbr\\u003ex: 1.794, y: 2.013\",\"Token: SEP\\u003cbr\\u003ex: -1.847, y: 2.254\"],\"marker\":{\"color\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],\"colorbar\":{\"ticktext\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"SEP\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],\"title\":{\"text\":\"Token ID\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":true,\"size\":10},\"mode\":\"markers+text\",\"text\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"SEP\"],\"textposition\":\"top center\",\"x\":{\"dtype\":\"f4\",\"bdata\":\"prVnwMKOIMDjZWPA3sM8wI+QBcB4xqE\\u002fgyaewEFhSL+TCJC9Pc6BwAEUacBveRU+liChwHJVSEDfJgrAbSKkwA8vS79gP5DAduN1wCtWWsDhThrAFrQPwLneH0D12CPAsbIfwNXqM8CLXGjAnu33vwrPQcApzQHAJiBOwCKPc8DFyvK\\u002fw0tRwCZZKUDZmIC\\u002fRElkQOqxLb901qE9NdIEwF0ZtT5o24y+EoU9wIpWIL+E1FVAMNCRwDFcp7+pZwtAdNAiwBKkocCjuA\\u002fAaRwkP5JglsBSQpfAQDD6v1ao4j8B5orAYdSKwMnyccBA1aY+ikGdvzFjd8Bot1tAns6GwGSmLEBDaVc\\u002f+98dwC3up8CucQjA96JZwFm9Mz6hu4\\u002fAX7oiwP7fjsAjID2+D8RAQA4f8r8Ty1BAWhC2vx3ehsCFCzTA2KIvwMFBaMBP933ARaghQJfAGMCAz0\\u002fA+OeOwAtvFsCsg47ABxadwMnURcCcAEHAH43EP2+C1D+YZ9Q\\u002fGoz+vx3ypMB\\u002fFVXAfJHlP0x57L8=\"},\"y\":{\"dtype\":\"f4\",\"bdata\":\"+nvLP+VSLEBjOWtAUE1nQEN1LkDgAvg\\u002fwrSuQEpDG0AY8iVARfQ3QO2TJUCj+B1ArVu\\u002fQKqh1T+2pzI\\u002fyYO0QKTrxT8LTMNAK+QIQKrAVT\\u002fSTPw\\u002f3ovKP4aCA0Bit5w\\u002ft9Y8QIzV4D91RoQ\\u002fFfvPP++uHEBSEhlA7EB0QPUBkkDMP4M\\u002fRmQ5Pxgy7z+3O+k\\u002fIMHXP6z3ij\\u002fdBKs\\u002f1+UcQPnMGkAGxxlA2RQpQLkSBEBqr\\u002fg\\u002ft22MQBaqCUBleM0\\u002fBeRzP3OlnEB9488\\u002fXQWRP+iDy0AegMxAKbAHQD5Hsj87nMVAfuiQQIGXqj\\u002fau5M\\u002fgU8QQFViMUAZ8dk\\u002fJk2jQL139T972gtAPmHTP5c6qECK1ic\\u002fRks6QETNnz+uKJdACbhPQNbCdECYC6Q\\u002fvvLdP2C1mz\\u002fItwBA4HoCQOKgvkAiyV1AoKcLQGGZ6z9aqFVAkiLXPyV+O0AdOABAY3enQBNvhj8Z6bhAKjuxQHeKA0BBbipAj3CkP\\u002fldzT8FUrs\\u002fphLwP6rhn0B\\u002fOl1A89EAQAc8EEA=\"},\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scattermap\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermap\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"UMAP Dim 1\"},\"range\":[-6.394102573394775,7.3612380027771]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"UMAP Dim 2\"},\"range\":[-0.9958956241607666,18.82225799560547]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"UMAP Dim 1\"},\"range\":[-6.394102573394775,7.3612380027771]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"UMAP Dim 2\"},\"range\":[-0.9958956241607666,18.82225799560547]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"2D UMAP of W_E (Embeddings)\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"2D UMAP of W_U (Unembeddings)\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"2D UMAP Projections\"},\"height\":600,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ae010b64-324f-4288-bbe4-1c1c3d743dc5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };            </script>        </div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
        "N_DIM_VIS = 2  # <-- CHANGE THIS VALUE to 2 or 3 to switch visualizations\n",
        "umap_n_neighbors = min(5, VOCAB - 1)  # Use smaller n_neighbors for small dataset (max VOCAB-1 = 10)\n",
        "umap_min_dist = 0.1  # Spread points out more\n",
        "umap_metric = 'euclidean' # default: euclidean \n",
        "\n",
        "def visualize_w_e_and_w_u(model):\n",
        "    \"\"\"\n",
        "    Extracts W_E and W_U, applies UMAP to get 2D or 3D projections,\n",
        "    and creates side-by-side interactive plots based on N_DIM_VIS.\n",
        "    \"\"\"\n",
        "    if N_DIM_VIS not in [2, 3]:\n",
        "        raise ValueError(\"N_DIM_VIS must be set to 2 or 3.\")\n",
        "    \n",
        "\n",
        "    print(f\"\\nStarting {N_DIM_VIS}D UMAP visualization  for W_E and W_U... \\n(n_neighbors={umap_n_neighbors}, min_dist={umap_min_dist}, metric={umap_metric})\")\n",
        "    model.eval()\n",
        "\n",
        "    w_e = model.embed.W_E.detach().cpu().numpy()\n",
        "    w_u = model.unembed.W_U.T.detach().cpu().numpy()\n",
        "    \n",
        "    reducer = umap.UMAP(\n",
        "        n_neighbors=umap_n_neighbors,\n",
        "        min_dist=umap_min_dist,\n",
        "        n_components=N_DIM_VIS,\n",
        "        random_state=42, # UMAP is stochastic, so we set a seed for reproducibility\n",
        "        metric=umap_metric,  # Use Euclidean distance for UMAP\n",
        "        # verbose=True,\n",
        "    )\n",
        "\n",
        "    w_e_proj = reducer.fit_transform(w_e)\n",
        "    w_u_proj = reducer.fit_transform(w_u)\n",
        "    labels = [str(d) for d in DIGITS] + ['SEP']\n",
        "\n",
        "    # --- Find common axis ranges across both projections ---\n",
        "    all_proj = np.vstack([w_e_proj, w_u_proj])\n",
        "    min_vals = all_proj.min(axis=0)\n",
        "    max_vals = all_proj.max(axis=0)\n",
        "    \n",
        "    # Add a 10% margin for better visualization\n",
        "    margin = (max_vals - min_vals) * 0.1\n",
        "    ranges = [(min_v - m, max_v + m) for min_v, max_v, m in zip(min_vals, max_vals, margin)]\n",
        "\n",
        "    if N_DIM_VIS == 3:\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=2,\n",
        "            specs=[[{'type': 'scene'}, {'type': 'scene'}]],\n",
        "            subplot_titles=('3D UMAP of W_E (Embeddings)', '3D UMAP of W_U (Unembeddings)')\n",
        "        )\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=w_e_proj[:, 0], y=w_e_proj[:, 1], z=w_e_proj[:, 2],\n",
        "            mode='markers+text', text=labels, textfont=dict(size=10, color='black'),\n",
        "            marker=dict(size=5, color=list(range(VOCAB)), colorscale='viridis'),\n",
        "            hoverinfo='text',\n",
        "            hovertext=[f'Token: {l}<br>x: {x:.2f}, y: {y:.2f}, z: {z:.2f}' for l, x, y, z in zip(labels, w_e_proj[:, 0], w_e_proj[:, 1], w_e_proj[:, 2])],\n",
        "            showlegend=False\n",
        "        ), row=1, col=1)\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=w_u_proj[:, 0], y=w_u_proj[:, 1], z=w_u_proj[:, 2],\n",
        "            mode='markers+text', text=labels, textfont=dict(size=10, color='black'),\n",
        "            marker=dict(\n",
        "                size=5, color=list(range(VOCAB)), colorscale='viridis', showscale=True,\n",
        "                colorbar=dict(title=\"Token ID\", tickvals=list(range(VOCAB)), ticktext=labels)\n",
        "            ),\n",
        "            hoverinfo='text',\n",
        "            hovertext=[f'Token: {l}<br>x: {x:.2f}, y: {y:.2f}, z: {z:.2f}' for l, x, y, z in zip(labels, w_u_proj[:, 0], w_u_proj[:, 1], w_u_proj[:, 2])]\n",
        "        ), row=1, col=2)\n",
        "        fig.update_layout(title_text='3D UMAP Projections', height=700, width=1400)\n",
        "        # Apply the same axis ranges to both 3D scenes\n",
        "        fig.update_scenes(\n",
        "            xaxis_title_text='Dim 1', yaxis_title_text='Dim 2', zaxis_title_text='Dim 3',\n",
        "            xaxis_range=ranges[0], yaxis_range=ranges[1], zaxis_range=ranges[2]\n",
        "        )\n",
        "    else:  # N_DIM_VIS == 2\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=2,\n",
        "            subplot_titles=('2D UMAP of W_E (Embeddings)', '2D UMAP of W_U (Unembeddings)')\n",
        "        )\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=w_e_proj[:, 0], y=w_e_proj[:, 1],\n",
        "            mode='markers+text', text=labels, textposition='top center',\n",
        "            marker=dict(size=10, color=list(range(VOCAB)), colorscale='viridis'),\n",
        "            hoverinfo='text',\n",
        "            hovertext=[f'Token: {l}<br>x: {x:.3f}, y: {y:.3f}' for l, x, y in zip(labels, w_e_proj[:, 0], w_e_proj[:, 1])],\n",
        "            showlegend=False\n",
        "        ), row=1, col=1)\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=w_u_proj[:, 0], y=w_u_proj[:, 1],\n",
        "            mode='markers+text', text=labels, textposition='top center',\n",
        "            marker=dict(\n",
        "                size=10, color=list(range(VOCAB)), colorscale='viridis', showscale=True,\n",
        "                colorbar=dict(title=\"Token ID\", tickvals=list(range(VOCAB)), ticktext=labels)\n",
        "            ),\n",
        "            hoverinfo='text',\n",
        "            hovertext=[f'Token: {l}<br>x: {x:.3f}, y: {y:.3f}' for l, x, y in zip(labels, w_u_proj[:, 0], w_u_proj[:, 1])]\n",
        "        ), row=1, col=2)\n",
        "        fig.update_layout(title_text='2D UMAP Projections', height=600, width=1200, template='plotly_white')\n",
        "        # Apply the same axis ranges to both 2D plots\n",
        "        fig.update_xaxes(title_text=\"UMAP Dim 1\", range=ranges[0])\n",
        "        fig.update_yaxes(title_text=\"UMAP Dim 2\", range=ranges[1])\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "visualize_w_e_and_w_u(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating pairwise angles between all vectors in the original 32D space...\n",
            "\n",
            "--- Pairwise Angles (Degrees) for W_E (Embeddings) in 32D ---\n",
            "|     |     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 |    10 |    11 |    12 |    13 |    14 |    15 |    16 |    17 |    18 |    19 |    20 |    21 |    22 |    23 |    24 |    25 |    26 |    27 |    28 |    29 |    30 |    31 |    32 |    33 |    34 |    35 |    36 |    37 |    38 |    39 |    40 |    41 |    42 |    43 |    44 |    45 |    46 |    47 |    48 |    49 |    50 |    51 |    52 |    53 |    54 |    55 |    56 |    57 |    58 |    59 |    60 |    61 |    62 |    63 |    64 |    65 |    66 |    67 |    68 |    69 |    70 |    71 |    72 |    73 |    74 |    75 |    76 |    77 |    78 |    79 |    80 |    81 |    82 |    83 |    84 |    85 |    86 |    87 |    88 |    89 |    90 |    91 |    92 |    93 |    94 |    95 |    96 |    97 |    98 |    99 |   SEP |\n",
            "|:----|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n",
            "| 0   |   0.0 |  87.6 | 112.6 |  86.7 |  85.1 |  91.5 |  84.8 |  84.0 |  77.3 |  93.8 |  84.1 |  75.1 |  94.9 |  83.0 | 111.4 | 100.2 |  85.6 |  91.6 |  98.5 |  86.4 |  83.2 |  74.1 | 105.8 |  85.0 |  98.8 | 104.2 |  79.6 |  80.7 |  78.0 |  83.1 |  87.5 |  98.7 |  98.9 | 102.0 |  85.9 |  83.2 |  98.8 |  84.8 |  84.1 |  97.3 |  86.8 |  86.2 |  87.9 |  90.6 |  89.9 |  85.8 |  90.7 |  87.0 |  83.3 |  76.1 |  81.9 |  80.6 |  94.7 |  89.9 |  92.0 |  91.9 |  86.6 | 107.5 |  83.7 |  99.1 |  93.2 |  97.8 |  97.5 |  82.5 |  92.5 |  86.8 |  76.9 | 102.8 |  99.1 |  84.1 |  81.6 |  90.2 |  74.3 | 103.2 |  94.1 | 102.0 |  97.8 | 104.1 |  84.0 |  91.1 |  80.4 |  91.3 |  81.2 |  82.6 |  89.8 |  83.7 |  80.5 |  87.2 |  81.8 |  88.5 |  86.3 |  91.5 |  81.4 |  99.9 |  97.9 |  99.9 |  75.1 |  95.3 |  91.0 | 111.7 |  94.7 |\n",
            "| 1   |  87.6 |   0.0 | 100.7 |  72.9 |  73.5 | 105.4 |  91.6 |  87.8 | 117.6 |  74.9 |  81.9 |  95.9 |  79.2 | 104.2 |  94.8 |  79.8 | 101.6 |  73.8 |  97.0 |  90.6 |  84.4 |  87.1 |  93.0 | 110.3 |  82.5 |  94.5 |  99.5 |  86.3 |  83.8 |  84.5 |  87.9 |  82.5 |  88.8 |  80.3 |  96.6 | 101.5 | 106.8 |  90.3 |  98.9 |  85.5 | 101.8 | 108.6 |  95.6 |  98.3 |  90.7 |  79.2 |  82.6 |  93.0 |  78.4 |  77.6 |  88.7 |  92.6 |  75.4 |  84.6 |  92.4 | 113.4 |  77.3 |  93.1 |  94.1 |  95.1 |  79.4 |  83.6 | 103.4 |  72.8 |  96.3 |  97.1 |  85.7 |  90.5 |  94.7 |  72.5 |  89.5 |  68.6 |  82.4 |  79.8 |  97.7 |  92.6 |  78.6 |  87.7 |  92.6 |  80.9 |  76.6 |  71.2 |  86.9 | 100.8 |  84.2 |  88.9 |  78.1 |  81.6 |  93.0 |  94.7 |  90.7 |  98.5 |  93.2 |  90.3 |  88.8 | 116.4 | 103.0 |  81.1 |  90.6 | 109.0 |  81.1 |\n",
            "| 2   | 112.6 | 100.7 |   0.0 |  83.1 |  81.1 | 109.1 |  97.6 |  99.1 | 106.7 |  91.6 |  76.7 | 100.2 |  86.9 | 110.1 |  77.1 |  77.2 |  97.1 |  81.2 |  83.9 | 101.0 |  82.7 |  85.3 |  97.2 |  78.2 |  96.3 |  78.6 |  87.7 |  82.0 | 105.7 |  82.8 |  77.6 |  81.1 |  80.5 |  80.4 | 103.4 |  96.0 |  98.6 | 100.9 |  93.7 |  91.3 | 112.6 |  81.5 |  75.8 |  93.8 | 114.6 |  89.6 | 100.5 |  93.7 |  96.9 |  90.3 |  90.5 | 114.8 |  89.4 |  77.3 |  83.8 |  96.5 |  77.8 |  79.2 |  76.0 |  86.8 | 110.3 |  90.0 | 101.6 | 102.4 |  97.0 | 102.7 |  91.9 |  82.7 |  94.3 |  77.4 |  94.5 |  97.5 |  99.0 |  82.0 |  98.8 |  96.0 |  79.1 |  91.4 | 113.2 |  90.9 |  94.3 |  92.6 |  83.2 |  83.5 | 110.7 |  78.2 |  91.1 |  90.4 | 103.7 |  76.3 |  81.5 |  96.9 |  79.2 |  92.0 |  95.2 |  85.0 |  82.7 |  78.3 |  76.5 |  95.6 |  91.6 |\n",
            "| 3   |  86.7 |  72.9 |  83.1 |   0.0 |  87.9 |  87.5 |  87.8 |  85.1 | 110.4 |  80.3 |  83.9 |  90.4 |  87.1 | 101.7 |  90.8 |  88.4 |  97.2 |  69.2 |  86.8 |  93.2 |  96.0 |  85.3 | 106.5 |  87.3 |  93.8 |  90.8 | 107.7 | 111.3 |  82.9 |  98.7 |  81.1 |  87.8 |  78.6 |  86.5 | 104.2 | 110.0 |  99.6 |  84.8 |  86.0 |  97.7 |  85.5 |  90.5 |  76.6 |  89.2 | 101.7 |  83.3 |  91.0 |  88.3 |  77.8 |  89.9 |  78.7 | 104.7 |  78.6 |  74.6 | 102.9 | 111.1 |  74.5 |  86.6 |  92.5 | 100.7 |  91.2 |  87.1 | 101.0 |  85.3 |  91.1 |  89.6 |  85.7 | 106.6 |  87.6 |  81.3 |  79.6 |  77.0 |  85.3 |  99.9 |  79.5 | 112.4 |  79.8 |  86.3 | 103.6 |  94.5 |  76.6 |  89.9 |  90.5 |  80.9 |  94.0 |  87.7 |  81.9 | 102.6 |  86.2 |  89.0 |  69.9 |  85.1 |  90.0 | 109.9 |  92.9 |  89.0 |  93.9 |  92.7 | 105.4 |  91.8 |  85.1 |\n",
            "| 4   |  85.1 |  73.5 |  81.1 |  87.9 |   0.0 | 113.6 |  91.1 |  89.3 | 105.2 |  78.7 |  83.6 |  88.2 |  81.8 |  98.5 |  98.3 |  88.6 | 114.3 |  77.9 |  99.5 |  86.5 |  81.0 |  94.8 |  97.9 |  92.3 |  97.8 |  76.5 |  84.2 | 101.2 |  99.1 |  80.3 |  76.0 |  88.5 |  85.7 |  92.4 | 104.9 | 101.6 | 109.2 |  92.6 |  97.8 |  76.5 |  92.7 |  85.3 |  95.4 |  98.6 |  97.4 |  93.4 | 109.9 |  95.5 | 107.7 |  81.3 | 100.3 | 101.7 |  81.9 |  79.2 |  85.9 |  97.6 |  90.1 |  83.0 |  84.5 |  89.6 | 101.0 |  85.7 |  88.0 |  89.1 | 104.4 | 112.0 |  83.9 |  74.9 |  77.1 |  84.0 |  96.3 |  98.5 |  86.2 | 100.2 |  95.9 | 101.0 | 102.7 | 100.3 |  83.7 |  86.1 |  75.7 |  83.6 |  77.3 | 100.4 |  92.8 |  81.3 |  81.9 |  72.6 |  81.0 |  80.8 |  98.5 |  90.8 |  82.3 | 109.1 | 101.3 | 120.2 |  75.5 |  83.7 |  92.7 | 104.7 | 100.1 |\n",
            "| 5   |  91.5 | 105.4 | 109.1 |  87.5 | 113.6 |   0.0 |  85.0 |  85.7 |  79.0 |  92.6 | 104.7 |  79.8 | 103.8 |  68.5 |  95.1 |  96.3 |  83.9 | 106.9 |  93.4 |  79.7 | 107.3 | 108.0 |  82.5 |  79.4 |  85.2 | 104.8 |  87.7 | 105.6 |  91.5 | 124.0 |  98.2 |  93.2 |  84.3 |  85.3 |  74.8 |  91.7 |  72.2 |  74.4 |  94.3 |  88.2 |  70.3 |  92.7 |  80.9 |  89.7 |  67.5 | 100.2 |  73.7 |  76.8 |  89.5 | 103.6 |  96.7 |  79.8 | 102.7 | 102.9 |  90.3 |  70.0 |  91.2 |  99.6 | 101.4 |  84.7 |  77.3 |  99.5 |  70.4 |  98.0 |  65.3 |  69.7 | 102.7 | 114.9 |  85.3 | 125.8 |  82.6 | 107.0 |  90.3 | 102.5 |  76.9 |  75.4 |  86.7 |  71.2 |  89.5 |  91.5 |  93.5 | 102.0 |  90.5 |  87.2 |  78.5 |  87.4 |  81.2 |  95.4 |  98.2 |  96.8 |  90.0 |  92.1 |  94.2 |  83.8 |  75.8 |  66.9 | 111.4 | 107.8 | 110.8 |  61.4 |  93.5 |\n",
            "| 6   |  84.8 |  91.6 |  97.6 |  87.8 |  91.1 |  85.0 |   0.0 | 113.9 |  87.5 |  89.0 |  87.2 |  91.5 |  85.3 |  95.8 |  94.6 |  81.4 |  97.0 |  77.1 |  79.5 |  83.2 |  87.7 |  91.5 | 104.6 |  87.7 |  94.6 |  89.3 |  75.4 |  97.8 |  74.2 |  97.1 |  74.7 |  83.7 |  79.6 | 111.9 | 115.2 |  95.6 |  98.2 |  94.4 |  96.8 |  78.8 |  96.8 |  97.0 |  80.2 |  90.4 | 100.0 |  77.9 | 106.5 | 118.1 |  82.3 |  83.3 |  82.3 |  88.6 |  80.2 |  94.5 |  86.7 |  98.7 |  87.2 |  78.4 | 100.9 |  99.3 |  86.9 |  80.0 |  99.7 |  83.2 | 102.1 |  98.2 |  95.8 |  76.4 |  82.3 | 100.4 |  91.7 |  78.8 |  85.2 | 104.3 | 108.5 | 100.9 |  94.6 | 107.9 |  86.7 |  82.6 |  93.1 |  77.0 |  97.5 |  81.9 |  97.9 |  78.4 |  73.0 |  75.0 | 107.6 |  89.5 |  81.2 |  84.4 |  94.8 | 107.4 |  93.3 |  88.6 |  88.4 |  81.4 |  94.2 | 100.8 |  87.8 |\n",
            "| 7   |  84.0 |  87.8 |  99.1 |  85.1 |  89.3 |  85.7 | 113.9 |   0.0 |  73.0 |  82.3 |  91.6 |  82.0 |  96.2 |  86.6 | 106.1 |  96.7 |  92.5 |  87.4 |  95.5 |  87.2 |  86.8 |  86.5 |  74.0 |  92.0 |  85.0 |  86.9 | 100.9 |  87.9 |  86.2 |  88.5 | 110.1 | 102.8 | 113.0 |  96.2 |  91.5 |  77.7 |  90.4 | 113.3 |  75.2 |  88.9 |  72.4 |  78.1 |  95.7 |  72.2 |  73.5 | 106.4 |  78.8 |  80.7 | 108.5 | 104.7 |  96.2 |  82.1 | 100.4 |  91.5 |  78.0 |  87.6 |  89.3 | 102.3 |  94.9 | 108.9 |  75.8 |  79.4 |  74.5 |  89.2 |  82.9 |  78.5 |  96.2 | 105.5 |  94.1 |  88.1 |  79.5 |  87.8 |  91.9 |  84.2 |  84.7 |  92.2 |  85.1 |  85.8 | 105.1 |  99.0 | 102.6 |  88.9 |  98.3 |  91.0 |  76.4 | 110.9 | 105.0 |  89.9 |  85.1 |  85.7 |  87.8 |  94.6 |  92.1 |  76.2 |  89.1 |  92.2 |  92.2 | 109.2 |  98.0 |  73.2 |  84.0 |\n",
            "| 8   |  77.3 | 117.6 | 106.7 | 110.4 | 105.2 |  79.0 |  87.5 |  73.0 |   0.0 | 102.8 |  94.3 |  69.5 |  92.3 |  78.3 | 103.3 | 103.3 |  80.7 | 114.0 |  73.4 |  84.8 |  81.8 |  84.7 |  75.0 |  78.7 |  83.7 |  86.7 |  78.0 |  74.5 |  77.2 |  90.1 |  91.3 |  93.8 | 107.3 | 103.7 |  85.9 |  81.4 |  78.5 |  96.3 |  72.9 |  80.9 |  68.0 |  69.4 |  85.8 |  83.2 |  75.1 |  91.4 | 100.3 |  82.5 |  95.9 |  96.9 |  80.1 |  70.5 |  98.0 |  98.9 |  98.2 |  80.4 |  90.9 | 105.2 | 112.2 | 109.0 |  87.0 |  84.7 |  78.0 |  84.4 |  95.6 |  84.0 |  92.9 |  89.3 |  92.7 |  93.2 |  95.3 | 106.8 |  96.5 | 102.6 |  95.9 |  79.7 |  94.3 |  84.9 |  90.5 |  96.7 | 107.5 |  97.4 |  99.3 |  88.2 |  89.6 | 104.0 | 106.4 |  87.0 |  78.8 |  87.5 | 104.2 |  85.4 |  96.3 |  76.1 |  83.3 |  81.7 |  84.3 | 105.3 |  90.4 |  79.1 |  94.3 |\n",
            "| 9   |  93.8 |  74.9 |  91.6 |  80.3 |  78.7 |  92.6 |  89.0 |  82.3 | 102.8 |   0.0 |  73.6 |  79.8 |  92.3 |  95.5 |  80.2 |  89.3 | 100.8 |  66.6 |  77.8 |  97.8 | 103.4 |  99.8 |  84.7 | 109.8 | 102.7 |  88.3 |  90.7 | 105.5 |  91.5 |  87.2 |  82.3 |  90.2 |  90.7 |  87.7 | 105.9 |  90.2 | 108.6 |  82.5 |  87.0 |  79.6 | 106.6 |  93.8 |  89.3 |  92.1 |  95.1 |  86.2 |  96.3 | 113.7 |  89.2 |  99.0 | 112.6 |  86.6 |  90.3 |  81.3 |  97.4 | 109.2 |  85.9 |  86.5 | 101.9 |  83.5 |  88.6 |  80.6 |  98.7 |  82.7 |  94.5 |  86.7 |  85.1 |  84.2 |  78.2 |  79.9 |  90.4 |  99.6 | 101.5 |  81.6 |  80.4 |  91.8 |  93.0 |  94.6 |  99.4 |  76.2 |  81.0 | 103.7 |  79.3 |  74.7 |  87.7 |  86.1 |  81.3 |  85.2 |  91.3 |  94.4 |  91.0 |  87.1 |  75.8 |  83.9 |  99.9 | 100.6 | 104.7 |  96.6 | 105.3 |  98.2 |  88.3 |\n",
            "| 10  |  84.1 |  81.9 |  76.7 |  83.9 |  83.6 | 104.7 |  87.2 |  91.6 |  94.3 |  73.6 |   0.0 |  84.4 |  80.1 |  92.3 |  87.4 |  87.7 |  99.1 |  80.7 |  89.2 |  93.7 |  80.3 |  79.8 |  89.3 |  82.9 | 101.3 |  72.4 |  82.4 |  88.9 | 104.3 |  96.2 |  87.7 |  85.4 |  71.0 | 109.6 |  96.8 |  78.2 | 112.2 |  72.5 |  97.5 |  81.3 | 104.5 |  94.2 |  99.9 |  92.2 | 110.1 |  76.3 |  90.6 | 100.8 |  82.5 |  90.9 |  86.3 |  89.8 |  98.5 |  80.3 | 106.2 | 110.5 |  85.3 | 100.2 |  89.0 |  99.8 |  94.7 |  79.4 |  95.9 |  95.3 |  90.7 | 103.6 |  99.4 |  85.6 |  71.5 |  71.6 |  93.8 |  80.5 |  88.2 |  84.5 | 102.8 |  97.5 |  77.5 |  96.8 | 107.0 |  94.4 |  92.4 |  86.7 |  81.0 |  88.5 | 110.5 |  78.5 |  77.8 |  85.7 |  85.3 |  94.6 |  86.9 |  89.2 |  81.9 |  79.1 | 103.7 |  84.2 |  75.9 |  81.8 |  97.3 |  96.7 |  87.8 |\n",
            "| 11  |  75.1 |  95.9 | 100.2 |  90.4 |  88.2 |  79.8 |  91.5 |  82.0 |  69.5 |  79.8 |  84.4 |   0.0 | 109.1 |  85.0 |  84.4 |  94.6 |  99.2 | 102.0 |  76.3 |  75.5 |  83.6 | 101.2 |  70.5 |  81.1 |  91.1 | 106.5 |  85.5 |  95.2 |  72.9 | 101.8 |  87.5 | 108.2 | 108.9 |  92.8 |  76.9 |  92.7 |  79.7 |  79.9 |  97.7 |  90.9 |  63.9 |  81.0 |  87.2 |  81.0 |  87.5 |  87.3 | 101.8 |  95.5 |  83.3 |  83.7 | 104.7 |  82.5 | 101.4 |  93.2 |  89.8 |  79.5 |  91.7 | 101.5 | 105.2 |  91.8 | 104.2 |  94.7 |  67.8 |  87.0 |  82.5 |  78.0 |  78.7 |  85.7 |  87.5 |  90.0 |  82.3 | 105.8 |  96.2 | 112.2 |  80.7 |  80.8 |  96.7 |  75.1 |  94.3 | 101.6 | 101.8 | 109.1 |  83.5 |  85.4 |  74.8 |  90.0 |  94.8 |  95.2 |  84.0 | 102.9 |  98.9 |  89.5 |  96.7 |  76.2 | 108.9 |  91.0 |  94.5 |  87.1 | 113.1 |  97.0 |  99.7 |\n",
            "| 12  |  94.9 |  79.2 |  86.9 |  87.1 |  81.8 | 103.8 |  85.3 |  96.2 |  92.3 |  92.3 |  80.1 | 109.1 |   0.0 | 100.5 |  98.4 |  72.1 |  99.7 |  83.1 | 100.8 |  93.9 |  74.1 |  83.2 | 100.6 |  91.8 |  83.5 |  90.7 |  88.2 | 106.6 |  81.5 |  80.2 |  78.5 |  74.2 |  93.0 |  92.0 | 100.8 |  97.1 | 110.2 | 103.7 | 110.0 |  99.8 |  97.5 |  93.8 |  87.3 | 101.8 | 103.5 |  86.9 | 100.2 | 103.6 |  98.2 |  84.8 |  95.3 |  95.2 |  66.1 |  70.5 | 100.5 |  99.9 |  70.9 |  85.0 |  98.4 | 104.0 |  91.2 |  78.4 | 108.1 |  84.8 | 110.5 | 110.8 |  97.6 |  75.1 |  85.6 |  80.0 | 103.6 |  73.9 |  87.4 |  74.6 | 102.7 | 107.6 | 100.5 |  97.5 |  94.5 |  77.3 |  73.6 |  78.8 |  96.5 |  76.4 | 106.5 |  88.1 |  85.8 |  89.7 | 106.6 |  72.9 |  79.2 |  90.2 |  74.2 |  94.0 |  86.9 |  95.4 |  88.0 |  75.1 |  71.3 | 103.5 |  76.7 |\n",
            "| 13  |  83.0 | 104.2 | 110.1 | 101.7 |  98.5 |  68.5 |  95.8 |  86.6 |  78.3 |  95.5 |  92.3 |  85.0 | 100.5 |   0.0 | 109.0 | 103.8 |  87.6 |  98.7 | 100.3 |  87.2 |  99.0 |  95.7 |  68.4 |  94.8 | 118.4 |  90.7 |  88.2 |  97.3 |  98.4 | 102.5 |  91.6 |  99.3 |  91.5 |  98.9 |  67.6 |  74.4 |  54.9 |  82.1 |  76.0 | 109.3 |  85.6 |  77.5 | 107.8 |  92.5 |  64.1 | 106.7 |  84.6 |  61.8 |  95.3 | 107.7 |  86.5 |  76.4 | 111.4 | 115.2 | 107.3 |  67.8 | 104.8 | 101.8 |  82.5 |  80.4 |  83.1 | 107.5 |  61.1 |  98.9 |  64.9 |  78.7 |  96.5 |  97.9 |  90.9 | 111.8 |  70.2 |  98.7 | 107.5 | 109.2 |  80.1 |  64.1 | 103.1 |  67.3 |  86.5 |  95.3 |  94.7 | 105.7 |  93.3 |  92.3 |  68.2 | 114.4 |  89.9 |  99.7 |  89.7 | 111.3 | 104.8 | 101.1 |  87.7 |  79.8 |  67.1 |  72.4 |  88.1 | 100.5 | 100.2 |  77.0 | 112.6 |\n",
            "| 14  | 111.4 |  94.8 |  77.1 |  90.8 |  98.3 |  95.1 |  94.6 | 106.1 | 103.3 |  80.2 |  87.4 |  84.4 |  98.4 | 109.0 |   0.0 | 100.3 |  79.2 | 101.3 |  84.6 | 109.5 | 103.9 |  99.4 |  91.1 |  84.4 |  83.0 |  81.7 |  98.8 |  89.2 |  87.1 | 101.2 |  81.3 | 100.6 |  82.5 |  82.1 |  83.2 |  84.6 |  88.1 |  68.3 | 109.4 |  83.6 |  93.2 | 100.5 |  93.8 |  81.6 | 103.9 |  90.6 |  82.3 | 107.0 |  81.5 | 100.7 |  97.9 |  96.4 | 100.6 |  93.5 |  80.1 |  92.5 |  95.6 |  80.2 |  93.7 |  80.8 | 103.4 |  73.6 | 100.7 |  89.2 |  99.9 |  92.2 |  71.5 |  90.0 |  69.9 |  85.6 |  94.4 |  91.4 |  92.2 |  86.2 |  72.1 |  82.6 |  75.7 |  89.2 |  81.6 |  96.5 |  82.8 |  80.2 |  83.8 |  79.3 |  90.7 |  81.4 |  83.6 | 106.3 |  86.8 |  92.2 |  89.1 |  78.3 |  97.0 |  78.0 | 107.0 |  83.3 |  92.1 |  80.7 |  91.8 |  84.6 |  78.5 |\n",
            "| 15  | 100.2 |  79.8 |  77.2 |  88.4 |  88.6 |  96.3 |  81.4 |  96.7 | 103.3 |  89.3 |  87.7 |  94.6 |  72.1 | 103.8 | 100.3 |   0.0 | 105.7 |  73.9 |  92.8 |  84.0 |  88.8 |  92.6 |  97.4 |  99.7 |  81.4 |  87.4 |  95.8 |  92.4 |  83.0 |  87.1 |  82.0 |  84.5 |  86.8 |  90.8 | 101.1 |  98.6 |  96.9 |  97.4 | 103.9 |  91.3 |  91.0 |  95.4 |  84.8 |  98.6 | 102.6 |  78.1 |  91.6 | 101.3 |  92.3 |  76.1 |  93.7 | 107.8 |  60.9 |  73.5 |  94.4 |  93.0 |  73.8 |  67.8 |  89.3 |  97.7 |  91.1 |  90.2 | 100.3 |  73.3 |  93.0 |  99.1 |  99.7 |  70.8 |  97.4 |  87.9 |  98.4 |  82.0 |  90.7 |  80.4 |  95.1 | 103.6 |  97.1 | 105.5 |  94.3 |  64.1 | 101.8 |  87.1 |  87.2 |  88.6 | 120.8 |  86.0 |  90.5 |  76.5 | 103.4 |  85.6 |  69.8 |  83.7 |  84.0 | 102.0 |  93.7 | 100.1 |  96.9 |  67.4 |  79.1 | 108.5 |  87.6 |\n",
            "| 16  |  85.6 | 101.6 |  97.1 |  97.2 | 114.3 |  83.9 |  97.0 |  92.5 |  80.7 | 100.8 |  99.1 |  99.2 |  99.7 |  87.6 |  79.2 | 105.7 |   0.0 |  93.3 |  81.0 | 109.1 | 105.0 |  72.8 | 103.9 |  91.1 |  71.0 | 101.5 |  93.2 |  84.2 |  79.2 |  95.3 | 103.5 |  90.3 |  84.2 |  76.4 |  86.4 |  78.1 |  85.2 |  84.8 |  81.3 |  96.6 |  93.8 |  91.4 |  94.7 | 103.1 |  81.3 |  95.4 |  79.7 |  86.7 |  93.2 |  78.3 |  80.8 |  81.8 | 100.9 | 103.7 |  90.8 |  86.6 |  87.8 |  89.9 |  82.6 |  88.2 |  85.2 |  87.0 |  92.9 | 105.5 | 106.7 |  94.4 |  82.7 |  95.9 | 108.0 |  83.9 |  91.4 |  90.1 |  75.4 |  94.7 |  76.7 |  90.0 |  84.2 |  89.1 |  76.7 |  90.9 |  84.0 | 101.2 |  77.9 | 107.4 |  90.8 |  84.6 | 107.6 | 107.8 |  82.1 |  91.0 |  98.5 |  84.5 |  87.7 |  78.3 |  94.9 |  78.4 | 106.6 |  99.7 |  90.2 |  78.5 |  82.5 |\n",
            "| 17  |  91.6 |  73.8 |  81.2 |  69.2 |  77.9 | 106.9 |  77.1 |  87.4 | 114.0 |  66.6 |  80.7 | 102.0 |  83.1 |  98.7 | 101.3 |  73.9 |  93.3 |   0.0 |  86.7 |  91.7 |  87.6 |  88.7 | 108.1 |  94.7 |  93.4 |  84.0 |  93.5 |  95.5 |  89.3 |  76.8 |  82.6 |  80.3 |  83.7 |  92.5 | 129.5 |  94.3 | 102.8 | 103.7 |  94.7 |  92.5 | 103.2 |  99.1 |  99.2 | 105.1 | 102.0 |  90.2 |  90.4 | 116.1 |  90.4 |  84.3 |  88.5 |  97.7 |  69.7 |  68.7 |  88.4 | 107.6 |  68.8 |  74.9 |  84.1 | 105.9 |  90.6 |  85.1 | 105.4 |  85.7 |  93.8 |  98.8 |  85.0 |  77.5 |  97.2 |  72.9 | 101.5 |  83.1 |  84.9 |  85.6 |  98.3 | 104.1 |  88.3 | 107.1 | 100.0 |  77.6 |  88.7 |  89.7 |  91.5 |  79.2 |  98.1 |  86.6 |  95.9 |  78.2 |  91.4 |  99.0 |  70.4 |  90.1 |  77.8 | 103.5 |  92.9 | 102.1 |  98.1 |  76.3 |  87.7 |  99.3 |  87.6 |\n",
            "| 18  |  98.5 |  97.0 |  83.9 |  86.8 |  99.5 |  93.4 |  79.5 |  95.5 |  73.4 |  77.8 |  89.2 |  76.3 | 100.8 | 100.3 |  84.6 |  92.8 |  81.0 |  86.7 |   0.0 |  84.9 |  73.9 |  81.6 |  85.7 |  98.9 |  82.4 |  90.5 |  90.3 |  88.1 |  67.6 |  94.4 |  76.1 |  85.0 |  85.2 |  75.9 | 112.7 | 111.0 | 103.5 |  98.3 |  76.9 |  81.7 |  90.1 |  76.5 |  72.9 |  83.2 |  86.6 |  91.7 |  93.3 |  96.2 |  84.2 |  81.3 |  74.3 |  79.5 |  94.9 |  97.2 |  93.6 |  94.1 |  97.1 |  78.8 | 109.6 |  83.4 |  88.1 |  84.2 |  96.0 |  80.9 | 106.6 |  86.2 |  68.9 |  83.1 | 105.7 |  82.4 |  90.1 | 100.5 | 101.4 |  94.3 |  97.4 |  85.5 |  89.4 |  93.5 |  96.1 | 100.9 |  91.7 | 107.4 |  74.2 |  87.5 | 100.9 |  87.9 |  92.4 |  80.6 |  83.4 |  90.7 |  93.3 |  79.6 |  84.7 |  80.0 | 102.0 |  81.8 | 102.5 |  84.5 |  81.1 |  92.5 |  99.0 |\n",
            "| 19  |  86.4 |  90.6 | 101.0 |  93.2 |  86.5 |  79.7 |  83.2 |  87.2 |  84.8 |  97.8 |  93.7 |  75.5 |  93.9 |  87.2 | 109.5 |  84.0 | 109.1 |  91.7 |  84.9 |   0.0 |  76.8 |  98.7 |  81.1 |  74.7 |  92.4 |  96.3 |  64.8 |  92.3 |  76.1 |  90.2 | 106.7 |  77.8 | 101.5 |  74.2 |  88.9 |  98.4 |  98.1 | 104.8 |  84.5 |  84.9 |  72.8 |  80.1 |  81.4 |  78.8 |  99.8 |  87.2 |  92.8 |  98.7 |  72.3 |  86.6 |  81.2 |  87.7 |  90.5 |  82.8 |  83.5 |  82.7 |  96.0 |  91.9 |  80.9 |  90.7 |  88.1 | 115.1 |  86.5 |  88.6 |  80.8 |  76.4 | 101.6 |  83.5 |  86.4 | 106.7 |  82.2 |  89.2 |  79.6 |  79.1 |  95.5 |  86.1 |  99.0 |  99.1 |  83.4 | 100.9 |  95.6 |  89.7 |  94.8 |  86.5 |  90.8 |  95.4 |  80.0 |  84.2 |  84.4 | 106.0 |  97.8 | 105.4 | 110.6 | 109.3 |  75.9 |  77.8 |  93.1 |  93.6 |  90.6 |  87.0 |  82.6 |\n",
            "| 20  |  83.2 |  84.4 |  82.7 |  96.0 |  81.0 | 107.3 |  87.7 |  86.8 |  81.8 | 103.4 |  80.3 |  83.6 |  74.1 |  99.0 | 103.9 |  88.8 | 105.0 |  87.6 |  73.9 |  76.8 |   0.0 |  82.8 |  84.4 |  78.5 |  81.5 |  98.2 |  80.8 |  81.4 |  80.7 |  83.6 |  77.0 |  88.2 |  89.8 |  83.2 | 104.8 |  92.6 | 104.0 | 116.2 | 101.1 | 103.2 |  80.2 |  76.0 |  93.3 |  77.7 |  96.0 |  83.7 |  93.7 |  89.8 |  90.6 |  78.9 |  82.9 |  78.6 |  90.8 |  92.4 |  78.7 |  81.6 |  88.2 |  84.1 | 104.7 | 105.4 |  92.3 |  96.3 |  98.3 |  93.8 |  85.5 | 107.2 |  74.3 |  76.1 |  99.1 |  81.5 |  92.7 |  82.0 |  93.2 |  84.0 | 135.3 |  99.1 |  81.4 |  90.7 |  96.5 |  97.6 |  99.2 |  81.7 | 100.3 |  88.0 |  87.9 |  86.5 |  87.5 |  70.1 |  85.2 |  89.8 |  88.9 |  93.9 |  74.8 |  80.7 |  87.9 | 101.7 |  78.7 |  74.0 |  75.0 |  97.2 | 104.7 |\n",
            "| 21  |  74.1 |  87.1 |  85.3 |  85.3 |  94.8 | 108.0 |  91.5 |  86.5 |  84.7 |  99.8 |  79.8 | 101.2 |  83.2 |  95.7 |  99.4 |  92.6 |  72.8 |  88.7 |  81.6 |  98.7 |  82.8 |   0.0 | 105.8 |  89.3 |  91.6 |  96.8 | 103.3 |  86.0 |  82.1 |  74.2 |  94.0 |  78.3 |  82.9 |  96.5 |  95.1 | 101.1 |  95.4 |  99.1 |  73.6 |  96.5 | 107.1 |  79.7 |  85.8 |  83.3 |  82.1 |  87.7 |  92.7 |  78.0 |  77.3 |  91.3 |  76.1 |  86.6 |  99.3 |  87.9 | 100.2 |  84.8 | 102.5 | 105.3 |  89.8 |  78.5 |  81.2 |  81.4 |  97.3 |  94.8 | 104.3 |  98.0 |  76.6 |  87.0 |  94.1 |  79.6 |  80.1 |  84.7 |  79.1 |  87.1 | 102.4 |  91.1 | 100.0 |  84.2 | 101.0 | 101.8 |  79.2 |  84.5 |  95.0 | 103.3 | 104.1 |  79.4 | 102.6 |  86.8 |  92.8 |  95.9 |  85.1 |  88.0 |  90.7 |  80.0 |  85.9 |  80.6 |  81.7 |  94.8 |  91.7 | 108.5 |  88.6 |\n",
            "| 22  | 105.8 |  93.0 |  97.2 | 106.5 |  97.9 |  82.5 | 104.6 |  74.0 |  75.0 |  84.7 |  89.3 |  70.5 | 100.6 |  68.4 |  91.1 |  97.4 | 103.9 | 108.1 |  85.7 |  81.1 |  84.4 | 105.8 |   0.0 | 102.3 | 106.1 |  97.4 |  95.7 |  88.7 |  93.4 | 100.3 |  97.9 | 104.5 | 104.1 |  84.2 |  70.3 |  74.3 |  63.8 |  90.5 |  70.2 |  87.9 |  77.9 |  97.4 |  95.7 |  73.1 |  66.4 |  88.8 |  86.5 |  79.4 |  87.7 |  91.1 | 106.4 |  80.1 | 106.6 | 103.3 |  89.0 |  76.6 | 110.6 | 100.1 | 111.7 |  86.6 |  79.2 |  90.6 |  63.9 | 100.0 |  65.9 |  85.0 | 101.1 |  92.1 |  96.8 |  94.5 |  78.9 |  92.9 | 116.3 |  90.2 |  79.4 |  68.1 |  86.1 |  61.8 | 102.1 |  97.5 | 124.6 |  89.6 | 102.4 |  95.8 |  69.4 | 109.4 |  90.4 | 107.3 |  93.5 | 103.6 | 112.7 | 109.7 |  99.4 |  71.2 |  82.3 |  79.4 |  85.3 | 105.0 |  99.4 |  67.5 | 105.0 |\n",
            "| 23  |  85.0 | 110.3 |  78.2 |  87.3 |  92.3 |  79.4 |  87.7 |  92.0 |  78.7 | 109.8 |  82.9 |  81.1 |  91.8 |  94.8 |  84.4 |  99.7 |  91.1 |  94.7 |  98.9 |  74.7 |  78.5 |  89.3 | 102.3 |   0.0 |  76.1 |  87.7 |  74.0 |  86.6 |  94.3 |  84.5 |  84.5 |  81.8 |  81.2 |  88.5 |  89.8 |  83.6 |  90.2 |  89.0 | 101.4 |  87.0 |  77.5 |  81.4 |  92.2 |  88.3 | 107.7 | 103.0 |  83.3 |  92.0 |  79.5 | 102.3 |  90.7 |  92.6 |  95.3 |  84.6 |  79.4 |  79.1 |  86.4 | 102.6 |  81.7 | 106.9 | 107.1 |  93.0 |  87.3 | 102.5 |  84.3 |  93.7 |  89.4 |  88.7 |  70.2 | 106.3 |  94.8 |  99.0 |  81.4 | 104.2 |  94.4 |  82.7 |  81.1 |  93.5 |  94.3 | 103.9 |  98.8 |  75.7 | 102.7 |  80.3 |  96.7 |  78.5 |  94.4 |  96.3 |  75.8 | 107.5 |  89.3 | 100.8 |  93.1 |  93.2 |  82.7 |  72.5 |  82.6 |  99.9 |  92.1 |  78.7 |  90.1 |\n",
            "| 24  |  98.8 |  82.5 |  96.3 |  93.8 |  97.8 |  85.2 |  94.6 |  85.0 |  83.7 | 102.7 | 101.3 |  91.1 |  83.5 | 118.4 |  83.0 |  81.4 |  71.0 |  93.4 |  82.4 |  92.4 |  81.5 |  91.6 | 106.1 |  76.1 |   0.0 |  99.7 |  96.5 |  90.3 |  77.0 |  83.3 |  95.7 |  88.3 |  81.6 |  81.7 |  87.1 |  98.8 | 102.6 |  99.6 | 106.4 |  77.3 |  76.1 |  95.0 |  84.7 | 105.5 |  91.0 |  95.6 |  78.5 |  93.3 | 102.3 |  84.7 |  93.4 |  72.8 |  76.3 |  89.6 |  73.7 |  92.6 |  78.5 |  77.2 |  98.4 |  97.3 |  97.2 |  88.9 |  89.9 |  81.1 | 100.5 |  97.1 |  82.8 |  85.8 |  89.7 |  88.2 | 111.3 |  82.6 |  83.4 |  89.5 |  90.2 |  88.3 |  79.1 |  99.7 |  81.3 |  84.5 |  96.9 |  75.9 |  81.8 | 108.4 |  94.8 |  76.7 | 103.9 |  82.8 |  77.0 |  87.6 |  78.9 |  77.9 |  83.5 |  80.2 | 106.0 |  99.5 | 108.5 |  89.4 |  78.6 |  87.3 |  73.2 |\n",
            "| 25  | 104.2 |  94.5 |  78.6 |  90.8 |  76.5 | 104.8 |  89.3 |  86.9 |  86.7 |  88.3 |  72.4 | 106.5 |  90.7 |  90.7 |  81.7 |  87.4 | 101.5 |  84.0 |  90.5 |  96.3 |  98.2 |  96.8 |  97.4 |  87.7 |  99.7 |   0.0 |  90.1 |  76.9 | 103.7 |  86.4 |  81.8 |  83.1 |  81.2 | 105.6 | 107.3 |  75.8 |  96.6 |  77.8 |  89.9 |  73.3 |  90.8 |  80.3 | 107.1 |  87.4 | 103.7 |  90.4 |  87.6 |  89.6 | 101.6 | 109.6 |  71.0 | 106.7 |  91.9 |  92.8 | 105.1 | 111.1 |  88.8 |  85.1 |  81.7 | 106.5 |  85.5 |  72.1 |  95.0 |  79.9 | 103.5 |  87.9 | 100.0 |  90.0 |  71.8 |  85.5 | 103.2 |  98.8 |  81.3 |  87.3 |  94.2 |  91.1 |  82.8 | 113.8 |  78.2 | 103.8 |  89.7 |  81.0 |  87.2 |  86.8 | 106.1 | 104.6 |  89.9 |  79.5 |  76.7 |  86.4 |  91.6 |  71.9 |  99.2 |  94.7 |  85.9 |  89.7 |  77.1 |  84.1 |  81.7 |  82.5 |  85.5 |\n",
            "| 26  |  79.6 |  99.5 |  87.7 | 107.7 |  84.2 |  87.7 |  75.4 | 100.9 |  78.0 |  90.7 |  82.4 |  85.5 |  88.2 |  88.2 |  98.8 |  95.8 |  93.2 |  93.5 |  90.3 |  64.8 |  80.8 | 103.3 |  95.7 |  74.0 |  96.5 |  90.1 |   0.0 |  79.9 |  88.5 |  91.0 |  87.7 |  86.7 | 105.6 |  84.7 |  94.2 |  78.6 | 102.6 |  94.5 |  91.0 |  87.6 |  87.3 |  75.5 |  79.7 |  89.0 | 103.4 |  84.7 | 107.0 | 112.4 |  85.0 |  89.6 |  81.7 |  79.7 |  88.4 |  84.6 |  81.8 |  99.7 |  85.1 |  84.4 |  75.3 |  96.9 |  98.5 | 106.0 |  99.3 |  97.0 |  88.4 |  80.3 | 107.2 |  91.5 |  82.7 |  81.7 | 112.3 |  95.0 |  85.4 |  86.2 | 112.1 |  97.7 |  87.7 | 106.1 |  90.6 |  87.1 |  81.3 |  97.4 |  89.1 |  76.8 |  97.5 |  71.7 |  74.2 |  78.7 |  90.3 |  88.8 | 114.2 |  94.2 |  85.1 |  94.9 |  80.4 |  87.1 |  95.3 |  91.3 |  74.0 |  95.2 |  91.5 |\n",
            "| 27  |  80.7 |  86.3 |  82.0 | 111.3 | 101.2 | 105.6 |  97.8 |  87.9 |  74.5 | 105.5 |  88.9 |  95.2 | 106.6 |  97.3 |  89.2 |  92.4 |  84.2 |  95.5 |  88.1 |  92.3 |  81.4 |  86.0 |  88.7 |  86.6 |  90.3 |  76.9 |  79.9 |   0.0 | 100.7 |  80.3 |  93.8 |  89.5 | 100.3 |  88.8 |  94.8 |  75.6 |  89.1 |  91.8 |  85.1 |  81.7 |  99.7 |  95.6 |  95.5 |  81.9 |  95.0 |  83.9 |  75.5 |  83.4 |  79.2 |  82.7 |  65.5 |  96.2 |  88.5 |  92.9 |  79.3 |  95.4 |  82.8 | 103.6 |  81.4 | 105.0 |  83.3 |  87.8 | 108.7 |  83.0 |  89.1 |  87.5 |  83.2 |  88.2 | 108.9 |  78.6 |  96.9 |  93.7 |  85.9 |  73.5 | 107.4 |  80.7 |  70.6 |  96.2 |  91.0 |  87.7 | 100.9 |  82.8 |  96.7 |  85.3 |  97.1 | 104.1 |  92.4 |  76.7 |  79.6 |  86.6 |  98.3 |  93.5 | 101.3 |  83.9 |  79.6 | 101.3 |  76.8 |  81.7 |  72.0 |  88.9 |  84.7 |\n",
            "| 28  |  78.0 |  83.8 | 105.7 |  82.9 |  99.1 |  91.5 |  74.2 |  86.2 |  77.2 |  91.5 | 104.3 |  72.9 |  81.5 |  98.4 |  87.1 |  83.0 |  79.2 |  89.3 |  67.6 |  76.1 |  80.7 |  82.1 |  93.4 |  94.3 |  77.0 | 103.7 |  88.5 | 100.7 |   0.0 |  87.8 |  86.9 | 100.8 | 108.0 |  79.5 |  92.5 |  97.7 |  93.2 | 108.1 |  89.2 |  97.2 |  76.4 |  79.6 |  72.8 |  88.0 |  87.8 |  92.0 |  97.2 | 108.8 |  86.8 |  81.1 |  77.8 |  79.4 |  98.1 |  93.0 |  91.7 |  90.8 |  80.3 |  72.6 | 103.1 | 101.8 |  97.2 |  87.1 |  99.5 |  70.3 | 114.0 |  83.3 |  73.0 |  88.3 | 104.5 |  83.3 |  81.8 |  82.0 |  81.3 |  95.6 |  87.3 |  93.3 | 105.5 | 100.7 |  77.3 | 101.6 |  87.1 |  85.6 |  77.3 |  84.0 |  91.2 |  92.9 |  92.8 |  99.1 |  96.3 |  97.7 |  93.0 |  87.9 |  91.4 |  95.8 |  92.5 |  82.8 | 107.1 |  88.1 |  84.7 | 104.1 |  82.1 |\n",
            "| 29  |  83.1 |  84.5 |  82.8 |  98.7 |  80.3 | 124.0 |  97.1 |  88.5 |  90.1 |  87.2 |  96.2 | 101.8 |  80.2 | 102.5 | 101.2 |  87.1 |  95.3 |  76.8 |  94.4 |  90.2 |  83.6 |  74.2 | 100.3 |  84.5 |  83.3 |  86.4 |  91.0 |  80.3 |  87.8 |   0.0 |  94.3 |  74.5 |  96.5 |  87.2 |  94.7 |  89.4 |  99.4 | 111.2 |  75.3 |  84.0 | 103.4 |  80.9 |  86.2 | 107.0 | 105.3 |  90.6 | 105.1 |  97.0 |  87.2 |  93.3 |  95.3 |  83.1 |  83.5 |  86.8 |  86.3 | 102.2 |  86.4 |  96.9 |  79.3 |  87.7 |  95.6 |  95.2 |  95.3 |  79.8 | 101.6 |  87.9 |  85.7 |  72.2 |  95.1 |  72.6 |  94.0 |  92.3 |  94.5 |  75.7 |  92.6 |  88.5 | 100.3 | 106.6 |  82.7 |  88.5 |  93.2 |  82.6 |  96.8 |  86.6 |  93.4 |  92.5 | 113.2 |  93.8 |  89.1 | 100.6 |  91.3 |  95.4 |  75.9 |  97.7 |  90.0 | 109.4 |  77.4 |  92.5 |  72.4 | 114.6 |  88.3 |\n",
            "| 30  |  87.5 |  87.9 |  77.6 |  81.1 |  76.0 |  98.2 |  74.7 | 110.1 |  91.3 |  82.3 |  87.7 |  87.5 |  78.5 |  91.6 |  81.3 |  82.0 | 103.5 |  82.6 |  76.1 | 106.7 |  77.0 |  94.0 |  97.9 |  84.5 |  95.7 |  81.8 |  87.7 |  93.8 |  86.9 |  94.3 |   0.0 |  82.9 |  83.9 |  87.6 | 109.2 |  95.8 |  95.6 |  86.3 | 100.4 | 102.2 |  95.6 |  82.9 |  96.1 |  89.3 |  98.8 |  91.0 |  94.9 |  91.2 |  91.6 |  90.0 |  96.6 | 102.2 |  73.7 |  91.9 |  96.5 |  93.5 |  91.6 |  74.0 |  98.6 |  96.1 | 104.3 |  80.2 | 108.2 |  79.0 | 104.7 | 110.8 |  75.0 |  84.7 |  82.1 |  90.2 | 107.4 |  93.6 | 102.9 | 120.6 |  99.6 |  93.5 |  84.7 |  99.7 | 103.7 |  79.6 |  82.2 |  80.4 |  91.9 |  74.9 |  95.9 |  78.4 |  72.4 |  78.4 |  87.9 |  86.1 |  82.6 |  88.8 |  75.0 |  85.8 |  93.0 |  93.9 |  91.1 |  80.4 |  72.1 | 112.1 | 103.9 |\n",
            "| 31  |  98.7 |  82.5 |  81.1 |  87.8 |  88.5 |  93.2 |  83.7 | 102.8 |  93.8 |  90.2 |  85.4 | 108.2 |  74.2 |  99.3 | 100.6 |  84.5 |  90.3 |  80.3 |  85.0 |  77.8 |  88.2 |  78.3 | 104.5 |  81.8 |  88.3 |  83.1 |  86.7 |  89.5 | 100.8 |  74.5 |  82.9 |   0.0 |  79.3 |  76.0 | 109.3 | 106.3 | 108.5 |  99.5 |  87.9 |  83.1 | 103.4 |  79.0 | 100.7 | 108.2 | 111.1 |  79.5 |  99.4 |  95.0 |  72.8 |  89.0 |  94.7 | 102.9 |  73.2 |  82.8 | 101.3 |  94.9 |  83.9 | 107.0 |  81.4 |  90.9 |  83.9 |  89.9 |  99.3 |  82.9 | 110.6 |  99.9 | 111.9 |  84.3 |  84.2 |  93.7 |  95.7 |  88.6 |  79.3 |  81.7 | 101.7 |  94.8 |  75.4 |  99.1 |  98.2 |  81.1 |  79.1 |  87.5 | 104.0 |  91.9 |  98.3 |  80.8 |  96.7 |  82.6 |  98.6 |  84.0 |  78.7 | 111.4 | 102.6 |  93.7 |  82.8 |  84.6 | 101.4 |  87.0 |  78.6 | 105.6 |  82.3 |\n",
            "| 32  |  98.9 |  88.8 |  80.5 |  78.6 |  85.7 |  84.3 |  79.6 | 113.0 | 107.3 |  90.7 |  71.0 | 108.9 |  93.0 |  91.5 |  82.5 |  86.8 |  84.2 |  83.7 |  85.2 | 101.5 |  89.8 |  82.9 | 104.1 |  81.2 |  81.6 |  81.2 | 105.6 | 100.3 | 108.0 |  96.5 |  83.9 |  79.3 |   0.0 |  94.1 |  97.2 |  99.6 |  99.0 |  70.2 |  92.6 |  69.3 |  99.4 |  97.9 |  90.8 | 111.0 |  98.5 |  83.2 |  84.4 |  83.7 |  83.8 |  92.9 |  80.3 |  82.9 |  95.7 |  98.3 |  96.0 |  92.0 |  92.3 |  83.6 |  96.3 |  81.3 |  87.9 |  91.2 |  92.2 |  94.1 |  84.8 | 116.8 |  83.2 |  84.3 |  73.5 |  98.9 |  84.2 |  89.2 |  97.4 |  95.2 |  90.3 |  89.8 |  74.6 |  90.0 |  80.8 |  80.4 |  94.0 |  77.7 |  81.1 | 104.9 | 110.6 |  73.5 |  79.0 |  87.3 |  78.4 | 100.4 |  76.7 |  81.7 |  75.0 | 100.4 |  96.9 |  91.5 |  74.8 |  86.7 | 102.1 |  80.8 | 100.2 |\n",
            "| 33  | 102.0 |  80.3 |  80.4 |  86.5 |  92.4 |  85.3 | 111.9 |  96.2 | 103.7 |  87.7 | 109.6 |  92.8 |  92.0 |  98.9 |  82.1 |  90.8 |  76.4 |  92.5 |  75.9 |  74.2 |  83.2 |  96.5 |  84.2 |  88.5 |  81.7 | 105.6 |  84.7 |  88.8 |  79.5 |  87.2 |  87.6 |  76.0 |  94.1 |   0.0 |  93.1 |  93.2 |  98.1 |  95.4 |  83.9 | 100.6 |  89.5 |  88.2 |  80.8 |  89.8 |  96.1 |  90.9 |  79.7 |  86.5 |  79.4 |  74.8 |  94.5 |  99.3 |  89.8 |  96.7 |  82.0 |  90.6 |  89.9 |  85.5 |  85.3 |  79.9 |  82.8 | 106.2 | 102.6 | 104.2 |  95.1 |  87.6 |  81.5 |  92.6 | 110.5 |  89.8 |  88.5 | 100.7 |  84.9 |  77.6 |  80.1 |  87.2 |  80.2 |  87.1 |  82.5 |  94.2 |  82.0 |  97.8 |  83.6 |  89.1 |  86.7 |  87.3 |  83.4 | 108.1 |  88.2 |  97.8 | 106.8 | 103.8 |  86.5 |  90.5 |  79.2 |  87.1 | 112.4 |  97.0 |  78.3 |  83.4 |  88.5 |\n",
            "| 34  |  85.9 |  96.6 | 103.4 | 104.2 | 104.9 |  74.8 | 115.2 |  91.5 |  85.9 | 105.9 |  96.8 |  76.9 | 100.8 |  67.6 |  83.2 | 101.1 |  86.4 | 129.5 | 112.7 |  88.9 | 104.8 |  95.1 |  70.3 |  89.8 |  87.1 | 107.3 |  94.2 |  94.8 |  92.5 |  94.7 | 109.2 | 109.3 |  97.2 |  93.1 |   0.0 |  82.4 |  59.1 |  75.1 |  86.2 |  99.4 |  82.6 |  90.0 |  89.6 |  94.6 |  72.5 |  91.9 |  85.9 |  65.1 |  91.1 | 102.6 |  98.8 |  72.7 | 111.7 |  97.5 |  98.7 |  70.0 | 102.0 |  97.0 |  82.3 |  67.0 | 108.3 | 115.8 |  66.4 |  90.6 |  67.8 |  75.4 |  96.0 | 106.1 |  83.1 |  99.7 |  80.3 |  88.9 |  97.9 |  94.3 |  68.7 |  63.7 |  98.8 |  67.0 |  82.7 |  97.0 |  96.9 |  90.5 |  81.7 | 107.7 |  69.8 |  93.0 |  93.2 | 109.0 |  87.1 | 100.4 |  99.6 |  99.9 |  98.1 |  77.9 |  84.7 |  74.4 |  90.4 | 105.6 |  97.3 |  87.8 |  89.7 |\n",
            "| 35  |  83.2 | 101.5 |  96.0 | 110.0 | 101.6 |  91.7 |  95.6 |  77.7 |  81.4 |  90.2 |  78.2 |  92.7 |  97.1 |  74.4 |  84.6 |  98.6 |  78.1 |  94.3 | 111.0 |  98.4 |  92.6 | 101.1 |  74.3 |  83.6 |  98.8 |  75.8 |  78.6 |  75.6 |  97.7 |  89.4 |  95.8 | 106.3 |  99.6 |  93.2 |  82.4 |   0.0 |  78.0 |  82.6 |  82.3 | 101.9 |  89.6 |  94.1 | 112.7 |  77.4 |  94.2 |  81.5 |  79.6 |  89.5 | 102.8 |  96.3 |  98.6 |  88.5 | 102.7 | 111.7 |  87.7 |  99.2 |  93.7 |  92.5 |  85.0 | 107.6 |  80.9 |  92.1 |  88.0 | 116.0 |  81.4 |  88.9 | 102.4 |  93.4 |  88.3 |  89.1 |  94.7 |  90.5 |  81.2 |  88.4 |  87.8 |  88.5 |  78.1 | 101.2 |  77.6 |  98.2 | 114.4 |  79.9 | 100.9 |  88.8 |  76.6 | 102.4 |  84.2 | 106.3 |  86.2 | 100.3 | 103.0 |  88.8 |  88.8 |  75.9 |  82.3 |  84.8 |  82.4 | 108.9 |  90.8 |  71.4 |  84.9 |\n",
            "| 36  |  98.8 | 106.8 |  98.6 |  99.6 | 109.2 |  72.2 |  98.2 |  90.4 |  78.5 | 108.6 | 112.2 |  79.7 | 110.2 |  54.9 |  88.1 |  96.9 |  85.2 | 102.8 | 103.5 |  98.1 | 104.0 |  95.4 |  63.8 |  90.2 | 102.6 |  96.6 | 102.6 |  89.1 |  93.2 |  99.4 |  95.6 | 108.5 |  99.0 |  98.1 |  59.1 |  78.0 |   0.0 |  81.5 |  80.4 | 105.6 |  75.8 |  87.6 | 101.0 |  87.9 |  55.9 |  92.9 |  89.6 |  66.5 |  92.2 | 105.9 |  93.3 |  83.6 | 103.0 | 103.1 | 100.7 |  62.6 |  97.1 |  95.6 |  99.4 |  84.8 |  93.0 | 102.4 |  56.8 |  97.9 |  62.2 |  74.6 |  90.9 | 101.0 |  96.2 | 102.4 |  82.0 | 105.0 | 100.3 | 110.2 |  79.1 |  59.2 |  94.8 |  52.4 |  86.6 | 100.5 | 113.6 |  93.4 | 105.7 | 100.1 |  62.3 | 104.8 | 107.9 | 106.3 |  96.9 | 108.9 |  97.3 |  96.0 | 106.8 |  78.1 |  68.6 |  73.8 |  93.5 | 101.4 | 104.2 |  73.0 | 110.5 |\n",
            "| 37  |  84.8 |  90.3 | 100.9 |  84.8 |  92.6 |  74.4 |  94.4 | 113.3 |  96.3 |  82.5 |  72.5 |  79.9 | 103.7 |  82.1 |  68.3 |  97.4 |  84.8 | 103.7 |  98.3 | 104.8 | 116.2 |  99.1 |  90.5 |  89.0 |  99.6 |  77.8 |  94.5 |  91.8 | 108.1 | 111.2 |  86.3 |  99.5 |  70.2 |  95.4 |  75.1 |  82.6 |  81.5 |   0.0 | 100.3 |  80.2 |  84.6 | 111.1 |  98.4 |  94.0 |  86.7 |  80.4 |  77.5 |  82.7 |  74.4 |  87.7 |  90.3 |  96.9 |  97.1 |  93.1 | 110.9 |  96.7 |  98.2 | 102.8 |  93.8 |  80.5 |  85.0 |  82.2 |  85.4 |  95.4 |  79.8 |  83.1 |  88.1 | 102.7 |  74.4 |  90.2 |  96.8 | 105.8 |  81.1 | 102.4 |  70.1 |  85.1 |  85.7 |  82.4 |  75.6 |  95.0 |  80.4 |  99.0 |  77.0 |  91.9 |  98.0 |  78.2 |  78.3 | 103.0 |  75.6 | 101.1 | 101.7 |  70.9 |  93.2 |  87.1 |  97.6 |  86.9 |  85.3 |  89.2 | 110.7 |  81.3 |  98.3 |\n",
            "| 38  |  84.1 |  98.9 |  93.7 |  86.0 |  97.8 |  94.3 |  96.8 |  75.2 |  72.9 |  87.0 |  97.5 |  97.7 | 110.0 |  76.0 | 109.4 | 103.9 |  81.3 |  94.7 |  76.9 |  84.5 | 101.1 |  73.6 |  70.2 | 101.4 | 106.4 |  89.9 |  91.0 |  85.1 |  89.2 |  75.3 | 100.4 |  87.9 |  92.6 |  83.9 |  86.2 |  82.3 |  80.4 | 100.3 |   0.0 |  83.3 |  95.4 |  77.0 |  76.9 |  83.7 |  76.5 |  83.5 |  99.4 |  72.8 |  85.1 |  94.5 |  79.1 |  79.0 |  99.4 | 100.5 | 100.5 |  98.2 | 108.2 |  96.6 |  91.2 |  81.9 |  77.8 | 101.7 |  83.0 |  96.1 |  88.3 |  82.6 |  98.9 | 100.3 | 102.2 |  86.3 |  76.1 |  93.4 | 102.6 |  93.6 |  77.1 |  84.5 |  91.3 |  86.8 |  94.7 |  95.7 | 107.2 |  92.0 |  95.7 | 102.6 |  87.5 | 102.5 |  94.1 | 105.4 |  81.1 |  98.1 | 104.6 | 100.0 |  95.5 |  98.0 |  77.0 |  79.2 |  81.3 | 130.3 |  92.2 |  81.3 |  96.9 |\n",
            "| 39  |  97.3 |  85.5 |  91.3 |  97.7 |  76.5 |  88.2 |  78.8 |  88.9 |  80.9 |  79.6 |  81.3 |  90.9 |  99.8 | 109.3 |  83.6 |  91.3 |  96.6 |  92.5 |  81.7 |  84.9 | 103.2 |  96.5 |  87.9 |  87.0 |  77.3 |  73.3 |  87.6 |  81.7 |  97.2 |  84.0 | 102.2 |  83.1 |  69.3 | 100.6 |  99.4 | 101.9 | 105.6 |  80.2 |  83.3 |   0.0 |  89.1 | 103.6 |  75.7 | 105.3 |  92.1 |  89.5 |  94.0 | 107.3 |  85.0 |  91.1 |  85.9 |  78.2 |  90.2 |  86.0 |  78.9 | 105.7 |  88.7 |  99.0 | 103.9 |  92.3 |  83.7 |  72.2 |  83.9 |  73.1 |  98.1 |  92.6 |  96.3 |  88.1 |  75.8 |  86.8 |  96.4 | 100.5 |  96.4 |  84.3 |  82.2 |  83.4 |  79.9 |  99.8 |  83.3 |  83.4 |  99.8 |  75.0 |  79.1 |  95.6 | 112.3 |  80.8 |  87.5 |  83.7 |  83.7 |  88.0 |  94.4 |  86.4 |  95.0 | 103.5 | 106.9 | 102.0 |  76.9 |  93.5 |  95.4 |  79.9 |  87.2 |\n",
            "| 40  |  86.8 | 101.8 | 112.6 |  85.5 |  92.7 |  70.3 |  96.8 |  72.4 |  68.0 | 106.6 | 104.5 |  63.9 |  97.5 |  85.6 |  93.2 |  91.0 |  93.8 | 103.2 |  90.1 |  72.8 |  80.2 | 107.1 |  77.9 |  77.5 |  76.1 |  90.8 |  87.3 |  99.7 |  76.4 | 103.4 |  95.6 | 103.4 |  99.4 |  89.5 |  82.6 |  89.6 |  75.8 |  84.6 |  95.4 |  89.1 |   0.0 |  76.5 |  95.0 |  80.1 |  79.8 |  90.0 |  91.8 |  88.7 |  86.5 |  91.8 |  88.6 |  76.4 |  92.6 |  97.0 |  81.6 |  76.2 |  95.8 |  88.6 | 104.6 | 107.3 |  82.0 |  90.6 |  67.2 |  84.6 |  71.6 |  76.1 |  95.0 | 103.3 |  82.6 |  89.9 |  98.0 |  93.1 |  90.4 | 103.6 |  84.8 | 101.6 |  81.1 |  83.9 |  79.1 |  99.7 | 100.0 |  95.3 | 103.5 |  89.3 |  82.7 |  89.7 | 103.1 |  97.1 |  73.4 |  98.3 |  96.8 |  74.6 |  94.4 |  88.5 |  95.2 |  94.4 |  89.3 |  86.2 |  98.5 |  74.3 | 104.3 |\n",
            "| 41  |  86.2 | 108.6 |  81.5 |  90.5 |  85.3 |  92.7 |  97.0 |  78.1 |  69.4 |  93.8 |  94.2 |  81.0 |  93.8 |  77.5 | 100.5 |  95.4 |  91.4 |  99.1 |  76.5 |  80.1 |  76.0 |  79.7 |  97.4 |  81.4 |  95.0 |  80.3 |  75.5 |  95.6 |  79.6 |  80.9 |  82.9 |  79.0 |  97.9 |  88.2 |  90.0 |  94.1 |  87.6 | 111.1 |  77.0 | 103.6 |  76.5 |   0.0 |  90.0 |  88.0 |  95.2 |  90.0 | 118.7 |  80.4 |  99.6 | 115.9 |  80.6 |  78.5 | 104.5 | 100.8 | 102.4 |  80.8 |  89.3 |  85.2 |  84.1 |  90.4 | 102.5 | 109.1 |  81.2 |  83.0 |  96.4 |  81.3 |  91.6 |  92.7 |  80.6 |  89.3 |  83.4 | 103.4 |  96.6 |  97.5 | 105.6 |  92.5 |  92.0 |  89.6 |  88.9 |  99.8 |  83.6 | 107.7 |  92.6 |  90.0 |  83.5 |  92.9 | 104.1 |  79.7 |  85.7 |  87.6 |  92.4 |  87.4 |  86.4 |  82.9 |  80.0 |  85.2 |  95.4 |  94.3 |  85.8 | 103.2 | 103.1 |\n",
            "| 42  |  87.9 |  95.6 |  75.8 |  76.6 |  95.4 |  80.9 |  80.2 |  95.7 |  85.8 |  89.3 |  99.9 |  87.2 |  87.3 | 107.8 |  93.8 |  84.8 |  94.7 |  99.2 |  72.9 |  81.4 |  93.3 |  85.8 |  95.7 |  92.2 |  84.7 | 107.1 |  79.7 |  95.5 |  72.8 |  86.2 |  96.1 | 100.7 |  90.8 |  80.8 |  89.6 | 112.7 | 101.0 |  98.4 |  76.9 |  75.7 |  95.0 |  90.0 |   0.0 |  91.3 |  92.2 |  86.7 | 105.2 |  95.9 |  88.8 |  86.0 |  77.2 |  80.2 |  88.2 |  84.9 |  81.3 | 105.9 |  80.8 |  82.9 | 100.4 |  75.6 |  92.1 | 101.5 |  96.9 |  89.1 |  92.1 |  75.4 |  84.1 |  96.2 | 101.8 |  83.2 |  85.8 |  99.6 | 101.4 |  81.1 |  84.4 |  93.3 | 100.2 |  88.7 |  93.7 |  92.7 |  87.0 |  96.6 |  73.6 |  76.9 | 108.2 |  79.7 |  73.6 |  97.7 | 108.0 |  80.0 |  89.2 |  79.5 |  75.5 | 106.9 |  94.9 |  89.2 |  87.0 |  95.9 |  85.8 |  92.3 |  82.9 |\n",
            "| 43  |  90.6 |  98.3 |  93.8 |  89.2 |  98.6 |  89.7 |  90.4 |  72.2 |  83.2 |  92.1 |  92.2 |  81.0 | 101.8 |  92.5 |  81.6 |  98.6 | 103.1 | 105.1 |  83.2 |  78.8 |  77.7 |  83.3 |  73.1 |  88.3 | 105.5 |  87.4 |  89.0 |  81.9 |  88.0 | 107.0 |  89.3 | 108.2 | 111.0 |  89.8 |  94.6 |  77.4 |  87.9 |  94.0 |  83.7 | 105.3 |  80.1 |  88.0 |  91.3 |   0.0 |  83.2 |  89.7 |  75.8 |  82.0 |  81.7 |  97.7 |  87.4 | 100.9 |  97.7 |  97.2 |  77.0 |  81.0 | 121.6 |  92.6 |  96.4 |  88.3 |  70.3 |  80.3 |  94.9 | 106.0 |  78.2 |  78.1 |  82.4 |  93.7 |  84.4 | 102.2 |  80.2 |  84.6 |  84.9 |  84.1 |  98.4 |  93.5 |  91.7 |  86.4 |  99.8 | 112.2 |  93.5 |  84.9 | 113.2 |  78.8 |  80.5 | 101.8 |  75.4 |  85.4 |  85.7 |  91.0 |  96.3 |  80.3 | 109.0 |  77.9 |  79.8 |  77.6 |  81.7 |  98.4 |  96.4 |  76.1 |  85.2 |\n",
            "| 44  |  89.9 |  90.7 | 114.6 | 101.7 |  97.4 |  67.5 | 100.0 |  73.5 |  75.1 |  95.1 | 110.1 |  87.5 | 103.5 |  64.1 | 103.9 | 102.6 |  81.3 | 102.0 |  86.6 |  99.8 |  96.0 |  82.1 |  66.4 | 107.7 |  91.0 | 103.7 | 103.4 |  95.0 |  87.8 | 105.3 |  98.8 | 111.1 |  98.5 |  96.1 |  72.5 |  94.2 |  55.9 |  86.7 |  76.5 |  92.1 |  79.8 |  95.2 |  92.2 |  83.2 |   0.0 | 109.8 |  74.4 |  62.8 | 101.7 |  96.4 |  91.8 |  69.9 | 105.3 |  99.5 |  95.2 |  64.5 | 107.8 |  97.3 | 111.0 |  77.7 |  75.1 |  84.7 |  59.8 |  92.7 |  71.5 |  73.9 |  83.5 | 111.8 | 105.6 |  99.4 |  86.4 | 104.1 |  96.1 | 105.4 |  87.3 |  63.8 | 103.8 |  57.8 |  94.5 | 100.8 |  95.6 |  95.9 |  87.5 | 110.7 |  71.5 |  93.7 | 102.2 |  86.4 |  93.6 |  96.5 | 105.0 |  91.4 |  93.3 |  69.6 |  75.4 |  79.3 | 104.3 | 108.6 |  98.3 |  71.7 | 114.0 |\n",
            "| 45  |  85.8 |  79.2 |  89.6 |  83.3 |  93.4 | 100.2 |  77.9 | 106.4 |  91.4 |  86.2 |  76.3 |  87.3 |  86.9 | 106.7 |  90.6 |  78.1 |  95.4 |  90.2 |  91.7 |  87.2 |  83.7 |  87.7 |  88.8 | 103.0 |  95.6 |  90.4 |  84.7 |  83.9 |  92.0 |  90.6 |  91.0 |  79.5 |  83.2 |  90.9 |  91.9 |  81.5 |  92.9 |  80.4 |  83.5 |  89.5 |  90.0 |  90.0 |  86.7 |  89.7 | 109.8 |   0.0 | 118.4 |  98.8 |  76.9 |  78.2 |  87.5 |  91.4 |  76.2 |  86.1 | 107.7 | 112.2 |  78.5 |  85.1 | 107.5 |  96.6 |  85.8 | 106.0 | 105.5 |  93.6 |  90.3 | 101.1 | 102.4 |  82.8 |  84.8 |  70.9 |  99.4 |  84.2 |  77.8 |  77.7 | 107.1 | 111.5 |  72.3 |  92.8 |  81.2 |  79.4 | 102.0 |  85.8 | 103.8 |  98.8 |  90.7 |  79.2 |  77.9 |  85.7 |  92.8 |  83.8 |  83.0 |  78.5 | 101.2 |  96.8 |  89.9 | 105.0 |  83.6 |  89.1 | 102.2 | 102.8 |  80.8 |\n",
            "| 46  |  90.7 |  82.6 | 100.5 |  91.0 | 109.9 |  73.7 | 106.5 |  78.8 | 100.3 |  96.3 |  90.6 | 101.8 | 100.2 |  84.6 |  82.3 |  91.6 |  79.7 |  90.4 |  93.3 |  92.8 |  93.7 |  92.7 |  86.5 |  83.3 |  78.5 |  87.6 | 107.0 |  75.5 |  97.2 | 105.1 |  94.9 |  99.4 |  84.4 |  79.7 |  85.9 |  79.6 |  89.6 |  77.5 |  99.4 |  94.0 |  91.8 | 118.7 | 105.2 |  75.8 |  74.4 | 118.4 |   0.0 |  72.7 |  85.0 |  89.2 |  84.9 |  97.0 |  96.4 |  94.8 |  78.8 |  79.7 | 101.0 |  97.7 |  81.4 |  91.8 |  73.6 |  77.5 |  94.9 |  92.5 |  75.9 |  81.6 |  78.8 | 107.1 | 100.8 | 108.0 |  85.8 |  84.0 |  81.0 |  84.4 |  78.6 |  73.1 |  81.3 |  92.3 |  92.9 |  99.2 |  91.4 |  78.5 |  84.3 |  87.7 |  92.6 | 100.6 |  83.4 |  92.8 |  77.4 | 101.5 |  86.1 |  94.7 |  92.8 |  77.8 |  83.1 |  73.8 |  97.9 |  93.4 |  81.8 |  65.4 |  84.5 |\n",
            "| 47  |  87.0 |  93.0 |  93.7 |  88.3 |  95.5 |  76.8 | 118.1 |  80.7 |  82.5 | 113.7 | 100.8 |  95.5 | 103.6 |  61.8 | 107.0 | 101.3 |  86.7 | 116.1 |  96.2 |  98.7 |  89.8 |  78.0 |  79.4 |  92.0 |  93.3 |  89.6 | 112.4 |  83.4 | 108.8 |  97.0 |  91.2 |  95.0 |  83.7 |  86.5 |  65.1 |  89.5 |  66.5 |  82.7 |  72.8 | 107.3 |  88.7 |  80.4 |  95.9 |  82.0 |  62.8 |  98.8 |  72.7 |   0.0 | 100.9 |  98.7 |  81.6 |  92.0 | 101.5 | 111.0 | 106.0 |  72.2 | 106.8 | 105.2 |  87.1 |  74.7 |  80.6 | 105.9 |  71.1 | 102.3 |  72.7 |  86.1 |  85.4 | 104.7 | 100.1 | 110.6 |  72.7 | 100.1 |  94.3 | 104.2 |  85.2 |  68.6 |  93.0 |  63.0 |  90.2 | 104.0 |  94.4 |  95.2 |  91.2 | 114.6 |  73.3 | 109.8 |  92.9 |  91.5 |  78.3 |  92.5 |  95.0 |  91.9 |  94.4 |  77.1 |  69.3 |  82.0 |  84.8 | 108.9 |  96.2 |  76.8 | 102.5 |\n",
            "| 48  |  83.3 |  78.4 |  96.9 |  77.8 | 107.7 |  89.5 |  82.3 | 108.5 |  95.9 |  89.2 |  82.5 |  83.3 |  98.2 |  95.3 |  81.5 |  92.3 |  93.2 |  90.4 |  84.2 |  72.3 |  90.6 |  77.3 |  87.7 |  79.5 | 102.3 | 101.6 |  85.0 |  79.2 |  86.8 |  87.2 |  91.6 |  72.8 |  83.8 |  79.4 |  91.1 | 102.8 |  92.2 |  74.4 |  85.1 |  85.0 |  86.5 |  99.6 |  88.8 |  81.7 | 101.7 |  76.9 |  85.0 | 100.9 |   0.0 |  84.7 |  78.7 |  91.9 |  91.8 |  83.5 |  90.7 |  87.8 | 101.7 | 110.1 |  87.9 |  85.7 |  78.3 |  86.8 | 104.4 |  81.0 |  78.6 |  83.0 |  89.6 |  91.3 |  84.0 |  84.6 |  81.4 |  77.1 |  91.4 |  81.0 |  89.2 |  92.3 |  76.7 |  82.3 |  98.7 |  87.8 |  82.4 |  87.1 | 110.2 |  75.4 | 100.7 |  79.7 |  87.1 | 101.8 |  84.7 | 116.5 |  97.8 | 101.6 | 104.2 |  95.7 |  82.1 |  82.6 |  84.6 |  81.7 |  97.4 |  99.1 |  93.4 |\n",
            "| 49  |  76.1 |  77.6 |  90.3 |  89.9 |  81.3 | 103.6 |  83.3 | 104.7 |  96.9 |  99.0 |  90.9 |  83.7 |  84.8 | 107.7 | 100.7 |  76.1 |  78.3 |  84.3 |  81.3 |  86.6 |  78.9 |  91.3 |  91.1 | 102.3 |  84.7 | 109.6 |  89.6 |  82.7 |  81.1 |  93.3 |  90.0 |  89.0 |  92.9 |  74.8 | 102.6 |  96.3 | 105.9 |  87.7 |  94.5 |  91.1 |  91.8 | 115.9 |  86.0 |  97.7 |  96.4 |  78.2 |  89.2 |  98.7 |  84.7 |   0.0 |  90.9 | 109.1 |  70.4 |  80.6 |  84.7 |  95.7 |  82.7 |  88.2 |  88.5 |  98.0 |  86.9 |  83.9 | 100.7 |  96.7 | 106.1 | 108.7 |  87.0 |  77.5 | 127.8 |  77.4 |  91.5 |  79.5 |  74.0 |  93.0 |  90.3 | 113.8 |  93.2 | 100.0 |  86.0 |  79.4 |  91.6 |  97.8 |  75.6 | 101.4 | 102.8 |  81.9 |  88.9 |  94.7 |  89.7 |  74.9 |  96.2 |  96.2 |  88.7 | 107.3 | 111.2 | 106.1 |  88.3 |  75.2 |  83.9 |  98.7 |  93.5 |\n",
            "| 50  |  81.9 |  88.7 |  90.5 |  78.7 | 100.3 |  96.7 |  82.3 |  96.2 |  80.1 | 112.6 |  86.3 | 104.7 |  95.3 |  86.5 |  97.9 |  93.7 |  80.8 |  88.5 |  74.3 |  81.2 |  82.9 |  76.1 | 106.4 |  90.7 |  93.4 |  71.0 |  81.7 |  65.5 |  77.8 |  95.3 |  96.6 |  94.7 |  80.3 |  94.5 |  98.8 |  98.6 |  93.3 |  90.3 |  79.1 |  85.9 |  88.6 |  80.6 |  77.2 |  87.4 |  91.8 |  87.5 |  84.9 |  81.6 |  78.7 |  90.9 |   0.0 |  85.6 |  95.9 |  91.0 | 100.5 | 101.4 |  81.1 |  82.8 |  82.5 | 101.3 |  83.8 |  95.2 | 108.5 |  77.2 |  92.8 |  83.2 |  79.0 |  96.4 | 101.0 |  79.8 |  85.5 |  83.2 |  83.2 |  81.7 | 104.8 |  92.4 |  86.5 |  96.8 |  78.6 | 101.8 |  80.6 |  89.3 |  81.8 |  87.5 | 110.5 | 103.9 |  84.3 |  81.2 |  74.6 |  92.2 |  93.8 |  74.6 |  95.7 | 106.7 |  73.9 |  85.8 |  77.8 |  78.8 |  80.0 |  84.4 |  87.3 |\n",
            "| 51  |  80.6 |  92.6 | 114.8 | 104.7 | 101.7 |  79.8 |  88.6 |  82.1 |  70.5 |  86.6 |  89.8 |  82.5 |  95.2 |  76.4 |  96.4 | 107.8 |  81.8 |  97.7 |  79.5 |  87.7 |  78.6 |  86.6 |  80.1 |  92.6 |  72.8 | 106.7 |  79.7 |  96.2 |  79.4 |  83.1 | 102.2 | 102.9 |  82.9 |  99.3 |  72.7 |  88.5 |  83.6 |  96.9 |  79.0 |  78.2 |  76.4 |  78.5 |  80.2 | 100.9 |  69.9 |  91.4 |  97.0 |  92.0 |  91.9 | 109.1 |  85.6 |   0.0 | 107.2 | 108.3 |  81.3 |  87.7 |  97.2 |  89.0 | 109.2 |  79.8 |  88.3 | 106.4 |  72.0 |  82.6 |  71.3 |  78.8 |  82.7 |  99.2 |  81.9 |  80.2 |  94.6 |  90.0 | 113.3 |  89.0 |  96.4 |  74.9 |  84.5 |  80.8 |  83.4 |  90.7 |  98.7 |  87.8 |  87.7 |  95.2 |  78.2 |  80.5 |  91.7 |  87.3 |  85.8 | 106.9 |  94.5 |  81.4 |  68.5 |  71.3 |  94.9 |  94.1 |  89.7 |  98.2 |  88.1 |  87.8 | 101.4 |\n",
            "| 52  |  94.7 |  75.4 |  89.4 |  78.6 |  81.9 | 102.7 |  80.2 | 100.4 |  98.0 |  90.3 |  98.5 | 101.4 |  66.1 | 111.4 | 100.6 |  60.9 | 100.9 |  69.7 |  94.9 |  90.5 |  90.8 |  99.3 | 106.6 |  95.3 |  76.3 |  91.9 |  88.4 |  88.5 |  98.1 |  83.5 |  73.7 |  73.2 |  95.7 |  89.8 | 111.7 | 102.7 | 103.0 |  97.1 |  99.4 |  90.2 |  92.6 | 104.5 |  88.2 |  97.7 | 105.3 |  76.2 |  96.4 | 101.5 |  91.8 |  70.4 |  95.9 | 107.2 |   0.0 |  65.1 |  88.5 | 111.1 |  72.3 |  81.2 |  92.2 | 103.3 |  88.8 |  87.9 | 105.3 |  79.7 | 101.7 | 102.3 |  96.2 |  75.9 |  94.0 |  82.1 | 116.3 |  79.0 |  86.6 |  88.5 |  94.4 | 109.4 |  86.6 | 104.4 |  96.6 |  60.9 |  86.9 |  83.3 | 103.4 |  82.5 | 102.8 |  87.4 |  84.9 |  77.4 |  91.5 |  69.0 |  67.1 |  82.2 |  92.4 | 106.6 |  97.8 | 113.2 |  94.9 |  76.9 |  71.5 | 103.1 |  77.7 |\n",
            "| 53  |  89.9 |  84.6 |  77.3 |  74.6 |  79.2 | 102.9 |  94.5 |  91.5 |  98.9 |  81.3 |  80.3 |  93.2 |  70.5 | 115.2 |  93.5 |  73.5 | 103.7 |  68.7 |  97.2 |  82.8 |  92.4 |  87.9 | 103.3 |  84.6 |  89.6 |  92.8 |  84.6 |  92.9 |  93.0 |  86.8 |  91.9 |  82.8 |  98.3 |  96.7 |  97.5 | 111.7 | 103.1 |  93.1 | 100.5 |  86.0 |  97.0 | 100.8 |  84.9 |  97.2 |  99.5 |  86.1 |  94.8 | 111.0 |  83.5 |  80.6 |  91.0 | 108.3 |  65.1 |   0.0 | 101.1 |  92.9 |  66.6 |  82.5 |  85.6 | 103.8 | 115.7 |  84.3 | 110.4 |  75.3 |  97.1 |  97.5 |  94.0 |  86.3 |  87.4 |  76.0 | 107.6 |  84.7 |  77.0 |  77.2 |  95.2 | 105.0 | 100.9 |  97.7 | 110.3 |  74.6 |  79.4 |  90.8 |  85.7 |  81.2 | 113.8 |  76.6 |  98.1 |  77.0 |  84.1 |  74.3 |  75.4 |  98.9 |  97.8 | 111.7 |  89.5 |  88.2 |  94.6 |  84.6 |  85.9 | 100.1 |  79.0 |\n",
            "| 54  |  92.0 |  92.4 |  83.8 | 102.9 |  85.9 |  90.3 |  86.7 |  78.0 |  98.2 |  97.4 | 106.2 |  89.8 | 100.5 | 107.3 |  80.1 |  94.4 |  90.8 |  88.4 |  93.6 |  83.5 |  78.7 | 100.2 |  89.0 |  79.4 |  73.7 | 105.1 |  81.8 |  79.3 |  91.7 |  86.3 |  96.5 | 101.3 |  96.0 |  82.0 |  98.7 |  87.7 | 100.7 | 110.9 | 100.5 |  78.9 |  81.6 | 102.4 |  81.3 |  77.0 |  95.2 | 107.7 |  78.8 | 106.0 |  90.7 |  84.7 | 100.5 |  81.3 |  88.5 | 101.1 |   0.0 |  83.8 | 101.8 |  87.0 |  82.2 |  86.0 |  81.4 |  84.6 |  88.9 |  98.1 |  79.4 |  90.2 |  82.8 |  88.5 |  92.0 |  91.9 |  92.8 |  79.8 | 101.3 |  81.0 |  89.9 |  95.5 |  75.0 |  98.3 |  94.7 |  85.4 |  96.6 |  74.9 |  99.7 |  78.7 |  85.9 |  81.1 |  82.4 |  89.0 |  96.8 |  87.7 |  86.7 |  92.0 |  80.0 |  88.1 | 109.1 | 105.2 |  79.3 |  79.1 |  77.1 |  82.3 |  85.5 |\n",
            "| 55  |  91.9 | 113.4 |  96.5 | 111.1 |  97.6 |  70.0 |  98.7 |  87.6 |  80.4 | 109.2 | 110.5 |  79.5 |  99.9 |  67.8 |  92.5 |  93.0 |  86.6 | 107.6 |  94.1 |  82.7 |  81.6 |  84.8 |  76.6 |  79.1 |  92.6 | 111.1 |  99.7 |  95.4 |  90.8 | 102.2 |  93.5 |  94.9 |  92.0 |  90.6 |  70.0 |  99.2 |  62.6 |  96.7 |  98.2 | 105.7 |  76.2 |  80.8 | 105.9 |  81.0 |  64.5 | 112.2 |  79.7 |  72.2 |  87.8 |  95.7 | 101.4 |  87.7 | 111.1 |  92.9 |  83.8 |   0.0 | 110.1 |  93.2 |  88.9 |  77.3 |  97.4 |  94.7 |  71.8 |  93.9 |  71.0 |  94.3 |  82.3 |  85.2 |  90.7 | 123.6 |  72.6 |  96.5 |  97.4 | 100.4 |  91.3 |  72.6 | 106.7 |  65.7 |  94.2 |  88.2 |  95.4 |  98.4 |  99.5 |  98.9 |  77.4 |  92.3 | 109.9 |  83.7 |  87.1 |  99.5 |  93.7 | 107.8 |  97.0 |  80.0 |  75.7 |  72.4 |  91.5 |  89.6 | 100.5 |  81.5 | 111.3 |\n",
            "| 56  |  86.6 |  77.3 |  77.8 |  74.5 |  90.1 |  91.2 |  87.2 |  89.3 |  90.9 |  85.9 |  85.3 |  91.7 |  70.9 | 104.8 |  95.6 |  73.8 |  87.8 |  68.8 |  97.1 |  96.0 |  88.2 | 102.5 | 110.6 |  86.4 |  78.5 |  88.8 |  85.1 |  82.8 |  80.3 |  86.4 |  91.6 |  83.9 |  92.3 |  89.9 | 102.0 |  93.7 |  97.1 |  98.2 | 108.2 |  88.7 |  95.8 |  89.3 |  80.8 | 121.6 | 107.8 |  78.5 | 101.0 | 106.8 | 101.7 |  82.7 |  81.1 |  97.2 |  72.3 |  66.6 | 101.8 | 110.1 |   0.0 |  84.9 |  95.2 | 125.7 | 107.7 |  94.4 | 106.2 |  75.4 | 103.6 |  95.1 |  91.6 |  89.2 | 104.4 |  73.3 |  96.5 |  94.7 |  78.3 |  80.3 | 100.0 | 100.6 |  80.7 |  94.7 |  88.0 |  74.6 |  88.9 |  93.8 |  78.5 |  77.8 |  98.2 |  93.7 |  95.4 |  86.4 | 101.2 |  72.0 |  72.4 |  93.2 |  83.7 | 105.0 |  87.6 | 104.5 | 103.6 |  78.7 |  87.3 |  95.1 |  78.3 |\n",
            "| 57  | 107.5 |  93.1 |  79.2 |  86.6 |  83.0 |  99.6 |  78.4 | 102.3 | 105.2 |  86.5 | 100.2 | 101.5 |  85.0 | 101.8 |  80.2 |  67.8 |  89.9 |  74.9 |  78.8 |  91.9 |  84.1 | 105.3 | 100.1 | 102.6 |  77.2 |  85.1 |  84.4 | 103.6 |  72.6 |  96.9 |  74.0 | 107.0 |  83.6 |  85.5 |  97.0 |  92.5 |  95.6 | 102.8 |  96.6 |  99.0 |  88.6 |  85.2 |  82.9 |  92.6 |  97.3 |  85.1 |  97.7 | 105.2 | 110.1 |  88.2 |  82.8 |  89.0 |  81.2 |  82.5 |  87.0 |  93.2 |  84.9 |   0.0 |  87.3 |  90.0 | 111.1 | 100.8 | 109.9 |  82.4 |  96.0 | 104.3 |  82.9 |  77.9 |  88.0 |  85.0 | 108.5 |  77.3 |  95.3 |  90.3 |  95.7 | 104.4 |  97.1 | 112.0 |  80.8 |  75.3 |  91.8 |  85.4 |  75.3 |  95.1 |  99.9 |  81.8 |  81.9 |  75.1 |  81.8 |  84.8 |  83.3 |  72.5 |  75.8 | 103.7 |  92.4 |  93.9 |  99.9 |  80.8 |  75.7 |  94.9 |  85.7 |\n",
            "| 58  |  83.7 |  94.1 |  76.0 |  92.5 |  84.5 | 101.4 | 100.9 |  94.9 | 112.2 | 101.9 |  89.0 | 105.2 |  98.4 |  82.5 |  93.7 |  89.3 |  82.6 |  84.1 | 109.6 |  80.9 | 104.7 |  89.8 | 111.7 |  81.7 |  98.4 |  81.7 |  75.3 |  81.4 | 103.1 |  79.3 |  98.6 |  81.4 |  96.3 |  85.3 |  82.3 |  85.0 |  99.4 |  93.8 |  91.2 | 103.9 | 104.6 |  84.1 | 100.4 |  96.4 | 111.0 | 107.5 |  81.4 |  87.1 |  87.9 |  88.5 |  82.5 | 109.2 |  92.2 |  85.6 |  82.2 |  88.9 |  95.2 |  87.3 |   0.0 |  79.1 | 103.3 | 103.2 |  97.4 |  93.0 |  92.3 |  83.7 | 103.7 |  92.1 |  90.4 |  92.6 |  87.5 |  75.3 |  80.0 |  85.3 |  81.1 |  96.3 |  95.6 | 111.2 |  90.5 |  89.0 |  71.0 |  97.7 |  79.6 |  90.7 |  95.7 |  91.1 |  92.7 |  92.5 |  83.4 |  90.2 |  92.6 | 103.0 |  92.2 | 102.9 |  89.4 |  82.0 |  88.7 |  82.5 |  73.4 | 102.8 |  79.3 |\n",
            "| 59  |  99.1 |  95.1 |  86.8 | 100.7 |  89.6 |  84.7 |  99.3 | 108.9 | 109.0 |  83.5 |  99.8 |  91.8 | 104.0 |  80.4 |  80.8 |  97.7 |  88.2 | 105.9 |  83.4 |  90.7 | 105.4 |  78.5 |  86.6 | 106.9 |  97.3 | 106.5 |  96.9 | 105.0 | 101.8 |  87.7 |  96.1 |  90.9 |  81.3 |  79.9 |  67.0 | 107.6 |  84.8 |  80.5 |  81.9 |  92.3 | 107.3 |  90.4 |  75.6 |  88.3 |  77.7 |  96.6 |  91.8 |  74.7 |  85.7 |  98.0 | 101.3 |  79.8 | 103.3 | 103.8 |  86.0 |  77.3 | 125.7 |  90.0 |  79.1 |   0.0 |  88.5 | 108.7 |  77.0 | 100.7 |  79.4 |  77.4 |  81.2 |  89.5 |  84.0 | 100.2 |  80.8 |  99.2 | 104.8 |  85.9 |  74.7 |  70.4 | 108.4 |  75.6 |  87.2 |  95.2 |  74.6 | 104.9 |  76.8 | 101.4 |  82.8 |  77.6 |  81.7 |  92.6 |  98.9 |  98.4 |  95.3 |  84.8 |  83.9 |  79.5 |  96.2 |  82.4 |  91.6 |  91.8 |  92.4 | 100.1 |  94.3 |\n",
            "| 60  |  93.2 |  79.4 | 110.3 |  91.2 | 101.0 |  77.3 |  86.9 |  75.8 |  87.0 |  88.6 |  94.7 | 104.2 |  91.2 |  83.1 | 103.4 |  91.1 |  85.2 |  90.6 |  88.1 |  88.1 |  92.3 |  81.2 |  79.2 | 107.1 |  97.2 |  85.5 |  98.5 |  83.3 |  97.2 |  95.6 | 104.3 |  83.9 |  87.9 |  82.8 | 108.3 |  80.9 |  93.0 |  85.0 |  77.8 |  83.7 |  82.0 | 102.5 |  92.1 |  70.3 |  75.1 |  85.8 |  73.6 |  80.6 |  78.3 |  86.9 |  83.8 |  88.3 |  88.8 | 115.7 |  81.4 |  97.4 | 107.7 | 111.1 | 103.3 |  88.5 |   0.0 |  73.9 |  85.5 | 104.3 |  79.3 |  78.7 |  98.6 |  95.6 |  99.0 |  93.6 |  82.4 |  89.0 |  86.8 |  74.1 |  91.3 |  96.7 |  79.8 |  83.1 |  84.3 |  94.7 |  89.7 |  91.1 | 112.5 |  88.0 |  89.0 | 102.1 |  83.5 |  94.4 |  99.0 |  96.4 | 100.2 |  76.1 |  91.9 |  80.8 |  80.0 |  98.0 |  86.2 |  91.8 |  98.4 |  69.7 |  91.0 |\n",
            "| 61  |  97.8 |  83.6 |  90.0 |  87.1 |  85.7 |  99.5 |  80.0 |  79.4 |  84.7 |  80.6 |  79.4 |  94.7 |  78.4 | 107.5 |  73.6 |  90.2 |  87.0 |  85.1 |  84.2 | 115.1 |  96.3 |  81.4 |  90.6 |  93.0 |  88.9 |  72.1 | 106.0 |  87.8 |  87.1 |  95.2 |  80.2 |  89.9 |  91.2 | 106.2 | 115.8 |  92.1 | 102.4 |  82.2 | 101.7 |  72.2 |  90.6 | 109.1 | 101.5 |  80.3 |  84.7 | 106.0 |  77.5 | 105.9 |  86.8 |  83.9 |  95.2 | 106.4 |  87.9 |  84.3 |  84.6 |  94.7 |  94.4 | 100.8 | 103.2 | 108.7 |  73.9 |   0.0 |  95.3 |  80.6 | 112.9 |  99.6 |  89.5 |  89.0 |  86.8 |  81.9 |  95.0 |  81.7 |  86.4 |  94.5 |  87.0 | 101.5 |  86.5 |  94.9 | 103.6 |  90.9 |  82.3 |  79.1 |  95.0 |  82.0 | 106.5 |  88.6 | 101.1 |  91.6 |  94.2 |  79.0 |  93.3 |  84.6 |  93.5 |  81.3 | 105.5 |  91.9 |  86.4 |  79.8 |  91.2 |  85.5 |  90.1 |\n",
            "| 62  |  97.5 | 103.4 | 101.6 | 101.0 |  88.0 |  70.4 |  99.7 |  74.5 |  78.0 |  98.7 |  95.9 |  67.8 | 108.1 |  61.1 | 100.7 | 100.3 |  92.9 | 105.4 |  96.0 |  86.5 |  98.3 |  97.3 |  63.9 |  87.3 |  89.9 |  95.0 |  99.3 | 108.7 |  99.5 |  95.3 | 108.2 |  99.3 |  92.2 | 102.6 |  66.4 |  88.0 |  56.8 |  85.4 |  83.0 |  83.9 |  67.2 |  81.2 |  96.9 |  94.9 |  59.8 | 105.5 |  94.9 |  71.1 | 104.4 | 100.7 | 108.5 |  72.0 | 105.3 | 110.4 |  88.9 |  71.8 | 106.2 | 109.9 |  97.4 |  77.0 |  85.5 |  95.3 |   0.0 | 101.4 |  68.5 |  70.6 | 102.7 | 101.1 |  87.7 | 102.7 |  78.2 | 106.6 | 102.5 | 113.6 |  75.4 |  66.7 |  96.0 |  61.2 |  88.9 | 109.7 | 110.4 | 101.6 |  90.9 | 110.9 |  66.0 |  94.5 | 109.7 | 103.0 |  98.7 | 101.6 |  97.9 |  94.1 |  96.2 |  73.2 |  98.8 |  82.1 |  92.6 | 100.5 | 107.5 |  74.2 | 114.7 |\n",
            "| 63  |  82.5 |  72.8 | 102.4 |  85.3 |  89.1 |  98.0 |  83.2 |  89.2 |  84.4 |  82.7 |  95.3 |  87.0 |  84.8 |  98.9 |  89.2 |  73.3 | 105.5 |  85.7 |  80.9 |  88.6 |  93.8 |  94.8 | 100.0 | 102.5 |  81.1 |  79.9 |  97.0 |  83.0 |  70.3 |  79.8 |  79.0 |  82.9 |  94.1 | 104.2 |  90.6 | 116.0 |  97.9 |  95.4 |  96.1 |  73.1 |  84.6 |  83.0 |  89.1 | 106.0 |  92.7 |  93.6 |  92.5 | 102.3 |  81.0 |  96.7 |  77.2 |  82.6 |  79.7 |  75.3 |  98.1 |  93.9 |  75.4 |  82.4 |  93.0 | 100.7 | 104.3 |  80.6 | 101.4 |   0.0 | 103.7 |  83.8 |  84.7 |  94.5 |  86.5 |  81.2 |  92.8 |  79.4 |  99.9 |  88.3 |  90.1 |  89.2 |  90.4 | 105.5 |  90.3 |  72.7 |  80.9 |  81.8 |  79.8 |  79.1 | 102.6 |  94.8 |  96.7 |  72.5 |  85.6 |  82.6 |  73.5 |  91.2 |  93.7 | 100.4 |  97.5 | 102.1 |  94.8 |  71.4 |  73.0 | 117.7 |  88.0 |\n",
            "| 64  |  92.5 |  96.3 |  97.0 |  91.1 | 104.4 |  65.3 | 102.1 |  82.9 |  95.6 |  94.5 |  90.7 |  82.5 | 110.5 |  64.9 |  99.9 |  93.0 | 106.7 |  93.8 | 106.6 |  80.8 |  85.5 | 104.3 |  65.9 |  84.3 | 100.5 | 103.5 |  88.4 |  89.1 | 114.0 | 101.6 | 104.7 | 110.6 |  84.8 |  95.1 |  67.8 |  81.4 |  62.2 |  79.8 |  88.3 |  98.1 |  71.6 |  96.4 |  92.1 |  78.2 |  71.5 |  90.3 |  75.9 |  72.7 |  78.6 | 106.1 |  92.8 |  71.3 | 101.7 |  97.1 |  79.4 |  71.0 | 103.6 |  96.0 |  92.3 |  79.4 |  79.3 | 112.9 |  68.5 | 103.7 |   0.0 |  71.5 |  96.0 | 103.9 |  85.0 |  99.5 |  84.9 |  91.9 | 110.9 |  85.8 |  91.9 |  78.1 |  80.4 |  62.5 |  99.3 |  89.8 | 109.1 |  96.2 | 108.7 |  86.5 |  74.3 |  91.1 |  86.1 |  91.5 |  86.9 | 114.4 |  92.8 |  90.5 |  82.8 |  80.5 |  74.5 |  87.5 |  81.2 |  93.0 | 105.0 |  68.6 | 110.5 |\n",
            "| 65  |  86.8 |  97.1 | 102.7 |  89.6 | 112.0 |  69.7 |  98.2 |  78.5 |  84.0 |  86.7 | 103.6 |  78.0 | 110.8 |  78.7 |  92.2 |  99.1 |  94.4 |  98.8 |  86.2 |  76.4 | 107.2 |  98.0 |  85.0 |  93.7 |  97.1 |  87.9 |  80.3 |  87.5 |  83.3 |  87.9 | 110.8 |  99.9 | 116.8 |  87.6 |  75.4 |  88.9 |  74.6 |  83.1 |  82.6 |  92.6 |  76.1 |  81.3 |  75.4 |  78.1 |  73.9 | 101.1 |  81.6 |  86.1 |  83.0 | 108.7 |  83.2 |  78.8 | 102.3 |  97.5 |  90.2 |  94.3 |  95.1 | 104.3 |  83.7 |  77.4 |  78.7 |  99.6 |  70.6 |  83.8 |  71.5 |   0.0 |  96.0 | 121.6 |  97.7 |  88.3 |  87.5 | 107.5 |  86.0 |  82.2 |  80.9 |  75.4 |  95.8 |  85.2 |  85.9 | 118.8 |  83.9 | 111.9 |  85.8 |  73.8 |  77.4 |  96.3 |  95.6 |  98.6 | 102.6 |  99.1 |  97.1 |  80.3 |  98.3 |  80.3 |  83.5 |  75.7 | 107.9 |  95.3 |  86.6 |  85.3 |  92.7 |\n",
            "| 66  |  76.9 |  85.7 |  91.9 |  85.7 |  83.9 | 102.7 |  95.8 |  96.2 |  92.9 |  85.1 |  99.4 |  78.7 |  97.6 |  96.5 |  71.5 |  99.7 |  82.7 |  85.0 |  68.9 | 101.6 |  74.3 |  76.6 | 101.1 |  89.4 |  82.8 | 100.0 | 107.2 |  83.2 |  73.0 |  85.7 |  75.0 | 111.9 |  83.2 |  81.5 |  96.0 | 102.4 |  90.9 |  88.1 |  98.9 |  96.3 |  95.0 |  91.6 |  84.1 |  82.4 |  83.5 | 102.4 |  78.8 |  85.4 |  89.6 |  87.0 |  79.0 |  82.7 |  96.2 |  94.0 |  82.8 |  82.3 |  91.6 |  82.9 | 103.7 |  81.2 |  98.6 |  89.5 | 102.7 |  84.7 |  96.0 |  96.0 |   0.0 |  78.3 |  99.2 |  88.4 |  79.7 | 104.2 |  93.5 |  94.7 |  89.3 |  76.2 | 102.2 |  85.0 |  76.1 |  98.8 |  79.5 |  89.2 |  74.9 |  80.8 |  88.3 |  94.8 |  85.7 |  84.7 |  74.0 | 100.1 |  84.9 |  73.8 |  76.3 |  87.4 |  97.4 | 103.3 |  82.3 |  79.8 |  90.2 |  95.5 |  97.2 |\n",
            "| 67  | 102.8 |  90.5 |  82.7 | 106.6 |  74.9 | 114.9 |  76.4 | 105.5 |  89.3 |  84.2 |  85.6 |  85.7 |  75.1 |  97.9 |  90.0 |  70.8 |  95.9 |  77.5 |  83.1 |  83.5 |  76.1 |  87.0 |  92.1 |  88.7 |  85.8 |  90.0 |  91.5 |  88.2 |  88.3 |  72.2 |  84.7 |  84.3 |  84.3 |  92.6 | 106.1 |  93.4 | 101.0 | 102.7 | 100.3 |  88.1 | 103.3 |  92.7 |  96.2 |  93.7 | 111.8 |  82.8 | 107.1 | 104.7 |  91.3 |  77.5 |  96.4 |  99.2 |  75.9 |  86.3 |  88.5 |  85.2 |  89.2 |  77.9 |  92.1 |  89.5 |  95.6 |  89.0 | 101.1 |  94.5 | 103.9 | 121.6 |  78.3 |   0.0 |  84.8 |  96.4 |  88.8 |  87.1 |  97.6 |  82.2 |  98.1 |  89.9 | 112.4 |  96.3 |  80.5 |  71.1 |  98.3 |  90.5 |  97.7 |  92.5 |  99.0 | 102.1 |  98.0 |  77.0 |  81.7 |  99.5 |  94.1 |  85.0 |  86.1 |  97.3 |  91.6 | 108.7 |  77.0 |  73.1 |  97.4 | 100.5 |  82.1 |\n",
            "| 68  |  99.1 |  94.7 |  94.3 |  87.6 |  77.1 |  85.3 |  82.3 |  94.1 |  92.7 |  78.2 |  71.5 |  87.5 |  85.6 |  90.9 |  69.9 |  97.4 | 108.0 |  97.2 | 105.7 |  86.4 |  99.1 |  94.1 |  96.8 |  70.2 |  89.7 |  71.8 |  82.7 | 108.9 | 104.5 |  95.1 |  82.1 |  84.2 |  73.5 | 110.5 |  83.1 |  88.3 |  96.2 |  74.4 | 102.2 |  75.8 |  82.6 |  80.6 | 101.8 |  84.4 | 105.6 |  84.8 | 100.8 | 100.1 |  84.0 | 127.8 | 101.0 |  81.9 |  94.0 |  87.4 |  92.0 |  90.7 | 104.4 |  88.0 |  90.4 |  84.0 |  99.0 |  86.8 |  87.7 |  86.5 |  85.0 |  97.7 |  99.2 |  84.8 |   0.0 | 101.2 | 100.2 |  85.9 |  95.0 |  94.3 |  88.2 |  88.9 |  85.3 |  93.9 |  88.6 |  87.1 |  81.5 |  72.2 | 104.2 |  86.9 |  90.5 |  76.7 |  77.9 |  81.7 |  76.6 | 100.9 |  85.4 |  79.0 |  96.3 |  86.7 |  91.4 |  86.4 |  79.8 |  94.6 | 106.5 |  91.2 |  79.8 |\n",
            "| 69  |  84.1 |  72.5 |  77.4 |  81.3 |  84.0 | 125.8 | 100.4 |  88.1 |  93.2 |  79.9 |  71.6 |  90.0 |  80.0 | 111.8 |  85.6 |  87.9 |  83.9 |  72.9 |  82.4 | 106.7 |  81.5 |  79.6 |  94.5 | 106.3 |  88.2 |  85.5 |  81.7 |  78.6 |  83.3 |  72.6 |  90.2 |  93.7 |  98.9 |  89.8 |  99.7 |  89.1 | 102.4 |  90.2 |  86.3 |  86.8 |  89.9 |  89.3 |  83.2 | 102.2 |  99.4 |  70.9 | 108.0 | 110.6 |  84.6 |  77.4 |  79.8 |  80.2 |  82.1 |  76.0 |  91.9 | 123.6 |  73.3 |  85.0 |  92.6 | 100.2 |  93.6 |  81.9 | 102.7 |  81.2 |  99.5 |  88.3 |  88.4 |  96.4 | 101.2 |   0.0 | 115.1 |  81.5 |  91.8 |  76.3 |  99.8 | 112.9 |  73.5 |  96.4 |  98.4 |  89.0 |  82.8 |  94.9 |  82.8 |  85.9 | 103.1 |  73.2 |  99.9 |  95.1 |  91.1 |  82.9 |  94.4 |  76.9 |  72.7 |  83.2 | 110.0 | 111.6 |  86.5 |  71.9 |  73.0 | 110.9 |  92.7 |\n",
            "| 70  |  81.6 |  89.5 |  94.5 |  79.6 |  96.3 |  82.6 |  91.7 |  79.5 |  95.3 |  90.4 |  93.8 |  82.3 | 103.6 |  70.2 |  94.4 |  98.4 |  91.4 | 101.5 |  90.1 |  82.2 |  92.7 |  80.1 |  78.9 |  94.8 | 111.3 | 103.2 | 112.3 |  96.9 |  81.8 |  94.0 | 107.4 |  95.7 |  84.2 |  88.5 |  80.3 |  94.7 |  82.0 |  96.8 |  76.1 |  96.4 |  98.0 |  83.4 |  85.8 |  80.2 |  86.4 |  99.4 |  85.8 |  72.7 |  81.4 |  91.5 |  85.5 |  94.6 | 116.3 | 107.6 |  92.8 |  72.6 |  96.5 | 108.5 |  87.5 |  80.8 |  82.4 |  95.0 |  78.2 |  92.8 |  84.9 |  87.5 |  79.7 |  88.8 | 100.2 | 115.1 |   0.0 |  86.2 |  97.2 |  89.4 |  75.8 |  80.5 | 101.4 |  70.8 |  84.9 |  99.0 |  93.0 |  95.7 |  88.1 |  90.0 |  76.9 | 118.1 |  88.1 | 107.6 |  98.7 |  97.1 |  88.6 | 108.7 |  98.8 | 100.7 |  83.6 |  81.6 |  78.8 |  98.1 | 115.8 |  82.8 |  96.4 |\n",
            "| 71  |  90.2 |  68.6 |  97.5 |  77.0 |  98.5 | 107.0 |  78.8 |  87.8 | 106.8 |  99.6 |  80.5 | 105.8 |  73.9 |  98.7 |  91.4 |  82.0 |  90.1 |  83.1 | 100.5 |  89.2 |  82.0 |  84.7 |  92.9 |  99.0 |  82.6 |  98.8 |  95.0 |  93.7 |  82.0 |  92.3 |  93.6 |  88.6 |  89.2 | 100.7 |  88.9 |  90.5 | 105.0 | 105.8 |  93.4 | 100.5 |  93.1 | 103.4 |  99.6 |  84.6 | 104.1 |  84.2 |  84.0 | 100.1 |  77.1 |  79.5 |  83.2 |  90.0 |  79.0 |  84.7 |  79.8 |  96.5 |  94.7 |  77.3 |  75.3 |  99.2 |  89.0 |  81.7 | 106.6 |  79.4 |  91.9 | 107.5 | 104.2 |  87.1 |  85.9 |  81.5 |  86.2 |   0.0 |  93.2 |  81.7 |  94.4 | 114.2 |  78.4 | 101.3 | 103.8 |  74.7 |  87.5 |  66.8 | 103.9 |  95.2 |  91.6 |  91.0 |  84.6 |  92.1 |  88.9 |  86.0 |  78.0 |  99.5 |  94.1 |  95.6 |  97.5 |  91.3 |  84.4 |  78.0 |  80.7 | 101.5 |  71.6 |\n",
            "| 72  |  74.3 |  82.4 |  99.0 |  85.3 |  86.2 |  90.3 |  85.2 |  91.9 |  96.5 | 101.5 |  88.2 |  96.2 |  87.4 | 107.5 |  92.2 |  90.7 |  75.4 |  84.9 | 101.4 |  79.6 |  93.2 |  79.1 | 116.3 |  81.4 |  83.4 |  81.3 |  85.4 |  85.9 |  81.3 |  94.5 | 102.9 |  79.3 |  97.4 |  84.9 |  97.9 |  81.2 | 100.3 |  81.1 | 102.6 |  96.4 |  90.4 |  96.6 | 101.4 |  84.9 |  96.1 |  77.8 |  81.0 |  94.3 |  91.4 |  74.0 |  83.2 | 113.3 |  86.6 |  77.0 | 101.3 |  97.4 |  78.3 |  95.3 |  80.0 | 104.8 |  86.8 |  86.4 | 102.5 |  99.9 | 110.9 |  86.0 |  93.5 |  97.6 |  95.0 |  91.8 |  97.2 |  93.2 |   0.0 |  85.9 | 100.0 | 103.2 |  94.7 | 110.0 |  69.3 | 110.5 |  76.4 |  85.0 |  83.0 | 102.7 |  91.3 |  81.4 |  90.7 |  83.7 |  87.6 |  81.7 |  87.3 |  85.6 | 116.6 | 100.9 |  84.4 |  82.6 | 104.5 |  95.6 |  95.0 |  91.0 |  70.0 |\n",
            "| 73  | 103.2 |  79.8 |  82.0 |  99.9 | 100.2 | 102.5 | 104.3 |  84.2 | 102.6 |  81.6 |  84.5 | 112.2 |  74.6 | 109.2 |  86.2 |  80.4 |  94.7 |  85.6 |  94.3 |  79.1 |  84.0 |  87.1 |  90.2 | 104.2 |  89.5 |  87.3 |  86.2 |  73.5 |  95.6 |  75.7 | 120.6 |  81.7 |  95.2 |  77.6 |  94.3 |  88.4 | 110.2 | 102.4 |  93.6 |  84.3 | 103.6 |  97.5 |  81.1 |  84.1 | 105.4 |  77.7 |  84.4 | 104.2 |  81.0 |  93.0 |  81.7 |  89.0 |  88.5 |  77.2 |  81.0 | 100.4 |  80.3 |  90.3 |  85.3 |  85.9 |  74.1 |  94.5 | 113.6 |  88.3 |  85.8 |  82.2 |  94.7 |  82.2 |  94.3 |  76.3 |  89.4 |  81.7 |  85.9 |   0.0 | 100.8 |  97.9 |  85.2 |  97.2 |  84.9 |  83.3 |  83.1 |  90.9 |  94.8 |  75.9 | 102.0 |  98.2 |  87.4 |  85.0 |  98.9 |  82.4 |  86.4 |  86.3 |  88.1 |  91.9 |  80.9 |  98.8 |  85.7 |  77.6 |  79.4 |  86.5 |  67.0 |\n",
            "| 74  |  94.1 |  97.7 |  98.8 |  79.5 |  95.9 |  76.9 | 108.5 |  84.7 |  95.9 |  80.4 | 102.8 |  80.7 | 102.7 |  80.1 |  72.1 |  95.1 |  76.7 |  98.3 |  97.4 |  95.5 | 135.3 | 102.4 |  79.4 |  94.4 |  90.2 |  94.2 | 112.1 | 107.4 |  87.3 |  92.6 |  99.6 | 101.7 |  90.3 |  80.1 |  68.7 |  87.8 |  79.1 |  70.1 |  77.1 |  82.2 |  84.8 | 105.6 |  84.4 |  98.4 |  87.3 | 107.1 |  78.6 |  85.2 |  89.2 |  90.3 | 104.8 |  96.4 |  94.4 |  95.2 |  89.9 |  91.3 | 100.0 |  95.7 |  81.1 |  74.7 |  91.3 |  87.0 |  75.4 |  90.1 |  91.9 |  80.9 |  89.3 |  98.1 |  88.2 |  99.8 |  75.8 |  94.4 | 100.0 | 100.8 |   0.0 |  79.3 |  99.4 |  86.2 |  81.2 |  89.9 |  90.6 |  95.8 |  75.6 |  88.1 |  88.1 | 103.0 |  92.1 | 133.5 |  83.0 |  99.6 |  92.7 |  91.8 |  93.2 |  98.9 | 105.5 |  80.3 |  90.1 | 105.5 | 102.2 |  76.8 |  80.2 |\n",
            "| 75  | 102.0 |  92.6 |  96.0 | 112.4 | 101.0 |  75.4 | 100.9 |  92.2 |  79.7 |  91.8 |  97.5 |  80.8 | 107.6 |  64.1 |  82.6 | 103.6 |  90.0 | 104.1 |  85.5 |  86.1 |  99.1 |  91.1 |  68.1 |  82.7 |  88.3 |  91.1 |  97.7 |  80.7 |  93.3 |  88.5 |  93.5 |  94.8 |  89.8 |  87.2 |  63.7 |  88.5 |  59.2 |  85.1 |  84.5 |  83.4 | 101.6 |  92.5 |  93.3 |  93.5 |  63.8 | 111.5 |  73.1 |  68.6 |  92.3 | 113.8 |  92.4 |  74.9 | 109.4 | 105.0 |  95.5 |  72.6 | 100.6 | 104.4 |  96.3 |  70.4 |  96.7 | 101.5 |  66.7 |  89.2 |  78.1 |  75.4 |  76.2 |  89.9 |  88.9 | 112.9 |  80.5 | 114.2 | 103.2 |  97.9 |  79.3 |   0.0 |  99.9 |  65.4 |  87.3 | 106.8 | 104.4 |  84.1 |  83.4 |  98.4 |  71.7 | 106.9 |  92.7 |  92.4 |  90.5 | 117.4 | 100.4 | 105.1 | 102.4 |  68.6 |  71.5 |  69.6 |  99.7 | 105.6 |  92.8 |  77.7 |  93.7 |\n",
            "| 76  |  97.8 |  78.6 |  79.1 |  79.8 | 102.7 |  86.7 |  94.6 |  85.1 |  94.3 |  93.0 |  77.5 |  96.7 | 100.5 | 103.1 |  75.7 |  97.1 |  84.2 |  88.3 |  89.4 |  99.0 |  81.4 | 100.0 |  86.1 |  81.1 |  79.1 |  82.8 |  87.7 |  70.6 | 105.5 | 100.3 |  84.7 |  75.4 |  74.6 |  80.2 |  98.8 |  78.1 |  94.8 |  85.7 |  91.3 |  79.9 |  81.1 |  92.0 | 100.2 |  91.7 | 103.8 |  72.3 |  81.3 |  93.0 |  76.7 |  93.2 |  86.5 |  84.5 |  86.6 | 100.9 |  75.0 | 106.7 |  80.7 |  97.1 |  95.6 | 108.4 |  79.8 |  86.5 |  96.0 |  90.4 |  80.4 |  95.8 | 102.2 | 112.4 |  85.3 |  73.5 | 101.4 |  78.4 |  94.7 |  85.2 |  99.4 |  99.9 |   0.0 |  88.7 | 102.5 |  84.8 | 102.2 |  73.4 | 105.9 |  88.3 |  87.2 |  78.3 |  81.6 |  94.3 |  88.3 |  85.7 |  79.7 |  95.5 |  91.9 |  76.5 |  97.4 |  96.1 |  90.7 |  86.1 |  82.5 |  79.5 |  90.0 |\n",
            "| 77  | 104.1 |  87.7 |  91.4 |  86.3 | 100.3 |  71.2 | 107.9 |  85.8 |  84.9 |  94.6 |  96.8 |  75.1 |  97.5 |  67.3 |  89.2 | 105.5 |  89.1 | 107.1 |  93.5 |  99.1 |  90.7 |  84.2 |  61.8 |  93.5 |  99.7 | 113.8 | 106.1 |  96.2 | 100.7 | 106.6 |  99.7 |  99.1 |  90.0 |  87.1 |  67.0 | 101.2 |  52.4 |  82.4 |  86.8 |  99.8 |  83.9 |  89.6 |  88.7 |  86.4 |  57.8 |  92.8 |  92.3 |  63.0 |  82.3 | 100.0 |  96.8 |  80.8 | 104.4 |  97.7 |  98.3 |  65.7 |  94.7 | 112.0 | 111.2 |  75.6 |  83.1 |  94.9 |  61.2 | 105.5 |  62.5 |  85.2 |  85.0 |  96.3 |  93.9 |  96.4 |  70.8 | 101.3 | 110.0 |  97.2 |  86.2 |  65.4 |  88.7 |   0.0 | 104.8 |  95.6 |  94.7 | 103.4 | 104.3 | 100.4 |  65.1 |  97.7 | 101.4 | 103.2 |  99.9 | 101.6 | 102.9 | 100.4 |  96.6 |  67.5 |  73.2 |  83.7 |  95.0 |  96.5 | 118.4 |  73.0 | 109.7 |\n",
            "| 78  |  84.0 |  92.6 | 113.2 | 103.6 |  83.7 |  89.5 |  86.7 | 105.1 |  90.5 |  99.4 | 107.0 |  94.3 |  94.5 |  86.5 |  81.6 |  94.3 |  76.7 | 100.0 |  96.1 |  83.4 |  96.5 | 101.0 | 102.1 |  94.3 |  81.3 |  78.2 |  90.6 |  91.0 |  77.3 |  82.7 | 103.7 |  98.2 |  80.8 |  82.5 |  82.7 |  77.6 |  86.6 |  75.6 |  94.7 |  83.3 |  79.1 |  88.9 |  93.7 |  99.8 |  94.5 |  81.2 |  92.9 |  90.2 |  98.7 |  86.0 |  78.6 |  83.4 |  96.6 | 110.3 |  94.7 |  94.2 |  88.0 |  80.8 |  90.5 |  87.2 |  84.3 | 103.6 |  88.9 |  90.3 |  99.3 |  85.9 |  76.1 |  80.5 |  88.6 |  98.4 |  84.9 | 103.8 |  69.3 |  84.9 |  81.2 |  87.3 | 102.5 | 104.8 |   0.0 |  99.0 |  85.7 |  88.6 |  76.6 |  98.9 |  84.6 | 101.2 |  87.0 |  96.3 |  77.6 |  97.3 |  99.8 |  66.6 |  93.1 | 108.6 |  88.9 | 104.8 |  83.8 |  89.0 |  95.4 |  82.6 |  86.4 |\n",
            "| 79  |  91.1 |  80.9 |  90.9 |  94.5 |  86.1 |  91.5 |  82.6 |  99.0 |  96.7 |  76.2 |  94.4 | 101.6 |  77.3 |  95.3 |  96.5 |  64.1 |  90.9 |  77.6 | 100.9 | 100.9 |  97.6 | 101.8 |  97.5 | 103.9 |  84.5 | 103.8 |  87.1 |  87.7 | 101.6 |  88.5 |  79.6 |  81.1 |  80.4 |  94.2 |  97.0 |  98.2 | 100.5 |  95.0 |  95.7 |  83.4 |  99.7 |  99.8 |  92.7 | 112.2 | 100.8 |  79.4 |  99.2 | 104.0 |  87.8 |  79.4 | 101.8 |  90.7 |  60.9 |  74.6 |  85.4 |  88.2 |  74.6 |  75.3 |  89.0 |  95.2 |  94.7 |  90.9 | 109.7 |  72.7 |  89.8 | 118.8 |  98.8 |  71.1 |  87.1 |  89.0 |  99.0 |  74.7 | 110.5 |  83.3 |  89.9 | 106.8 |  84.8 |  95.6 |  99.0 |   0.0 |  89.0 |  90.2 |  96.7 |  86.4 | 104.0 |  83.9 |  87.9 |  75.4 |  88.0 |  76.5 |  79.1 |  96.0 |  74.4 | 102.9 |  93.1 | 116.5 |  89.9 |  79.5 |  87.4 | 102.7 |  87.7 |\n",
            "| 80  |  80.4 |  76.6 |  94.3 |  76.6 |  75.7 |  93.5 |  93.1 | 102.6 | 107.5 |  81.0 |  92.4 | 101.8 |  73.6 |  94.7 |  82.8 | 101.8 |  84.0 |  88.7 |  91.7 |  95.6 |  99.2 |  79.2 | 124.6 |  98.8 |  96.9 |  89.7 |  81.3 | 100.9 |  87.1 |  93.2 |  82.2 |  79.1 |  94.0 |  82.0 |  96.9 | 114.4 | 113.6 |  80.4 | 107.2 |  99.8 | 100.0 |  83.6 |  87.0 |  93.5 |  95.6 | 102.0 |  91.4 |  94.4 |  82.4 |  91.6 |  80.6 |  98.7 |  86.9 |  79.4 |  96.6 |  95.4 |  88.9 |  91.8 |  71.0 |  74.6 |  89.7 |  82.3 | 110.4 |  80.9 | 109.1 |  83.9 |  79.5 |  98.3 |  81.5 |  82.8 |  93.0 |  87.5 |  76.4 |  83.1 |  90.6 | 104.4 | 102.2 |  94.7 |  85.7 |  89.0 |   0.0 | 105.6 |  76.1 |  77.0 |  95.9 |  78.4 |  81.3 |  83.0 |  90.6 |  76.1 |  94.3 |  80.2 |  83.3 |  94.8 |  90.2 |  95.3 | 100.5 |  74.2 |  83.1 | 110.1 |  83.1 |\n",
            "| 81  |  91.3 |  71.2 |  92.6 |  89.9 |  83.6 | 102.0 |  77.0 |  88.9 |  97.4 | 103.7 |  86.7 | 109.1 |  78.8 | 105.7 |  80.2 |  87.1 | 101.2 |  89.7 | 107.4 |  89.7 |  81.7 |  84.5 |  89.6 |  75.7 |  75.9 |  81.0 |  97.4 |  82.8 |  85.6 |  82.6 |  80.4 |  87.5 |  77.7 |  97.8 |  90.5 |  79.9 |  93.4 |  99.0 |  92.0 |  75.0 |  95.3 | 107.7 |  96.6 |  84.9 |  95.9 |  85.8 |  78.5 |  95.2 |  87.1 |  97.8 |  89.3 |  87.8 |  83.3 |  90.8 |  74.9 |  98.4 |  93.8 |  85.4 |  97.7 | 104.9 |  91.1 |  79.1 | 101.6 |  81.8 |  96.2 | 111.9 |  89.2 |  90.5 |  72.2 |  94.9 |  95.7 |  66.8 |  85.0 |  90.9 |  95.8 |  84.1 |  73.4 | 103.4 |  88.6 |  90.2 | 105.6 |   0.0 | 105.7 |  96.4 |  92.6 |  86.4 |  71.9 |  89.0 |  89.3 |  96.1 |  77.1 | 101.2 | 102.0 |  97.2 |  85.8 |  90.7 |  74.7 |  99.0 |  79.3 |  89.2 |  74.0 |\n",
            "| 82  |  81.2 |  86.9 |  83.2 |  90.5 |  77.3 |  90.5 |  97.5 |  98.3 |  99.3 |  79.3 |  81.0 |  83.5 |  96.5 |  93.3 |  83.8 |  87.2 |  77.9 |  91.5 |  74.2 |  94.8 | 100.3 |  95.0 | 102.4 | 102.7 |  81.8 |  87.2 |  89.1 |  96.7 |  77.3 |  96.8 |  91.9 | 104.0 |  81.1 |  83.6 |  81.7 | 100.9 | 105.7 |  77.0 |  95.7 |  79.1 | 103.5 |  92.6 |  73.6 | 113.2 |  87.5 | 103.8 |  84.3 |  91.2 | 110.2 |  75.6 |  81.8 |  87.7 | 103.4 |  85.7 |  99.7 |  99.5 |  78.5 |  75.3 |  79.6 |  76.8 | 112.5 |  95.0 |  90.9 |  79.8 | 108.7 |  85.8 |  74.9 |  97.7 | 104.2 |  82.8 |  88.1 | 103.9 |  83.0 |  94.8 |  75.6 |  83.4 | 105.9 | 104.3 |  76.6 |  96.7 |  76.1 | 105.7 |   0.0 | 100.1 | 105.9 |  81.2 |  82.3 |  88.5 |  86.1 |  79.6 |  92.8 |  84.2 |  74.4 |  97.3 | 112.6 |  87.6 | 101.1 |  85.8 |  82.2 |  98.2 |  90.9 |\n",
            "| 83  |  82.6 | 100.8 |  83.5 |  80.9 | 100.4 |  87.2 |  81.9 |  91.0 |  88.2 |  74.7 |  88.5 |  85.4 |  76.4 |  92.3 |  79.3 |  88.6 | 107.4 |  79.2 |  87.5 |  86.5 |  88.0 | 103.3 |  95.8 |  80.3 | 108.4 |  86.8 |  76.8 |  85.3 |  84.0 |  86.6 |  74.9 |  91.9 | 104.9 |  89.1 | 107.7 |  88.8 | 100.1 |  91.9 | 102.6 |  95.6 |  89.3 |  90.0 |  76.9 |  78.8 | 110.7 |  98.8 |  87.7 | 114.6 |  75.4 | 101.4 |  87.5 |  95.2 |  82.5 |  81.2 |  78.7 |  98.9 |  77.8 |  95.1 |  90.7 | 101.4 |  88.0 |  82.0 | 110.9 |  79.1 |  86.5 |  73.8 |  80.8 |  92.5 |  86.9 |  85.9 |  90.0 |  95.2 | 102.7 |  75.9 |  88.1 |  98.4 |  88.3 | 100.4 |  98.9 |  86.4 |  77.0 |  96.4 | 100.1 |   0.0 | 100.3 |  97.8 |  74.8 |  97.5 | 102.9 |  87.4 |  78.6 |  85.7 |  75.4 |  96.4 |  89.6 |  89.9 |  80.9 |  72.8 |  76.4 |  93.2 |  86.1 |\n",
            "| 84  |  89.8 |  84.2 | 110.7 |  94.0 |  92.8 |  78.5 |  97.9 |  76.4 |  89.6 |  87.7 | 110.5 |  74.8 | 106.5 |  68.2 |  90.7 | 120.8 |  90.8 |  98.1 | 100.9 |  90.8 |  87.9 | 104.1 |  69.4 |  96.7 |  94.8 | 106.1 |  97.5 |  97.1 |  91.2 |  93.4 |  95.9 |  98.3 | 110.6 |  86.7 |  69.8 |  76.6 |  62.3 |  98.0 |  87.5 | 112.3 |  82.7 |  83.5 | 108.2 |  80.5 |  71.5 |  90.7 |  92.6 |  73.3 | 100.7 | 102.8 | 110.5 |  78.2 | 102.8 | 113.8 |  85.9 |  77.4 |  98.2 |  99.9 |  95.7 |  82.8 |  89.0 | 106.5 |  66.0 | 102.6 |  74.3 |  77.4 |  88.3 |  99.0 |  90.5 | 103.1 |  76.9 |  91.6 |  91.3 | 102.0 |  88.1 |  71.7 |  87.2 |  65.1 |  84.6 | 104.0 |  95.9 |  92.6 | 105.9 | 100.3 |   0.0 | 107.1 |  94.3 |  95.7 |  95.0 |  97.9 |  96.2 | 102.5 | 103.8 |  70.1 |  79.6 |  91.7 | 103.8 | 106.5 | 106.8 |  83.2 |  92.8 |\n",
            "| 85  |  83.7 |  88.9 |  78.2 |  87.7 |  81.3 |  87.4 |  78.4 | 110.9 | 104.0 |  86.1 |  78.5 |  90.0 |  88.1 | 114.4 |  81.4 |  86.0 |  84.6 |  86.6 |  87.9 |  95.4 |  86.5 |  79.4 | 109.4 |  78.5 |  76.7 | 104.6 |  71.7 | 104.1 |  92.9 |  92.5 |  78.4 |  80.8 |  73.5 |  87.3 |  93.0 | 102.4 | 104.8 |  78.2 | 102.5 |  80.8 |  89.7 |  92.9 |  79.7 | 101.8 |  93.7 |  79.2 | 100.6 | 109.8 |  79.7 |  81.9 | 103.9 |  80.5 |  87.4 |  76.6 |  81.1 |  92.3 |  93.7 |  81.8 |  91.1 |  77.6 | 102.1 |  88.6 |  94.5 |  94.8 |  91.1 |  96.3 |  94.8 | 102.1 |  76.7 |  73.2 | 118.1 |  91.0 |  81.4 |  98.2 | 103.0 | 106.9 |  78.3 |  97.7 | 101.2 |  83.9 |  78.4 |  86.4 |  81.2 |  97.8 | 107.1 |   0.0 |  84.2 |  81.9 |  97.1 |  87.0 |  86.6 |  86.2 |  74.9 |  86.0 | 109.5 |  89.3 |  99.7 |  86.3 |  84.5 | 109.2 | 100.4 |\n",
            "| 86  |  80.5 |  78.1 |  91.1 |  81.9 |  81.9 |  81.2 |  73.0 | 105.0 | 106.4 |  81.3 |  77.8 |  94.8 |  85.8 |  89.9 |  83.6 |  90.5 | 107.6 |  95.9 |  92.4 |  80.0 |  87.5 | 102.6 |  90.4 |  94.4 | 103.9 |  89.9 |  74.2 |  92.4 |  92.8 | 113.2 |  72.4 |  96.7 |  79.0 |  83.4 |  93.2 |  84.2 | 107.9 |  78.3 |  94.1 |  87.5 | 103.1 | 104.1 |  73.6 |  75.4 | 102.2 |  77.9 |  83.4 |  92.9 |  87.1 |  88.9 |  84.3 |  91.7 |  84.9 |  98.1 |  82.4 | 109.9 |  95.4 |  81.9 |  92.7 |  81.7 |  83.5 | 101.1 | 109.7 |  96.7 |  86.1 |  95.6 |  85.7 |  98.0 |  77.9 |  99.9 |  88.1 |  84.6 |  90.7 |  87.4 |  92.1 |  92.7 |  81.6 | 101.4 |  87.0 |  87.9 |  81.3 |  71.9 |  82.3 |  74.8 |  94.3 |  84.2 |   0.0 |  86.0 |  97.1 |  84.1 |  84.3 |  86.2 |  85.7 | 100.9 |  89.8 |  90.0 |  80.0 |  92.0 |  85.4 |  89.4 |  78.2 |\n",
            "| 87  |  87.2 |  81.6 |  90.4 | 102.6 |  72.6 |  95.4 |  75.0 |  89.9 |  87.0 |  85.2 |  85.7 |  95.2 |  89.7 |  99.7 | 106.3 |  76.5 | 107.8 |  78.2 |  80.6 |  84.2 |  70.1 |  86.8 | 107.3 |  96.3 |  82.8 |  79.5 |  78.7 |  76.7 |  99.1 |  93.8 |  78.4 |  82.6 |  87.3 | 108.1 | 109.0 | 106.3 | 106.3 | 103.0 | 105.4 |  83.7 |  97.1 |  79.7 |  97.7 |  85.4 |  86.4 |  85.7 |  92.8 |  91.5 | 101.8 |  94.7 |  81.2 |  87.3 |  77.4 |  77.0 |  89.0 |  83.7 |  86.4 |  75.1 |  92.5 |  92.6 |  94.4 |  91.6 | 103.0 |  72.5 |  91.5 |  98.6 |  84.7 |  77.0 |  81.7 |  95.1 | 107.6 |  92.1 |  83.7 |  85.0 | 133.5 |  92.4 |  94.3 | 103.2 |  96.3 |  75.4 |  83.0 |  89.0 |  88.5 |  97.5 |  95.7 |  81.9 |  86.0 |   0.0 |  82.2 |  75.6 |  78.1 |  79.7 |  91.2 |  87.0 |  84.3 | 106.1 |  95.5 |  75.5 |  82.9 | 105.7 |  90.7 |\n",
            "| 88  |  81.8 |  93.0 | 103.7 |  86.2 |  81.0 |  98.2 | 107.6 |  85.1 |  78.8 |  91.3 |  85.3 |  84.0 | 106.6 |  89.7 |  86.8 | 103.4 |  82.1 |  91.4 |  83.4 |  84.4 |  85.2 |  92.8 |  93.5 |  75.8 |  77.0 |  76.7 |  90.3 |  79.6 |  96.3 |  89.1 |  87.9 |  98.6 |  78.4 |  88.2 |  87.1 |  86.2 |  96.9 |  75.6 |  81.1 |  83.7 |  73.4 |  85.7 | 108.0 |  85.7 |  93.6 |  92.8 |  77.4 |  78.3 |  84.7 |  89.7 |  74.6 |  85.8 |  91.5 |  84.1 |  96.8 |  87.1 | 101.2 |  81.8 |  83.4 |  98.9 |  99.0 |  94.2 |  98.7 |  85.6 |  86.9 | 102.6 |  74.0 |  81.7 |  76.6 |  91.1 |  98.7 |  88.9 |  87.6 |  98.9 |  83.0 |  90.5 |  88.3 |  99.9 |  77.6 |  88.0 |  90.6 |  89.3 |  86.1 | 102.9 |  95.0 |  97.1 |  97.1 |  82.2 |   0.0 | 105.1 | 103.1 |  78.9 |  92.2 |  96.9 |  88.5 |  97.0 |  78.7 | 100.4 |  96.8 |  78.4 |  91.0 |\n",
            "| 89  |  88.5 |  94.7 |  76.3 |  89.0 |  80.8 |  96.8 |  89.5 |  85.7 |  87.5 |  94.4 |  94.6 | 102.9 |  72.9 | 111.3 |  92.2 |  85.6 |  91.0 |  99.0 |  90.7 | 106.0 |  89.8 |  95.9 | 103.6 | 107.5 |  87.6 |  86.4 |  88.8 |  86.6 |  97.7 | 100.6 |  86.1 |  84.0 | 100.4 |  97.8 | 100.4 | 100.3 | 108.9 | 101.1 |  98.1 |  88.0 |  98.3 |  87.6 |  80.0 |  91.0 |  96.5 |  83.8 | 101.5 |  92.5 | 116.5 |  74.9 |  92.2 | 106.9 |  69.0 |  74.3 |  87.7 |  99.5 |  72.0 |  84.8 |  90.2 |  98.4 |  96.4 |  79.0 | 101.6 |  82.6 | 114.4 |  99.1 | 100.1 |  99.5 | 100.9 |  82.9 |  97.1 |  86.0 |  81.7 |  82.4 |  99.6 | 117.4 |  85.7 | 101.6 |  97.3 |  76.5 |  76.1 |  96.1 |  79.6 |  87.4 |  97.9 |  87.0 |  84.1 |  75.6 | 105.1 |   0.0 |  70.8 |  85.9 |  91.1 |  98.7 | 107.4 | 101.5 |  89.2 |  81.4 |  72.5 |  96.0 |  79.7 |\n",
            "| 90  |  86.3 |  90.7 |  81.5 |  69.9 |  98.5 |  90.0 |  81.2 |  87.8 | 104.2 |  91.0 |  86.9 |  98.9 |  79.2 | 104.8 |  89.1 |  69.8 |  98.5 |  70.4 |  93.3 |  97.8 |  88.9 |  85.1 | 112.7 |  89.3 |  78.9 |  91.6 | 114.2 |  98.3 |  93.0 |  91.3 |  82.6 |  78.7 |  76.7 | 106.8 |  99.6 | 103.0 |  97.3 | 101.7 | 104.6 |  94.4 |  96.8 |  92.4 |  89.2 |  96.3 | 105.0 |  83.0 |  86.1 |  95.0 |  97.8 |  96.2 |  93.8 |  94.5 |  67.1 |  75.4 |  86.7 |  93.7 |  72.4 |  83.3 |  92.6 |  95.3 | 100.2 |  93.3 |  97.9 |  73.5 |  92.8 |  97.1 |  84.9 |  94.1 |  85.4 |  94.4 |  88.6 |  78.0 |  87.3 |  86.4 |  92.7 | 100.4 |  79.7 | 102.9 |  99.8 |  79.1 |  94.3 |  77.1 |  92.8 |  78.6 |  96.2 |  86.6 |  84.3 |  78.1 | 103.1 |  70.8 |   0.0 |  84.7 |  89.6 |  97.1 | 107.9 |  90.9 |  87.7 |  76.0 |  82.2 | 102.5 |  74.4 |\n",
            "| 91  |  91.5 |  98.5 |  96.9 |  85.1 |  90.8 |  92.1 |  84.4 |  94.6 |  85.4 |  87.1 |  89.2 |  89.5 |  90.2 | 101.1 |  78.3 |  83.7 |  84.5 |  90.1 |  79.6 | 105.4 |  93.9 |  88.0 | 109.7 | 100.8 |  77.9 |  71.9 |  94.2 |  93.5 |  87.9 |  95.4 |  88.8 | 111.4 |  81.7 | 103.8 |  99.9 |  88.8 |  96.0 |  70.9 | 100.0 |  86.4 |  74.6 |  87.4 |  79.5 |  80.3 |  91.4 |  78.5 |  94.7 |  91.9 | 101.6 |  96.2 |  74.6 |  81.4 |  82.2 |  98.9 |  92.0 | 107.8 |  93.2 |  72.5 | 103.0 |  84.8 |  76.1 |  84.6 |  94.1 |  91.2 |  90.5 |  80.3 |  73.8 |  85.0 |  79.0 |  76.9 | 108.7 |  99.5 |  85.6 |  86.3 |  91.8 | 105.1 |  95.5 | 100.4 |  66.6 |  96.0 |  80.2 | 101.2 |  84.2 |  85.7 | 102.5 |  86.2 |  86.2 |  79.7 |  78.9 |  85.9 |  84.7 |   0.0 |  76.0 |  85.7 | 105.3 | 108.3 |  82.0 |  74.9 |  94.4 |  87.6 |  85.2 |\n",
            "| 92  |  81.4 |  93.2 |  79.2 |  90.0 |  82.3 |  94.2 |  94.8 |  92.1 |  96.3 |  75.8 |  81.9 |  96.7 |  74.2 |  87.7 |  97.0 |  84.0 |  87.7 |  77.8 |  84.7 | 110.6 |  74.8 |  90.7 |  99.4 |  93.1 |  83.5 |  99.2 |  85.1 | 101.3 |  91.4 |  75.9 |  75.0 | 102.6 |  75.0 |  86.5 |  98.1 |  88.8 | 106.8 |  93.2 |  95.5 |  95.0 |  94.4 |  86.4 |  75.5 | 109.0 |  93.3 | 101.2 |  92.8 |  94.4 | 104.2 |  88.7 |  95.7 |  68.5 |  92.4 |  97.8 |  80.0 |  97.0 |  83.7 |  75.8 |  92.2 |  83.9 |  91.9 |  93.5 |  96.2 |  93.7 |  82.8 |  98.3 |  76.3 |  86.1 |  96.3 |  72.7 |  98.8 |  94.1 | 116.6 |  88.1 |  93.2 | 102.4 |  91.9 |  96.6 |  93.1 |  74.4 |  83.3 | 102.0 |  74.4 |  75.4 | 103.8 |  74.9 |  85.7 |  91.2 |  92.2 |  91.1 |  89.6 |  76.0 |   0.0 |  84.2 | 104.2 | 109.5 |  81.3 |  79.1 |  75.9 |  98.9 | 108.8 |\n",
            "| 93  |  99.9 |  90.3 |  92.0 | 109.9 | 109.1 |  83.8 | 107.4 |  76.2 |  76.1 |  83.9 |  79.1 |  76.2 |  94.0 |  79.8 |  78.0 | 102.0 |  78.3 | 103.5 |  80.0 | 109.3 |  80.7 |  80.0 |  71.2 |  93.2 |  80.2 |  94.7 |  94.9 |  83.9 |  95.8 |  97.7 |  85.8 |  93.7 | 100.4 |  90.5 |  77.9 |  75.9 |  78.1 |  87.1 |  98.0 | 103.5 |  88.5 |  82.9 | 106.9 |  77.9 |  69.6 |  96.8 |  77.8 |  77.1 |  95.7 | 107.3 | 106.7 |  71.3 | 106.6 | 111.7 |  88.1 |  80.0 | 105.0 | 103.7 | 102.9 |  79.5 |  80.8 |  81.3 |  73.2 | 100.4 |  80.5 |  80.3 |  87.4 |  97.3 |  86.7 |  83.2 | 100.7 |  95.6 | 100.9 |  91.9 |  98.9 |  68.6 |  76.5 |  67.5 | 108.6 | 102.9 |  94.8 |  97.2 |  97.3 |  96.4 |  70.1 |  86.0 | 100.9 |  87.0 |  96.9 |  98.7 |  97.1 |  85.7 |  84.2 |   0.0 |  92.9 |  80.5 | 108.1 |  89.7 |  88.2 |  89.2 |  94.1 |\n",
            "| 94  |  97.9 |  88.8 |  95.2 |  92.9 | 101.3 |  75.8 |  93.3 |  89.1 |  83.3 |  99.9 | 103.7 | 108.9 |  86.9 |  67.1 | 107.0 |  93.7 |  94.9 |  92.9 | 102.0 |  75.9 |  87.9 |  85.9 |  82.3 |  82.7 | 106.0 |  85.9 |  80.4 |  79.6 |  92.5 |  90.0 |  93.0 |  82.8 |  96.9 |  79.2 |  84.7 |  82.3 |  68.6 |  97.6 |  77.0 | 106.9 |  95.2 |  80.0 |  94.9 |  79.8 |  75.4 |  89.9 |  83.1 |  69.3 |  82.1 | 111.2 |  73.9 |  94.9 |  97.8 |  89.5 | 109.1 |  75.7 |  87.6 |  92.4 |  89.4 |  96.2 |  80.0 | 105.5 |  98.8 |  97.5 |  74.5 |  83.5 |  97.4 |  91.6 |  91.4 | 110.0 |  83.6 |  97.5 |  84.4 |  80.9 | 105.5 |  71.5 |  97.4 |  73.2 |  88.9 |  93.1 |  90.2 |  85.8 | 112.6 |  89.6 |  79.6 | 109.5 |  89.8 |  84.3 |  88.5 | 107.4 | 107.9 | 105.3 | 104.2 |  92.9 |   0.0 |  73.0 |  99.4 | 111.0 |  95.3 |  74.0 |  91.0 |\n",
            "| 95  |  99.9 | 116.4 |  85.0 |  89.0 | 120.2 |  66.9 |  88.6 |  92.2 |  81.7 | 100.6 |  84.2 |  91.0 |  95.4 |  72.4 |  83.3 | 100.1 |  78.4 | 102.1 |  81.8 |  77.8 | 101.7 |  80.6 |  79.4 |  72.5 |  99.5 |  89.7 |  87.1 | 101.3 |  82.8 | 109.4 |  93.9 |  84.6 |  91.5 |  87.1 |  74.4 |  84.8 |  73.8 |  86.9 |  79.2 | 102.0 |  94.4 |  85.2 |  89.2 |  77.6 |  79.3 | 105.0 |  73.8 |  82.0 |  82.6 | 106.1 |  85.8 |  94.1 | 113.2 |  88.2 | 105.2 |  72.4 | 104.5 |  93.9 |  82.0 |  82.4 |  98.0 |  91.9 |  82.1 | 102.1 |  87.5 |  75.7 | 103.3 | 108.7 |  86.4 | 111.6 |  81.6 |  91.3 |  82.6 |  98.8 |  80.3 |  69.6 |  96.1 |  83.7 | 104.8 | 116.5 |  95.3 |  90.7 |  87.6 |  89.9 |  91.7 |  89.3 |  90.0 | 106.1 |  97.0 | 101.5 |  90.9 | 108.3 | 109.5 |  80.5 |  73.0 |   0.0 | 105.7 | 111.4 |  89.8 |  73.4 |  85.4 |\n",
            "| 96  |  75.1 | 103.0 |  82.7 |  93.9 |  75.5 | 111.4 |  88.4 |  92.2 |  84.3 | 104.7 |  75.9 |  94.5 |  88.0 |  88.1 |  92.1 |  96.9 | 106.6 |  98.1 | 102.5 |  93.1 |  78.7 |  81.7 |  85.3 |  82.6 | 108.5 |  77.1 |  95.3 |  76.8 | 107.1 |  77.4 |  91.1 | 101.4 |  74.8 | 112.4 |  90.4 |  82.4 |  93.5 |  85.3 |  81.3 |  76.9 |  89.3 |  95.4 |  87.0 |  81.7 | 104.3 |  83.6 |  97.9 |  84.8 |  84.6 |  88.3 |  77.8 |  89.7 |  94.9 |  94.6 |  79.3 |  91.5 | 103.6 |  99.9 |  88.7 |  91.6 |  86.2 |  86.4 |  92.6 |  94.8 |  81.2 | 107.9 |  82.3 |  77.0 |  79.8 |  86.5 |  78.8 |  84.4 | 104.5 |  85.7 |  90.1 |  99.7 |  90.7 |  95.0 |  83.8 |  89.9 | 100.5 |  74.7 | 101.1 |  80.9 | 103.8 |  99.7 |  80.0 |  95.5 |  78.7 |  89.2 |  87.7 |  82.0 |  81.3 | 108.1 |  99.4 | 105.7 |   0.0 |  81.5 |  92.2 |  87.4 |  96.5 |\n",
            "| 97  |  95.3 |  81.1 |  78.3 |  92.7 |  83.7 | 107.8 |  81.4 | 109.2 | 105.3 |  96.6 |  81.8 |  87.1 |  75.1 | 100.5 |  80.7 |  67.4 |  99.7 |  76.3 |  84.5 |  93.6 |  74.0 |  94.8 | 105.0 |  99.9 |  89.4 |  84.1 |  91.3 |  81.7 |  88.1 |  92.5 |  80.4 |  87.0 |  86.7 |  97.0 | 105.6 | 108.9 | 101.4 |  89.2 | 130.3 |  93.5 |  86.2 |  94.3 |  95.9 |  98.4 | 108.6 |  89.1 |  93.4 | 108.9 |  81.7 |  75.2 |  78.8 |  98.2 |  76.9 |  84.6 |  79.1 |  89.6 |  78.7 |  80.8 |  82.5 |  91.8 |  91.8 |  79.8 | 100.5 |  71.4 |  93.0 |  95.3 |  79.8 |  73.1 |  94.6 |  71.9 |  98.1 |  78.0 |  95.6 |  77.6 | 105.5 | 105.6 |  86.1 |  96.5 |  89.0 |  79.5 |  74.2 |  99.0 |  85.8 |  72.8 | 106.5 |  86.3 |  92.0 |  75.5 | 100.4 |  81.4 |  76.0 |  74.9 |  79.1 |  89.7 | 111.0 | 111.4 |  81.5 |   0.0 |  73.7 | 112.3 |  94.3 |\n",
            "| 98  |  91.0 |  90.6 |  76.5 | 105.4 |  92.7 | 110.8 |  94.2 |  98.0 |  90.4 | 105.3 |  97.3 | 113.1 |  71.3 | 100.2 |  91.8 |  79.1 |  90.2 |  87.7 |  81.1 |  90.6 |  75.0 |  91.7 |  99.4 |  92.1 |  78.6 |  81.7 |  74.0 |  72.0 |  84.7 |  72.4 |  72.1 |  78.6 | 102.1 |  78.3 |  97.3 |  90.8 | 104.2 | 110.7 |  92.2 |  95.4 |  98.5 |  85.8 |  85.8 |  96.4 |  98.3 | 102.2 |  81.8 |  96.2 |  97.4 |  83.9 |  80.0 |  88.1 |  71.5 |  85.9 |  77.1 | 100.5 |  87.3 |  75.7 |  73.4 |  92.4 |  98.4 |  91.2 | 107.5 |  73.0 | 105.0 |  86.6 |  90.2 |  97.4 | 106.5 |  73.0 | 115.8 |  80.7 |  95.0 |  79.4 | 102.2 |  92.8 |  82.5 | 118.4 |  95.4 |  87.4 |  83.1 |  79.3 |  82.2 |  76.4 | 106.8 |  84.5 |  85.4 |  82.9 |  96.8 |  72.5 |  82.2 |  94.4 |  75.9 |  88.2 |  95.3 |  89.8 |  92.2 |  73.7 |   0.0 | 107.1 |  88.4 |\n",
            "| 99  | 111.7 | 109.0 |  95.6 |  91.8 | 104.7 |  61.4 | 100.8 |  73.2 |  79.1 |  98.2 |  96.7 |  97.0 | 103.5 |  77.0 |  84.6 | 108.5 |  78.5 |  99.3 |  92.5 |  87.0 |  97.2 | 108.5 |  67.5 |  78.7 |  87.3 |  82.5 |  95.2 |  88.9 | 104.1 | 114.6 | 112.1 | 105.6 |  80.8 |  83.4 |  87.8 |  71.4 |  73.0 |  81.3 |  81.3 |  79.9 |  74.3 | 103.2 |  92.3 |  76.1 |  71.7 | 102.8 |  65.4 |  76.8 |  99.1 |  98.7 |  84.4 |  87.8 | 103.1 | 100.1 |  82.3 |  81.5 |  95.1 |  94.9 | 102.8 | 100.1 |  69.7 |  85.5 |  74.2 | 117.7 |  68.6 |  85.3 |  95.5 | 100.5 |  91.2 | 110.9 |  82.8 | 101.5 |  91.0 |  86.5 |  76.8 |  77.7 |  79.5 |  73.0 |  82.6 | 102.7 | 110.1 |  89.2 |  98.2 |  93.2 |  83.2 | 109.2 |  89.4 | 105.7 |  78.4 |  96.0 | 102.5 |  87.6 |  98.9 |  89.2 |  74.0 |  73.4 |  87.4 | 112.3 | 107.1 |   0.0 |  90.8 |\n",
            "| SEP |  94.7 |  81.1 |  91.6 |  85.1 | 100.1 |  93.5 |  87.8 |  84.0 |  94.3 |  88.3 |  87.8 |  99.7 |  76.7 | 112.6 |  78.5 |  87.6 |  82.5 |  87.6 |  99.0 |  82.6 | 104.7 |  88.6 | 105.0 |  90.1 |  73.2 |  85.5 |  91.5 |  84.7 |  82.1 |  88.3 | 103.9 |  82.3 | 100.2 |  88.5 |  89.7 |  84.9 | 110.5 |  98.3 |  96.9 |  87.2 | 104.3 | 103.1 |  82.9 |  85.2 | 114.0 |  80.8 |  84.5 | 102.5 |  93.4 |  93.5 |  87.3 | 101.4 |  77.7 |  79.0 |  85.5 | 111.3 |  78.3 |  85.7 |  79.3 |  94.3 |  91.0 |  90.1 | 114.7 |  88.0 | 110.5 |  92.7 |  97.2 |  82.1 |  79.8 |  92.7 |  96.4 |  71.6 |  70.0 |  67.0 |  80.2 |  93.7 |  90.0 | 109.7 |  86.4 |  87.7 |  83.1 |  74.0 |  90.9 |  86.1 |  92.8 | 100.4 |  78.2 |  90.7 |  91.0 |  79.7 |  74.4 |  85.2 | 108.8 |  94.1 |  91.0 |  85.4 |  96.5 |  94.3 |  88.4 |  90.8 |   0.0 |\n",
            "\n",
            "--- Pairwise Angles (Degrees) for W_U (Unembeddings) in 32D ---\n",
            "|     |     0 |     1 |     2 |     3 |     4 |     5 |     6 |     7 |     8 |     9 |    10 |    11 |    12 |    13 |    14 |    15 |    16 |    17 |    18 |    19 |    20 |    21 |    22 |    23 |    24 |    25 |    26 |    27 |    28 |    29 |    30 |    31 |    32 |    33 |    34 |    35 |    36 |    37 |    38 |    39 |    40 |    41 |    42 |    43 |    44 |    45 |    46 |    47 |    48 |    49 |    50 |    51 |    52 |    53 |    54 |    55 |    56 |    57 |    58 |    59 |    60 |    61 |    62 |    63 |    64 |    65 |    66 |    67 |    68 |    69 |    70 |    71 |    72 |    73 |    74 |    75 |    76 |    77 |    78 |    79 |    80 |    81 |    82 |    83 |    84 |    85 |    86 |    87 |    88 |    89 |    90 |    91 |    92 |    93 |    94 |    95 |    96 |    97 |    98 |    99 |   SEP |\n",
            "|:----|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n",
            "| 0   |   0.0 |  77.8 |  97.7 |  80.6 |  75.8 |  98.0 |  85.4 |  82.8 |  87.6 |  83.6 |  82.8 |  86.3 | 100.1 | 108.2 |  99.9 |  98.3 |  89.8 |  95.1 |  88.8 |  92.9 |  85.1 |  70.9 | 120.0 |  82.0 |  95.8 |  88.3 |  79.2 |  74.1 |  72.6 |  77.8 |  87.3 |  99.1 |  88.3 |  92.1 | 108.9 |  89.4 | 114.1 |  87.0 |  92.6 |  86.6 | 102.0 |  88.2 |  70.5 |  88.5 | 109.3 |  92.5 |  87.5 | 102.0 |  78.6 |  85.3 |  81.1 | 104.6 |  98.2 |  95.3 |  81.4 | 108.4 |  89.3 | 101.9 |  68.3 | 101.1 |  86.0 |  88.8 | 114.1 |  86.2 | 107.2 |  98.9 |  74.7 |  98.5 |  91.4 |  85.5 |  89.7 |  95.8 |  73.5 | 101.8 |  95.4 | 115.0 |  92.1 | 116.3 |  88.0 |  93.3 |  74.5 |  84.3 |  66.4 |  79.0 | 107.3 |  77.2 |  72.5 |  89.4 |  84.1 |  93.9 |  95.0 |  79.9 |  71.9 | 108.3 | 105.5 | 112.0 |  74.3 |  98.0 |  91.4 | 122.9 | 101.0 |\n",
            "| 1   |  77.8 |   0.0 |  90.2 |  68.8 |  63.2 | 112.1 |  83.7 |  94.6 | 120.0 |  69.6 |  72.9 | 106.2 |  78.0 | 115.6 | 106.3 |  81.0 | 111.5 |  75.4 |  90.9 |  95.8 |  76.8 |  91.6 | 105.4 | 107.9 |  68.4 |  87.5 |  98.8 |  80.1 |  80.9 |  73.3 |  76.7 |  71.3 |  90.0 |  87.4 | 111.9 | 116.5 | 116.5 |  97.0 | 105.7 |  73.8 | 108.5 | 113.0 |  86.7 | 110.3 | 104.7 |  66.5 |  91.9 | 103.0 |  82.9 |  70.0 |  88.9 | 106.9 |  77.7 |  78.6 |  91.5 | 119.6 |  74.1 |  84.6 |  92.0 | 111.2 |  84.4 |  76.1 | 115.7 |  67.5 | 106.5 | 110.0 |  99.2 |  83.5 |  99.9 |  61.8 | 108.6 |  72.0 |  71.3 |  71.9 | 110.4 | 110.6 |  74.9 | 106.0 | 105.0 |  76.2 |  70.3 |  72.4 |  83.4 |  90.6 | 108.7 |  72.8 |  75.8 |  73.0 |  90.8 |  87.0 |  88.2 |  87.4 |  80.0 | 106.9 | 101.1 | 127.0 | 102.0 |  80.8 |  77.9 | 117.6 |  89.2 |\n",
            "| 2   |  97.7 |  90.2 |   0.0 |  68.3 |  70.3 | 116.9 |  77.5 | 107.8 | 117.3 |  74.2 |  77.3 | 107.0 |  77.6 | 112.7 |  92.4 |  67.5 | 110.0 |  69.5 |  78.5 | 110.2 |  80.2 |  94.7 | 105.9 |  91.7 |  84.5 |  80.1 | 101.6 |  78.2 |  91.8 |  76.7 |  60.6 |  68.1 |  89.1 |  87.1 | 111.8 | 107.6 | 101.6 | 115.9 | 106.6 |  87.5 | 121.5 |  95.9 |  68.9 | 104.6 | 116.3 |  65.4 | 116.0 | 102.7 | 101.6 |  64.5 |  99.6 | 128.0 |  77.2 |  69.1 |  82.2 | 107.0 |  61.6 |  62.0 |  81.4 | 110.1 | 121.7 |  74.3 | 107.6 |  75.4 | 108.8 | 112.2 | 100.2 |  66.6 | 110.8 |  69.7 | 104.8 |  80.6 |  82.5 |  70.1 | 115.9 | 110.6 |  83.4 | 105.7 | 118.0 |  69.8 |  84.2 |  92.5 |  85.4 |  68.0 | 114.9 |  70.7 |  86.0 |  70.0 | 103.9 |  60.6 |  68.7 |  89.3 |  70.2 | 112.2 | 110.4 | 108.2 |  87.1 |  61.8 |  70.2 | 106.8 |  90.0 |\n",
            "| 3   |  80.6 |  68.8 |  68.3 |   0.0 |  79.8 | 103.2 |  68.9 | 103.0 | 113.1 |  69.2 |  80.0 | 109.8 |  72.0 | 111.1 | 102.7 |  78.7 | 104.7 |  65.3 |  73.5 | 107.6 |  88.5 |  91.9 | 117.5 |  89.5 |  75.4 |  88.5 | 110.7 | 105.2 |  73.3 |  87.1 |  54.6 |  68.3 |  83.2 |  90.1 | 119.1 | 126.2 | 109.3 | 108.6 | 101.5 |  96.2 | 102.1 |  97.8 |  67.7 | 102.9 | 110.6 |  68.3 | 106.4 | 104.5 |  84.3 |  70.7 |  89.0 | 117.3 |  67.7 |  68.9 |  96.4 | 120.2 |  64.6 |  67.1 |  97.7 | 120.0 | 106.7 |  70.6 | 114.2 |  65.7 | 110.8 | 113.0 |  93.8 |  83.4 |  95.3 |  70.2 | 106.5 |  66.5 |  77.3 |  88.6 | 104.2 | 122.0 |  82.1 | 104.5 | 117.8 |  73.4 |  62.2 |  88.7 |  93.7 |  65.2 | 112.3 |  72.6 |  77.9 |  78.2 |  88.7 |  71.3 |  67.4 |  76.3 |  69.9 | 119.8 | 112.6 | 108.0 | 101.2 |  76.4 |  82.9 | 109.7 |  95.1 |\n",
            "| 4   |  75.8 |  63.2 |  70.3 |  79.8 |   0.1 | 119.0 |  84.7 |  90.2 | 109.6 |  71.4 |  79.3 |  98.9 |  82.8 | 116.8 | 101.9 |  88.7 | 119.0 |  81.8 |  89.5 |  93.0 |  68.0 |  93.7 | 108.4 |  85.5 |  86.9 |  73.1 |  77.8 |  83.5 |  91.1 |  64.3 |  71.3 |  79.5 |  86.6 |  84.6 | 119.7 | 108.2 | 119.5 |  99.2 |  98.3 |  66.3 | 105.6 |  93.2 |  78.0 | 103.4 | 112.0 |  85.8 | 111.6 | 106.7 |  97.9 |  77.1 |  99.0 | 114.5 |  82.6 |  78.7 |  74.9 | 111.2 |  82.0 |  85.7 |  80.0 | 105.9 | 101.5 |  76.9 | 110.3 |  83.8 | 113.0 | 112.5 |  91.6 |  79.9 |  88.2 |  71.8 | 102.8 |  94.6 |  77.3 |  87.5 | 108.6 | 113.7 |  87.6 | 111.7 |  97.5 |  81.0 |  69.5 |  75.4 |  78.9 |  82.7 | 111.9 |  66.5 |  74.5 |  74.5 |  88.5 |  81.1 |  99.6 |  92.5 |  67.3 | 118.4 | 103.9 | 127.4 |  77.7 |  87.7 |  77.6 | 113.4 | 102.0 |\n",
            "| 5   |  98.0 | 112.1 | 116.9 | 103.2 | 119.0 |   0.0 | 108.8 |  69.2 |  63.9 | 103.6 | 113.5 |  61.7 | 109.4 |  54.4 |  88.0 | 103.3 |  69.2 | 111.2 |  98.3 |  80.1 | 108.3 | 100.8 |  62.6 |  81.6 |  99.7 | 109.3 |  84.9 | 108.0 | 102.6 | 120.5 | 111.0 | 104.3 |  85.4 |  83.2 |  56.9 |  70.7 |  62.5 |  60.0 |  70.7 | 100.3 |  57.2 |  76.7 |  98.5 |  79.6 |  54.5 | 114.4 |  65.1 |  54.1 |  91.5 | 111.6 |  93.6 |  61.6 | 106.6 | 110.6 | 101.6 |  52.4 | 102.0 | 113.4 | 103.1 |  60.4 |  67.1 | 111.1 |  56.8 | 109.5 |  52.9 |  54.3 |  89.9 | 117.4 |  81.1 | 130.6 |  61.5 | 113.4 | 101.0 | 116.3 |  57.0 |  59.6 |  94.0 |  55.1 |  74.8 | 104.3 | 105.9 | 110.9 |  91.6 | 108.7 |  52.6 | 106.9 | 103.4 | 108.7 |  94.8 | 105.3 | 102.9 | 102.4 | 103.8 |  54.2 |  55.3 |  53.0 | 109.3 | 114.8 | 124.3 |  49.5 |  86.7 |\n",
            "| 6   |  85.4 |  83.7 |  77.5 |  68.9 |  84.7 | 108.8 |   0.0 | 116.4 | 105.3 |  72.6 |  82.8 | 113.8 |  61.3 | 103.5 | 105.0 |  67.1 | 106.2 |  49.4 |  85.0 | 104.9 |  98.2 |  96.5 | 113.8 | 102.1 |  81.3 |  91.1 | 103.4 |  98.3 |  75.6 |  86.1 |  63.1 |  64.2 |  90.8 | 116.5 | 123.3 | 112.7 | 101.8 | 116.6 | 109.3 |  80.8 | 116.9 | 114.2 |  82.4 | 108.2 | 107.3 |  60.5 | 120.4 | 124.2 |  99.2 |  64.3 |  99.4 | 105.1 |  56.9 |  65.5 |  84.7 | 114.3 |  57.3 |  59.8 | 101.6 | 115.2 | 103.7 |  65.6 | 104.7 |  61.0 | 114.3 | 115.8 | 106.1 |  56.5 |  98.8 |  83.3 | 108.1 |  59.1 |  77.4 |  78.9 | 118.6 | 108.9 |  94.1 | 110.7 | 102.3 |  54.6 |  85.8 |  78.7 | 101.4 |  63.6 | 109.4 |  78.3 |  77.2 |  56.8 | 113.1 |  59.9 |  49.7 |  81.6 |  88.4 | 121.2 | 115.6 | 105.7 |  96.9 |  58.9 |  80.6 | 111.7 |  86.1 |\n",
            "| 7   |  82.8 |  94.6 | 107.8 | 103.0 |  90.2 |  69.2 | 116.4 |   0.0 |  62.1 |  92.7 |  93.3 |  66.3 | 114.8 |  81.7 |  97.4 | 112.6 |  83.7 | 104.9 |  89.7 |  77.7 |  82.3 |  80.9 |  75.2 |  84.6 |  98.9 |  84.4 |  81.4 |  87.4 |  94.0 |  86.6 | 117.2 | 109.9 |  93.6 |  90.4 |  81.8 |  73.4 |  86.1 |  85.9 |  64.7 |  82.0 |  64.7 |  68.2 |  90.9 |  72.5 |  73.0 | 121.3 |  68.8 |  73.8 |  93.2 | 114.1 |  83.1 |  71.2 | 118.7 | 107.8 |  79.0 |  70.1 | 108.2 | 116.9 |  83.9 |  82.4 |  64.2 |  96.6 |  75.7 | 104.7 |  70.3 |  64.9 |  83.0 | 120.4 |  84.9 |  97.4 |  70.3 | 113.2 |  96.9 | 100.4 |  75.0 |  84.8 |  83.9 |  81.2 |  90.3 | 115.7 | 104.5 |  97.0 |  82.1 | 103.7 |  74.8 | 106.0 | 108.4 | 101.3 |  85.7 | 109.5 | 108.9 |  96.5 |  89.0 |  66.0 |  77.1 |  78.7 |  86.5 | 119.0 | 106.4 |  75.8 |  89.8 |\n",
            "| 8   |  87.6 | 120.0 | 117.3 | 113.1 | 109.6 |  63.9 | 105.3 |  62.1 |   0.0 | 105.2 | 103.1 |  56.2 | 105.6 |  68.2 |  88.0 | 113.3 |  68.3 | 114.9 |  86.1 |  79.2 |  94.5 |  80.1 |  73.2 |  78.3 | 102.0 |  92.3 |  72.2 |  89.3 |  88.3 | 101.3 | 109.6 | 107.4 | 100.6 |  95.3 |  71.1 |  65.6 |  70.8 |  74.6 |  66.4 |  93.8 |  56.9 |  60.7 |  98.9 |  66.5 |  68.4 | 109.1 |  83.0 |  73.5 |  86.4 | 111.0 |  79.1 |  67.1 | 109.1 | 106.5 | 104.2 |  61.0 | 106.9 | 116.2 | 100.4 |  83.5 |  75.2 |  97.1 |  69.6 | 105.2 |  76.8 |  70.0 |  78.5 | 103.4 |  76.9 | 108.1 |  75.3 | 115.9 | 100.5 | 112.8 |  73.1 |  72.8 | 100.5 |  70.7 |  74.6 | 109.2 | 108.3 | 111.0 |  94.6 |  99.6 |  69.4 | 113.9 | 116.9 | 104.1 |  74.4 | 104.0 | 111.6 |  91.3 | 106.1 |  62.2 |  70.4 |  68.1 |  86.1 | 113.9 | 111.3 |  70.5 |  87.8 |\n",
            "| 9   |  83.6 |  69.6 |  74.2 |  69.2 |  71.4 | 103.6 |  72.6 |  92.7 | 105.2 |   0.0 |  66.1 |  97.8 |  84.9 | 112.7 |  96.7 |  78.4 | 111.1 |  67.1 |  64.2 |  98.7 | 102.4 | 100.4 | 108.4 | 103.0 |  82.5 |  74.8 |  95.1 |  93.3 |  81.9 |  69.4 |  68.9 |  64.3 |  97.8 |  94.2 | 120.4 | 104.4 | 118.3 | 108.2 | 101.5 |  65.1 | 115.1 | 100.4 |  76.3 | 104.3 | 113.8 |  71.2 | 107.3 | 123.0 |  86.1 |  82.2 | 109.7 | 111.4 |  84.4 |  76.5 |  86.0 | 119.4 |  77.2 |  75.2 |  90.6 | 108.4 |  99.7 |  64.2 | 113.7 |  64.5 | 112.4 | 104.3 | 107.7 |  70.6 |  88.0 |  72.1 | 112.7 |  83.0 |  82.6 |  68.0 |  99.7 | 112.2 |  89.6 | 116.4 | 112.9 |  73.9 |  69.6 |  95.2 |  81.2 |  62.5 | 112.7 |  70.9 |  80.9 |  72.0 |  91.9 |  82.9 |  83.7 |  77.0 |  66.5 | 106.8 | 113.0 | 115.3 | 107.0 |  83.1 |  83.9 | 112.9 |  93.5 |\n",
            "| 10  |  82.8 |  72.9 |  77.3 |  80.0 |  79.3 | 113.5 |  82.8 |  93.3 | 103.1 |  66.1 |   0.0 |  96.5 |  84.5 | 102.5 |  92.2 |  87.5 |  99.8 |  81.2 |  77.2 |  98.6 |  82.0 |  73.0 | 103.3 |  98.3 |  90.3 |  69.8 |  89.0 |  86.8 |  90.7 |  87.5 |  88.8 |  84.7 |  75.7 | 113.1 | 108.3 |  95.7 | 110.7 |  91.6 | 109.1 |  75.2 | 113.8 |  98.8 |  88.0 | 100.9 | 109.5 |  75.0 |  98.2 | 113.8 |  84.3 |  87.1 |  76.6 | 100.4 | 100.3 |  83.9 |  99.0 | 116.0 |  86.8 |  88.7 |  79.6 | 108.9 |  94.8 |  69.9 | 107.1 |  84.3 | 100.2 | 111.6 | 100.1 |  77.3 |  79.2 |  63.7 | 107.8 |  76.9 |  83.6 |  71.2 | 110.3 | 108.9 |  80.1 | 104.8 | 112.2 |  92.1 |  80.5 |  91.4 |  69.9 |  81.4 | 119.1 |  73.7 |  77.9 |  79.1 |  83.4 |  94.0 |  89.6 |  70.3 |  71.9 |  93.7 | 113.5 | 101.0 |  79.6 |  76.4 |  88.8 | 108.8 |  97.8 |\n",
            "| 11  |  86.3 | 106.2 | 107.0 | 109.8 |  98.9 |  61.7 | 113.8 |  66.3 |  56.2 |  97.8 |  96.5 |   0.0 | 120.8 |  68.0 |  83.8 | 106.9 |  86.0 | 111.2 |  87.1 |  70.8 |  85.7 |  92.8 |  58.2 |  78.4 | 109.1 | 107.4 |  72.3 |  94.2 |  95.8 | 108.8 | 107.5 | 115.1 | 104.2 |  86.0 |  61.7 |  74.3 |  65.1 |  64.5 |  77.5 |  96.9 |  57.4 |  72.1 |  97.9 |  63.1 |  66.4 | 106.2 |  80.7 |  68.3 |  82.3 | 102.7 |  94.0 |  70.4 | 110.4 | 103.7 |  99.0 |  54.0 | 110.2 | 119.7 |  99.2 |  68.8 |  84.5 | 109.0 |  60.7 | 109.1 |  58.6 |  57.2 |  74.9 | 107.5 |  82.6 | 105.4 |  65.7 | 120.6 | 105.7 | 122.1 |  71.9 |  64.8 |  95.2 |  59.4 |  85.6 | 115.6 | 116.8 | 113.6 |  87.3 | 103.0 |  59.5 | 102.5 | 110.5 | 107.3 |  86.0 | 112.7 | 111.5 | 103.3 | 110.3 |  54.6 |  75.7 |  66.8 |  96.1 | 105.4 | 121.1 |  73.8 |  88.3 |\n",
            "| 12  | 100.1 |  78.0 |  77.6 |  72.0 |  82.8 | 109.4 |  61.3 | 114.8 | 105.6 |  84.9 |  84.5 | 120.8 |   0.0 |  94.7 | 118.2 |  51.2 | 116.4 |  58.6 | 107.7 | 109.2 |  91.6 |  98.3 | 101.8 | 100.5 |  69.2 |  96.1 | 110.3 | 111.4 |  82.6 |  81.8 |  66.3 |  68.1 | 105.4 | 112.6 | 102.5 | 112.4 |  96.5 | 114.7 | 118.3 |  98.7 | 105.4 | 114.0 |  85.4 | 116.0 |  96.1 |  69.4 | 115.4 | 105.6 | 116.0 |  63.9 | 109.6 | 104.8 |  45.1 |  50.6 | 100.7 | 105.5 |  52.0 |  59.7 | 108.4 | 117.6 | 107.9 |  65.8 |  96.2 |  63.0 | 105.0 | 113.9 | 111.3 |  54.5 | 103.6 |  74.7 | 113.7 |  56.4 |  84.3 |  66.4 | 113.8 | 100.9 | 115.4 |  95.9 | 107.8 |  51.6 |  77.5 |  83.0 | 112.5 |  67.0 | 108.1 |  86.1 |  85.6 |  66.5 | 111.5 |  54.5 |  57.5 |  83.5 |  77.3 | 110.0 | 101.6 | 104.8 |  97.8 |  57.1 |  69.1 |  99.1 |  77.2 |\n",
            "| 13  | 108.2 | 115.6 | 112.7 | 111.1 | 116.8 |  54.4 | 103.5 |  81.7 |  68.2 | 112.7 | 102.5 |  68.0 |  94.7 |   0.0 | 100.1 |  91.8 |  73.0 |  92.4 | 110.7 |  86.2 | 106.5 |  97.9 |  40.2 |  94.3 | 120.2 | 108.2 |  90.5 | 113.4 | 116.8 | 114.5 | 104.7 | 105.3 |  96.5 | 104.3 |  42.3 |  66.7 |  35.7 |  68.5 |  62.7 | 118.2 |  64.5 |  78.2 | 124.4 |  88.6 |  40.3 | 113.8 |  77.5 |  47.4 |  99.9 | 104.3 |  93.5 |  49.7 |  93.1 | 101.0 | 115.3 |  46.2 | 100.6 | 106.7 |  98.9 |  59.3 |  82.7 | 111.5 |  31.0 | 103.1 |  40.4 |  56.9 |  91.6 | 101.4 |  92.0 | 111.0 |  62.0 |  96.0 | 126.3 | 115.5 |  65.4 |  37.4 | 108.6 |  38.1 |  82.3 |  94.1 | 115.6 | 113.5 | 106.5 | 107.9 |  38.6 | 122.0 | 110.3 | 107.2 |  93.8 | 102.7 |  94.9 | 116.8 | 108.0 |  51.4 |  54.8 |  44.9 |  95.3 |  95.7 | 108.8 |  47.0 |  84.6 |\n",
            "| 14  |  99.9 | 106.3 |  92.4 | 102.7 | 101.9 |  88.0 | 105.0 |  97.4 |  88.0 |  96.7 |  92.2 |  83.8 | 118.2 | 100.1 |   0.0 | 115.0 |  73.0 | 122.9 |  80.4 |  86.4 |  93.9 |  89.1 |  87.5 |  71.9 |  97.3 |  72.2 |  82.8 |  74.2 |  82.2 |  92.7 |  98.1 | 100.8 |  72.8 |  68.6 |  83.1 |  72.0 |  88.4 |  66.0 |  91.2 |  78.7 |  83.6 |  81.1 |  90.2 |  71.2 | 101.2 |  97.0 |  77.1 |  96.6 |  63.0 | 117.4 |  81.0 |  87.8 | 117.1 | 117.0 |  80.3 |  89.9 | 114.8 | 108.0 |  86.8 |  76.5 |  85.2 |  91.8 |  97.9 | 107.0 |  96.9 |  84.9 |  75.7 | 109.0 |  61.8 |  96.9 |  82.1 | 108.3 |  89.2 |  94.7 |  68.8 |  86.0 |  60.5 |  93.5 |  66.1 | 116.3 |  90.0 |  71.2 |  84.3 |  89.9 |  89.4 |  89.1 |  81.9 | 120.5 |  82.5 | 108.6 | 108.7 |  85.5 | 100.9 |  83.6 |  87.8 |  78.3 |  77.0 | 106.0 |  94.0 |  85.2 |  98.6 |\n",
            "| 15  |  98.3 |  81.0 |  67.5 |  78.7 |  88.7 | 103.3 |  67.1 | 112.6 | 113.3 |  78.4 |  87.5 | 106.9 |  51.2 |  91.8 | 115.0 |   0.0 | 120.7 |  56.4 | 101.3 | 108.3 | 103.9 | 104.0 |  91.3 | 103.8 |  77.7 |  96.4 | 126.2 |  99.1 |  91.2 |  87.3 |  67.4 |  70.7 | 108.3 | 111.2 |  91.4 | 106.3 |  84.6 | 110.1 | 112.8 |  95.3 | 109.0 | 116.0 |  88.5 | 112.0 |  88.5 |  61.7 | 103.8 |  91.6 | 110.3 |  58.4 | 118.1 | 111.1 |  52.2 |  54.4 | 100.2 |  96.6 |  56.5 |  57.5 | 109.0 | 111.4 | 111.3 |  70.2 |  86.0 |  54.6 |  91.2 |  98.5 | 114.6 |  54.2 | 121.8 |  81.9 | 107.4 |  61.5 |  84.8 |  70.2 | 113.4 |  89.9 | 113.3 |  93.6 | 116.1 |  53.0 |  98.2 |  90.2 | 107.3 |  79.8 | 104.0 |  82.7 |  94.4 |  61.0 | 112.2 |  61.1 |  49.6 |  87.7 |  84.3 | 106.8 | 101.5 | 103.3 | 104.4 |  54.5 |  77.0 |  96.9 |  70.7 |\n",
            "| 16  |  89.8 | 111.5 | 110.0 | 104.7 | 119.0 |  69.2 | 106.2 |  83.7 |  68.3 | 111.1 |  99.8 |  86.0 | 116.4 |  73.0 |  73.0 | 120.7 |   0.0 | 107.1 |  81.9 |  89.7 |  98.8 |  68.5 |  82.0 |  83.4 |  95.8 |  99.0 |  80.0 |  88.7 |  86.5 | 101.7 | 115.6 | 108.2 |  73.0 |  72.4 |  74.7 |  63.4 |  77.0 |  69.7 |  63.7 | 101.3 |  77.7 |  77.8 | 103.3 |  83.9 |  74.6 | 111.3 |  68.1 |  76.5 |  74.6 | 102.4 |  71.4 |  65.9 | 117.4 | 120.5 |  91.7 |  72.1 | 112.6 | 106.7 |  79.1 |  67.5 |  69.7 | 101.4 |  80.7 | 125.7 |  83.5 |  82.0 |  72.6 | 107.9 |  86.6 | 100.4 |  72.9 | 102.2 |  92.0 | 110.6 |  60.6 |  79.2 |  84.1 |  76.1 |  65.4 | 109.9 |  95.8 | 100.1 |  79.6 | 116.9 |  73.3 |  96.0 | 108.1 | 124.4 |  72.7 | 116.1 | 115.9 |  93.1 |  95.7 |  69.3 |  73.6 |  62.9 |  89.8 | 113.8 | 108.9 |  69.8 |  90.6 |\n",
            "| 17  |  95.1 |  75.4 |  69.5 |  65.3 |  81.8 | 111.2 |  49.4 | 104.9 | 114.9 |  67.1 |  81.2 | 111.2 |  58.6 |  92.4 | 122.9 |  56.4 | 107.1 |   0.0 |  92.4 | 109.2 |  99.2 | 103.4 | 100.4 | 104.1 |  78.1 |  92.7 | 107.7 |  99.3 |  93.4 |  76.6 |  64.2 |  68.6 | 103.3 | 112.3 | 111.8 | 110.8 |  89.7 | 117.9 | 106.5 |  90.2 | 113.9 | 118.4 |  92.6 | 122.3 |  93.7 |  71.2 | 110.0 | 108.2 | 106.8 |  57.2 | 107.9 | 103.1 |  45.3 |  49.5 |  87.4 | 107.6 |  48.5 |  55.3 |  93.9 | 115.4 | 113.2 |  71.9 |  91.4 |  61.9 |  96.0 | 102.9 | 106.9 |  58.1 | 114.3 |  67.1 | 116.6 |  61.3 |  87.4 |  77.7 | 113.5 |  96.6 | 100.4 |  98.2 | 117.8 |  50.8 |  90.8 |  90.0 | 105.9 |  69.6 | 101.4 |  82.2 |  96.7 |  58.6 | 105.3 |  67.0 |  50.8 |  88.2 |  80.1 | 110.4 | 108.0 | 104.2 | 107.6 |  55.7 |  76.5 | 101.8 |  72.1 |\n",
            "| 18  |  88.8 |  90.9 |  78.5 |  73.5 |  89.5 |  98.3 |  85.0 |  89.7 |  86.1 |  64.2 |  77.2 |  87.1 | 107.7 | 110.7 |  80.4 | 101.3 |  81.9 |  92.4 |   0.0 |  84.1 |  82.6 |  81.1 | 107.7 |  92.4 |  81.0 |  79.4 |  87.6 |  90.6 |  68.4 |  89.2 |  78.2 |  78.6 |  78.7 |  72.0 | 118.1 | 104.4 | 117.1 | 105.8 |  86.1 |  78.1 |  98.7 |  78.9 |  71.0 |  86.2 | 110.3 |  87.8 |  92.7 | 109.9 |  78.4 |  90.7 |  80.2 |  99.3 | 104.5 | 106.2 |  83.5 | 108.9 | 101.0 |  84.6 |  92.0 |  96.3 |  90.2 |  78.8 | 110.5 |  90.0 | 116.3 | 100.4 |  79.3 |  87.5 |  91.3 |  83.8 |  96.0 |  95.8 |  88.2 |  89.6 |  92.6 | 108.8 |  75.0 | 110.5 |  99.9 | 105.3 |  76.7 | 102.3 |  64.9 |  80.1 | 110.1 |  78.7 |  81.8 |  91.1 |  76.9 |  98.6 |  96.5 |  71.8 |  73.9 |  97.6 | 113.6 |  95.8 | 100.6 |  92.7 |  81.2 | 103.5 | 108.8 |\n",
            "| 19  |  92.9 |  95.8 | 110.2 | 107.6 |  93.0 |  80.1 | 104.9 |  77.7 |  79.2 |  98.7 |  98.6 |  70.8 | 109.2 |  86.2 |  86.4 | 108.3 |  89.7 | 109.2 |  84.1 |   0.0 |  78.4 |  86.5 |  81.9 |  73.8 |  96.3 |  92.2 |  60.6 |  90.0 |  85.9 |  89.6 | 120.8 |  93.9 |  92.7 |  66.1 |  84.2 |  85.9 |  95.1 |  92.4 |  78.6 |  87.6 |  68.3 |  72.0 |  90.5 |  67.1 |  94.6 | 105.6 |  77.3 |  92.0 |  69.1 | 102.5 |  74.6 |  81.3 | 111.5 | 103.3 |  85.8 |  77.2 | 117.3 | 105.0 |  72.6 |  75.6 |  72.5 | 122.1 |  86.0 | 106.1 |  80.8 |  75.1 |  86.6 |  96.9 |  77.5 | 111.3 |  71.0 | 103.5 |  87.7 |  87.3 |  77.5 |  84.3 |  95.2 |  89.9 |  70.9 | 120.2 |  94.6 |  96.3 |  83.1 |  99.2 |  82.0 | 107.0 |  91.2 | 103.1 |  73.9 | 119.0 | 115.6 |  99.7 | 110.3 |  88.9 |  75.7 |  77.4 |  87.9 | 107.5 |  98.4 |  84.3 |  99.8 |\n",
            "| 20  |  85.1 |  76.8 |  80.2 |  88.5 |  68.0 | 108.3 |  98.2 |  82.3 |  94.5 | 102.4 |  82.0 |  85.7 |  91.6 | 106.5 |  93.9 | 103.9 |  98.8 |  99.2 |  82.6 |  78.4 |   0.0 |  76.3 |  98.6 |  74.6 |  79.7 |  93.2 |  77.8 |  80.9 |  76.6 |  83.2 |  86.3 |  98.5 |  76.0 |  72.5 | 106.7 |  93.6 | 109.1 | 107.1 | 100.8 |  97.1 |  89.3 |  79.3 |  87.2 |  76.3 | 106.2 |  94.0 |  88.1 |  94.7 |  93.8 |  91.5 |  76.0 |  91.6 | 103.7 | 102.1 |  73.3 |  91.8 |  96.6 |  89.4 |  85.5 |  99.8 |  88.8 |  97.4 | 106.6 | 105.6 |  97.3 | 104.4 |  63.6 |  87.6 |  90.7 |  82.7 |  88.7 |  96.3 |  86.7 |  88.8 | 116.2 | 105.1 |  76.1 |  98.8 |  92.0 | 106.2 |  88.4 |  78.7 |  83.5 |  91.4 |  98.5 |  84.5 |  80.0 |  85.3 |  81.0 | 101.1 | 102.6 |  90.9 |  76.9 |  88.6 |  91.4 | 102.7 |  77.3 |  85.1 |  81.7 | 102.6 |  97.5 |\n",
            "| 21  |  70.9 |  91.6 |  94.7 |  91.9 |  93.7 | 100.8 |  96.5 |  80.9 |  80.1 | 100.4 |  73.0 |  92.8 |  98.3 |  97.9 |  89.1 | 104.0 |  68.5 | 103.4 |  81.1 |  86.5 |  76.3 |   0.0 | 107.6 |  76.2 |  93.0 |  92.0 |  88.7 |  87.9 |  73.1 |  74.8 | 107.6 |  98.0 |  71.8 |  91.6 |  97.2 |  90.6 |  98.4 |  90.2 |  79.8 |  93.9 | 102.0 |  72.6 |  80.9 |  79.3 |  95.6 |  97.4 |  89.6 |  91.6 |  71.0 | 105.1 |  65.7 |  85.7 | 116.4 | 104.3 |  96.6 |  89.8 | 111.5 | 107.0 |  74.7 |  81.4 |  77.5 |  92.4 | 100.6 | 109.7 |  99.6 |  99.0 |  67.1 |  88.1 |  79.5 |  89.2 |  82.7 |  97.3 |  78.5 |  93.3 |  96.0 |  97.8 |  96.8 |  93.0 |  90.2 | 112.3 |  84.7 |  88.5 |  79.2 | 104.0 | 102.6 |  80.7 |  95.0 |  99.6 |  81.5 | 115.5 | 102.2 |  80.7 |  84.6 |  85.0 |  86.4 |  85.6 |  76.3 | 106.4 | 103.0 | 108.3 |  97.1 |\n",
            "| 22  | 120.0 | 105.4 | 105.9 | 117.5 | 108.4 |  62.6 | 113.8 |  75.2 |  73.2 | 108.4 | 103.3 |  58.2 | 101.8 |  40.2 |  87.5 |  91.3 |  82.0 | 100.4 | 107.7 |  81.9 |  98.6 | 107.6 |   0.0 |  99.5 | 116.1 | 109.4 |  90.1 | 101.5 | 112.5 | 113.8 | 109.8 | 112.6 | 104.0 |  92.2 |  37.0 |  65.5 |  39.6 |  68.8 |  60.9 | 105.0 |  62.5 |  88.3 | 117.2 |  76.2 |  40.3 | 105.6 |  74.8 |  50.0 |  97.8 |  97.7 |  98.8 |  54.9 |  97.5 |  98.2 | 104.7 |  46.9 | 104.8 | 105.2 | 110.1 |  61.2 |  81.2 | 105.9 |  38.3 | 105.7 |  40.9 |  56.7 |  93.1 | 103.9 |  98.6 | 101.3 |  59.5 |  97.7 | 125.3 | 106.3 |  64.0 |  40.5 |  98.2 |  36.2 |  86.3 |  97.4 | 128.4 | 103.2 | 104.3 | 115.6 |  40.6 | 118.7 | 110.4 | 113.0 |  96.3 | 102.2 | 104.2 | 125.3 | 114.9 |  51.2 |  57.2 |  52.0 |  95.1 | 101.9 | 108.8 |  40.1 |  76.7 |\n",
            "| 23  |  82.0 | 107.9 |  91.7 |  89.5 |  85.5 |  81.6 | 102.1 |  84.6 |  78.3 | 103.0 |  98.3 |  78.4 | 100.5 |  94.3 |  71.9 | 103.8 |  83.4 | 104.1 |  92.4 |  73.8 |  74.6 |  76.2 |  99.5 |   0.0 |  86.2 |  88.2 |  78.8 |  87.1 |  86.7 |  79.4 |  93.9 |  93.3 |  73.6 |  71.3 |  92.2 |  81.4 |  92.7 |  83.2 |  95.1 |  88.9 |  78.7 |  73.3 |  88.4 |  77.3 | 100.3 | 114.8 |  76.6 |  88.1 |  70.0 | 109.4 |  91.1 |  93.0 | 102.6 |  98.8 |  77.7 |  77.8 | 103.5 | 112.3 |  80.5 |  93.5 |  94.6 |  96.2 |  90.7 | 109.2 |  91.0 |  88.0 |  67.8 |  94.0 |  65.5 | 111.2 |  82.4 | 110.2 |  81.0 | 111.1 |  83.7 |  86.6 |  85.2 |  90.2 |  83.7 | 110.6 |  89.7 |  76.8 |  98.5 |  88.5 |  90.2 |  84.8 |  96.0 | 105.5 |  70.7 | 113.3 | 104.1 |  96.1 |  88.2 |  90.6 |  78.3 |  79.9 |  78.3 | 109.6 | 103.1 |  86.8 |  95.8 |\n",
            "| 24  |  95.8 |  68.4 |  84.5 |  75.4 |  86.9 |  99.7 |  81.3 |  98.9 | 102.0 |  82.5 |  90.3 | 109.1 |  69.2 | 120.2 |  97.3 |  77.7 |  95.8 |  78.1 |  81.0 |  96.3 |  79.7 |  93.0 | 116.1 |  86.2 |   0.0 |  98.1 | 104.1 |  92.0 |  74.8 |  78.2 |  78.8 |  70.5 |  88.2 |  82.3 | 108.7 | 110.5 | 114.6 | 112.7 | 120.9 |  79.4 |  99.8 | 110.0 |  76.8 | 110.1 | 107.2 |  73.2 |  91.1 | 107.3 | 102.2 |  72.4 | 101.9 | 103.3 |  69.3 |  74.6 |  77.5 | 109.2 |  69.2 |  67.2 | 100.6 | 116.5 | 102.7 |  80.2 | 110.7 |  76.7 | 112.2 | 117.2 |  92.7 |  70.6 |  94.8 |  83.6 | 123.9 |  73.0 |  68.2 |  72.8 | 105.2 | 108.5 |  81.9 | 108.8 | 100.0 |  75.8 |  84.5 |  74.4 |  91.6 |  91.1 | 111.5 |  71.1 |  89.5 |  70.9 |  85.2 |  76.9 |  71.5 |  72.2 |  76.8 | 102.8 | 110.3 | 113.5 | 109.8 |  82.2 |  73.1 | 102.7 |  89.2 |\n",
            "| 25  |  88.3 |  87.5 |  80.1 |  88.5 |  73.1 | 109.3 |  91.1 |  84.4 |  92.3 |  74.8 |  69.8 | 107.4 |  96.1 | 108.2 |  72.2 |  96.4 |  99.0 |  92.7 |  79.4 |  92.2 |  93.2 |  92.0 | 109.4 |  88.2 |  98.1 |   0.0 |  82.2 |  68.2 |  90.3 |  73.8 |  82.9 |  79.9 |  82.8 |  92.2 | 114.3 |  79.3 | 108.9 |  86.5 |  96.8 |  68.2 |  97.4 |  77.1 |  91.8 |  90.5 | 114.0 |  90.2 |  88.3 | 106.4 |  85.3 | 106.3 |  75.4 | 113.4 | 100.2 | 100.4 |  89.2 | 117.6 |  93.5 |  95.9 |  69.5 | 108.2 |  83.8 |  73.8 | 108.3 |  84.4 | 113.9 |  92.1 |  99.2 |  98.7 |  72.3 |  77.2 | 105.6 | 103.2 |  72.3 |  77.5 |  93.9 | 106.2 |  70.9 | 122.0 |  79.9 | 105.7 |  71.8 |  76.7 |  75.8 |  73.1 | 113.5 |  91.8 |  78.7 |  87.4 |  80.9 |  91.4 |  97.3 |  67.1 |  82.9 | 102.3 |  99.7 | 102.4 |  74.6 |  89.2 |  72.2 | 104.4 |  99.3 |\n",
            "| 26  |  79.2 |  98.8 | 101.6 | 110.7 |  77.8 |  84.9 | 103.4 |  81.4 |  72.2 |  95.1 |  89.0 |  72.3 | 110.3 |  90.5 |  82.8 | 126.2 |  80.0 | 107.7 |  87.6 |  60.6 |  77.8 |  88.7 |  90.1 |  78.8 | 104.1 |  82.2 |   0.0 |  78.0 |  92.5 |  90.8 | 105.9 | 105.9 |  88.7 |  70.5 |  92.0 |  71.6 |  97.2 |  74.3 |  78.4 |  85.5 |  75.7 |  68.4 |  86.5 |  76.4 |  95.4 | 105.1 |  90.2 |  98.0 |  81.4 | 103.5 |  72.6 |  80.2 | 110.2 | 101.4 |  84.8 |  88.4 | 107.5 | 103.1 |  68.5 |  78.0 |  81.1 | 116.3 |  95.4 | 117.2 |  86.9 |  80.8 |  85.7 | 109.8 |  71.3 |  88.6 |  91.5 | 120.2 |  88.6 |  99.3 |  85.3 |  93.1 |  84.7 |  94.5 |  70.4 | 111.3 |  87.0 |  96.4 |  69.9 |  93.3 |  88.0 |  84.5 |  82.0 | 102.0 |  77.7 | 109.3 | 129.0 |  91.1 |  90.1 |  84.2 |  76.0 |  84.4 |  82.8 | 111.2 |  90.8 |  91.4 |  94.8 |\n",
            "| 27  |  74.1 |  80.1 |  78.2 | 105.2 |  83.5 | 108.0 |  98.3 |  87.4 |  89.3 |  93.3 |  86.8 |  94.2 | 111.4 | 113.4 |  74.2 |  99.1 |  88.7 |  99.3 |  90.6 |  90.0 |  80.9 |  87.9 | 101.5 |  87.1 |  92.0 |  68.2 |  78.0 |   0.0 |  94.3 |  76.7 |  94.5 |  91.9 |  86.3 |  80.3 | 100.4 |  78.6 | 101.8 |  86.1 |  93.8 |  67.8 | 106.2 |  93.5 |  87.4 |  84.8 | 109.4 |  81.6 |  77.4 |  95.3 |  73.7 |  86.1 |  68.7 | 108.4 | 101.6 |  95.8 |  74.4 | 103.0 |  87.4 |  98.2 |  67.2 | 103.9 |  85.9 |  90.0 | 113.4 |  87.2 | 102.8 |  95.1 |  83.7 |  95.6 |  96.5 |  81.0 |  96.3 | 100.6 |  77.1 |  77.0 | 101.3 |  99.2 |  63.1 | 111.5 |  82.8 |  92.7 |  94.7 |  75.5 |  75.2 |  90.0 | 105.6 |  90.6 |  85.3 |  83.2 |  76.1 |  91.0 |  99.2 |  88.7 |  96.4 |  99.7 |  92.3 | 110.9 |  68.0 |  91.5 |  79.1 | 104.8 |  84.2 |\n",
            "| 28  |  72.6 |  80.9 |  91.8 |  73.3 |  91.1 | 102.6 |  75.6 |  94.0 |  88.3 |  81.9 |  90.7 |  95.8 |  82.6 | 116.8 |  82.2 |  91.2 |  86.5 |  93.4 |  68.4 |  85.9 |  76.6 |  73.1 | 112.5 |  86.7 |  74.8 |  90.3 |  92.5 |  94.3 |   0.0 |  76.8 |  81.5 |  89.3 |  91.2 |  76.2 | 114.0 |  95.1 | 111.4 | 105.8 |  95.2 |  90.6 |  94.2 |  86.9 |  67.1 |  83.1 | 109.6 |  80.5 |  94.5 | 117.4 |  80.2 |  88.1 |  80.8 | 102.3 |  99.7 |  95.6 |  89.1 | 107.6 |  86.0 |  78.3 |  97.5 | 107.3 |  84.9 |  77.7 | 120.0 |  83.3 | 124.6 | 104.9 |  78.8 |  85.6 |  93.5 |  80.9 |  94.1 |  88.3 |  67.4 |  86.8 |  94.1 | 115.0 |  92.9 | 115.4 |  81.7 |  99.6 |  79.4 |  71.1 |  76.2 |  74.8 | 113.7 |  80.1 |  82.4 |  99.3 |  96.7 |  96.4 |  92.8 |  78.2 |  79.6 | 107.5 |  98.3 |  96.2 |  98.8 |  93.5 |  82.6 | 114.4 |  93.9 |\n",
            "| 29  |  77.8 |  73.3 |  76.7 |  87.1 |  64.3 | 120.5 |  86.1 |  86.6 | 101.3 |  69.4 |  87.5 | 108.8 |  81.8 | 114.5 |  92.7 |  87.3 | 101.7 |  76.6 |  89.2 |  89.6 |  83.2 |  74.8 | 113.8 |  79.4 |  78.2 |  73.8 |  90.8 |  76.7 |  76.8 |   0.0 |  85.7 |  71.9 |  93.1 |  83.2 | 116.1 |  95.6 | 110.6 | 112.8 |  92.1 |  66.5 | 109.8 |  87.8 |  73.6 | 107.9 | 116.8 |  87.3 | 105.6 | 115.2 |  82.3 |  91.9 | 101.1 | 105.6 |  87.5 |  86.4 |  68.5 | 113.5 |  83.9 |  92.6 |  70.5 | 100.9 |  91.7 |  82.2 | 106.9 |  81.5 | 116.5 | 101.9 |  90.4 |  73.7 |  88.4 |  68.3 | 104.1 |  95.7 |  78.7 |  72.1 |  96.1 | 105.2 |  89.5 | 115.9 |  89.6 |  86.5 |  79.5 |  69.3 |  92.2 |  73.3 | 108.4 |  77.7 |  93.5 |  90.4 |  94.1 |  95.9 |  90.7 |  87.7 |  68.8 | 111.9 | 101.2 | 118.3 |  78.3 |  92.8 |  73.4 | 120.0 |  93.4 |\n",
            "| 30  |  87.3 |  76.7 |  60.6 |  54.6 |  71.3 | 111.0 |  63.1 | 117.2 | 109.6 |  68.9 |  88.8 | 107.5 |  66.3 | 104.7 |  98.1 |  67.4 | 115.6 |  64.2 |  78.2 | 120.8 |  86.3 | 107.6 | 109.8 |  93.9 |  78.8 |  82.9 | 105.9 |  94.5 |  81.5 |  85.7 |   0.0 |  60.1 |  95.4 |  92.5 | 118.0 | 110.3 | 102.2 | 106.1 | 109.4 |  98.5 | 110.0 | 101.4 |  87.7 | 105.2 | 109.3 |  69.1 | 107.3 | 102.3 | 100.4 |  68.0 | 111.2 | 123.6 |  56.5 |  72.6 |  94.7 | 110.4 |  65.4 |  61.7 | 100.5 | 118.4 | 119.2 |  63.0 | 110.2 |  58.2 | 116.3 | 116.3 |  98.0 |  72.2 | 101.1 |  76.0 | 120.2 |  71.8 |  88.2 |  93.4 | 112.3 | 107.7 |  84.3 | 110.0 | 119.2 |  59.8 |  72.3 |  79.4 | 100.1 |  59.3 | 109.3 |  69.3 |  72.6 |  66.1 |  98.6 |  61.8 |  62.8 |  88.5 |  69.8 | 110.2 | 108.1 | 108.7 | 104.4 |  63.2 |  57.5 | 116.1 |  90.2 |\n",
            "| 31  |  99.1 |  71.3 |  68.1 |  68.3 |  79.5 | 104.3 |  64.2 | 109.9 | 107.4 |  64.3 |  84.7 | 115.1 |  68.1 | 105.3 | 100.8 |  70.7 | 108.2 |  68.6 |  78.6 |  93.9 |  98.5 |  98.0 | 112.6 |  93.3 |  70.5 |  79.9 | 105.9 |  91.9 |  89.3 |  71.9 |  60.1 |   0.0 |  95.7 |  92.1 | 116.3 | 118.4 | 110.1 | 117.9 | 105.2 |  77.8 | 110.5 |  94.9 |  92.5 | 115.7 | 116.4 |  63.3 | 111.7 | 109.0 |  84.3 |  71.6 | 109.5 | 120.0 |  67.6 |  75.8 |  94.6 | 106.7 |  69.5 |  76.9 |  84.9 | 112.4 | 100.7 |  66.1 | 104.7 |  55.5 | 118.1 | 112.5 | 126.3 |  62.4 |  93.8 |  84.7 | 108.9 |  62.7 |  73.1 |  67.8 | 109.8 | 106.7 |  82.2 | 109.5 | 112.5 |  64.1 |  66.1 |  87.5 | 102.7 |  72.3 | 105.5 |  74.9 |  89.3 |  64.3 |  97.6 |  66.9 |  67.7 |  94.3 |  88.4 | 110.5 | 105.2 | 106.2 | 109.6 |  66.8 |  67.4 | 114.9 |  99.8 |\n",
            "| 32  |  88.3 |  90.0 |  89.1 |  83.2 |  86.6 |  85.4 |  90.8 |  93.6 | 100.6 |  97.8 |  75.7 | 104.2 | 105.4 |  96.5 |  72.8 | 108.3 |  73.0 | 103.3 |  78.7 |  92.7 |  76.0 |  71.8 | 104.0 |  73.6 |  88.2 |  82.8 |  88.7 |  86.3 |  91.2 |  93.1 |  95.4 |  95.7 |   0.0 |  79.4 | 100.9 |  94.5 | 104.5 |  78.8 |  85.6 |  74.5 |  99.1 |  82.3 |  82.6 |  98.8 | 103.1 |  96.6 |  81.9 |  90.9 |  76.8 | 109.6 |  65.9 |  81.7 | 113.4 | 115.0 |  81.0 |  99.6 | 103.2 |  95.5 |  85.5 |  80.2 |  78.5 |  98.5 | 102.0 | 108.9 |  95.1 | 107.3 |  73.7 |  99.9 |  64.5 | 102.6 |  80.7 |  97.8 |  95.9 | 100.5 |  86.7 |  96.9 |  62.9 |  96.3 |  77.7 | 100.5 |  88.1 |  75.6 |  73.1 | 104.2 | 103.2 |  79.5 |  69.6 |  99.2 |  76.3 | 111.2 |  98.8 |  82.8 |  75.2 |  96.5 |  90.2 |  89.7 |  69.3 | 104.2 | 105.3 |  89.5 |  92.4 |\n",
            "| 33  |  92.1 |  87.4 |  87.1 |  90.1 |  84.6 |  83.2 | 116.5 |  90.4 |  95.3 |  94.2 | 113.1 |  86.0 | 112.6 | 104.3 |  68.6 | 111.2 |  72.4 | 112.3 |  72.0 |  66.1 |  72.5 |  91.6 |  92.2 |  71.3 |  82.3 |  92.2 |  70.5 |  80.3 |  76.2 |  83.2 |  92.5 |  92.1 |  79.4 |   0.0 |  94.9 |  84.1 | 105.0 |  85.9 |  80.6 |  93.9 |  81.2 |  78.2 |  79.1 |  77.8 | 104.5 | 102.9 |  70.9 |  87.1 |  71.2 |  93.1 |  86.8 |  98.5 | 109.7 | 115.5 |  75.4 |  88.5 | 109.8 |  96.9 |  77.6 |  77.3 |  76.9 | 108.2 | 105.3 | 115.9 | 100.2 |  89.4 |  72.2 | 104.8 |  91.4 |  97.6 |  79.8 | 112.8 |  83.1 |  96.1 |  70.4 |  97.3 |  74.6 |  97.7 |  72.3 | 112.1 |  80.3 |  86.3 |  75.3 |  97.4 |  90.0 |  86.2 |  83.5 | 118.9 |  77.0 | 113.1 | 123.1 |  99.0 |  83.0 |  92.2 |  77.9 |  89.9 |  96.7 | 110.8 |  86.5 |  88.4 |  92.5 |\n",
            "| 34  | 108.9 | 111.9 | 111.8 | 119.1 | 119.7 |  56.9 | 123.3 |  81.8 |  71.1 | 120.4 | 108.3 |  61.7 | 102.5 |  42.3 |  83.1 |  91.4 |  74.7 | 111.8 | 118.1 |  84.2 | 106.7 |  97.2 |  37.0 |  92.2 | 108.7 | 114.3 |  92.0 | 100.4 | 114.0 | 116.1 | 118.0 | 116.3 | 100.9 |  94.9 |   0.0 |  68.0 |  36.9 |  62.1 |  67.4 | 112.2 |  61.6 |  79.1 | 113.1 |  78.8 |  40.5 | 105.3 |  74.4 |  41.5 |  95.0 | 106.3 |  93.7 |  51.6 |  99.2 |  95.7 | 112.7 |  43.5 | 104.7 | 106.7 | 103.5 |  54.6 |  90.3 | 119.2 |  40.6 | 103.5 |  39.2 |  53.2 |  87.9 | 110.7 |  89.9 | 108.9 |  60.9 |  99.0 | 117.4 | 107.3 |  63.3 |  38.2 | 104.8 |  36.1 |  78.3 |  99.5 | 119.4 | 105.4 | 101.6 | 123.3 |  40.8 | 113.7 | 112.0 | 111.4 |  88.8 | 100.1 |  99.4 | 115.3 | 119.2 |  50.9 |  55.8 |  51.8 |  93.4 | 106.2 | 112.5 |  47.0 |  79.4 |\n",
            "| 35  |  89.4 | 116.5 | 107.6 | 126.2 | 108.2 |  70.7 | 112.7 |  73.4 |  65.6 | 104.4 |  95.7 |  74.3 | 112.4 |  66.7 |  72.0 | 106.3 |  63.4 | 110.8 | 104.4 |  85.9 |  93.6 |  90.6 |  65.5 |  81.4 | 110.5 |  79.3 |  71.6 |  78.6 |  95.1 |  95.6 | 110.3 | 118.4 |  94.5 |  84.1 |  68.0 |   0.0 |  67.2 |  66.1 |  73.4 | 102.7 |  71.9 |  79.1 | 112.2 |  65.0 |  73.0 | 110.9 |  62.1 |  73.3 |  91.5 | 112.2 |  88.9 |  73.6 | 115.8 | 120.6 |  87.6 |  70.8 | 113.9 | 110.4 |  79.3 |  75.4 |  68.4 | 101.5 |  72.1 | 122.9 |  70.5 |  66.4 |  84.4 | 108.1 |  83.8 | 100.6 |  80.0 | 114.7 |  93.6 | 103.3 |  67.2 |  70.3 |  88.2 |  80.9 |  64.1 | 114.9 | 113.8 |  87.1 |  89.7 | 101.5 |  66.4 | 105.7 |  98.5 | 119.6 |  85.3 | 113.7 | 116.6 |  94.4 |  95.5 |  57.6 |  63.6 |  63.5 |  79.4 | 110.9 |  98.5 |  66.0 |  86.6 |\n",
            "| 36  | 114.1 | 116.5 | 101.6 | 109.3 | 119.5 |  62.5 | 101.8 |  86.1 |  70.8 | 118.3 | 110.7 |  65.1 |  96.5 |  35.7 |  88.4 |  84.6 |  77.0 |  89.7 | 117.1 |  95.1 | 109.1 |  98.4 |  39.6 |  92.7 | 114.6 | 108.9 |  97.2 | 101.8 | 111.4 | 110.6 | 102.2 | 110.1 | 104.5 | 105.0 |  36.9 |  67.2 |   0.0 |  65.9 |  67.8 | 116.1 |  64.7 |  83.0 | 117.6 |  81.7 |  36.2 |  99.7 |  83.9 |  46.8 |  97.5 | 100.4 |  95.0 |  57.8 |  86.1 |  89.9 | 113.2 |  46.2 |  92.8 |  99.8 | 107.5 |  63.7 |  91.8 | 109.4 |  33.1 |  99.6 |  39.9 |  52.1 |  89.4 | 100.6 |  95.9 | 101.9 |  70.5 |  98.4 | 115.8 | 114.5 |  71.5 |  35.7 | 103.9 |  33.5 |  83.1 |  91.8 | 128.5 | 103.9 | 116.7 | 109.7 |  40.6 | 114.1 | 121.1 | 104.5 | 100.4 |  96.2 |  86.9 | 112.5 | 120.5 |  53.3 |  54.7 |  48.7 |  97.1 |  93.6 | 110.2 |  50.8 |  74.4 |\n",
            "| 37  |  87.0 |  97.0 | 115.9 | 108.6 |  99.2 |  60.0 | 116.6 |  85.9 |  74.6 | 108.2 |  91.6 |  64.5 | 114.7 |  68.5 |  66.0 | 110.1 |  69.7 | 117.9 | 105.8 |  92.4 | 107.1 |  90.2 |  68.8 |  83.2 | 112.7 |  86.5 |  74.3 |  86.1 | 105.8 | 112.8 | 106.1 | 117.9 |  78.8 |  85.9 |  62.1 |  66.1 |  65.9 |   0.0 |  79.6 |  90.4 |  68.0 |  86.4 | 104.8 |  80.8 |  65.4 | 108.3 |  66.7 |  63.9 |  73.9 | 110.2 |  79.1 |  75.2 | 108.9 | 110.6 | 108.6 |  71.7 | 111.8 | 125.6 |  91.2 |  63.4 |  69.6 | 101.3 |  71.5 | 115.4 |  65.6 |  66.8 |  81.7 | 122.2 |  68.4 | 102.6 |  77.6 | 120.2 |  96.2 | 122.7 |  57.1 |  66.1 |  83.2 |  64.8 |  67.0 | 110.6 |  98.3 |  93.4 |  82.2 | 110.6 |  72.7 |  94.5 |  93.0 | 119.8 |  84.2 | 115.7 | 119.4 |  91.6 | 103.9 |  63.3 |  64.7 |  68.5 |  85.6 | 112.5 | 116.4 |  70.3 |  85.0 |\n",
            "| 38  |  92.6 | 105.7 | 106.6 | 101.5 |  98.3 |  70.7 | 109.3 |  64.7 |  66.4 | 101.5 | 109.1 |  77.5 | 118.3 |  62.7 |  91.2 | 112.8 |  63.7 | 106.5 |  86.1 |  78.6 | 100.8 |  79.8 |  60.9 |  95.1 | 120.9 |  96.8 |  78.4 |  93.8 |  95.2 |  92.1 | 109.4 | 105.2 |  85.6 |  80.6 |  67.4 |  73.4 |  67.8 |  79.6 |   0.0 |  93.7 |  71.2 |  63.6 |  95.3 |  75.8 |  60.6 | 111.0 |  82.5 |  61.9 |  81.6 | 108.8 |  78.4 |  61.8 | 112.4 | 110.9 |  97.0 |  67.7 | 118.4 | 106.1 |  88.9 |  58.0 |  71.1 | 109.2 |  66.0 | 108.9 |  71.8 |  67.0 |  87.0 | 118.3 |  90.1 |  99.3 |  58.6 | 106.2 | 113.7 | 115.2 |  60.1 |  68.7 |  93.5 |  67.3 |  79.0 | 105.8 | 105.1 |  98.5 |  86.1 | 118.1 |  61.0 | 102.6 | 102.9 | 118.2 |  81.6 | 110.2 | 115.9 | 117.0 | 101.1 |  75.3 |  62.0 |  61.9 |  84.5 | 130.8 | 105.1 |  67.0 |  89.0 |\n",
            "| 39  |  86.6 |  73.8 |  87.5 |  96.2 |  66.3 | 100.3 |  80.8 |  82.0 |  93.8 |  65.1 |  75.2 |  96.9 |  98.7 | 118.2 |  78.7 |  95.3 | 101.3 |  90.2 |  78.1 |  87.6 |  97.1 |  93.9 | 105.0 |  88.9 |  79.4 |  68.2 |  85.5 |  67.8 |  90.6 |  66.5 |  98.5 |  77.8 |  74.5 |  93.9 | 112.2 | 102.7 | 116.1 |  90.4 |  93.7 |   0.0 | 103.2 | 100.7 |  74.4 | 108.0 | 108.6 |  84.1 |  97.4 | 117.4 |  78.6 |  92.8 |  88.8 |  97.9 |  97.2 |  87.6 |  66.8 | 112.3 |  86.5 |  97.7 |  87.6 | 100.6 |  83.5 |  72.2 | 105.8 |  78.5 | 110.1 | 101.3 | 102.7 |  86.0 |  74.8 |  83.4 |  98.6 |  96.5 |  80.8 |  75.5 |  90.6 | 102.9 |  69.9 | 112.1 |  86.3 |  85.5 |  87.8 |  72.7 |  74.1 |  88.3 | 113.9 |  75.3 |  85.7 |  81.3 |  86.6 |  91.0 |  96.2 |  83.0 |  85.0 | 111.9 | 108.7 | 115.4 |  77.2 |  99.4 |  92.4 | 100.9 |  93.8 |\n",
            "| 40  | 102.0 | 108.5 | 121.5 | 102.1 | 105.6 |  57.2 | 116.9 |  64.7 |  56.9 | 115.1 | 113.8 |  57.4 | 105.4 |  64.5 |  83.6 | 109.0 |  77.7 | 113.9 |  98.7 |  68.3 |  89.3 | 102.0 |  62.5 |  78.7 |  99.8 |  97.4 |  75.7 | 106.2 |  94.2 | 109.8 | 110.0 | 110.5 |  99.1 |  81.2 |  61.6 |  71.9 |  64.7 |  68.0 |  71.2 | 103.2 |   0.0 |  65.3 | 108.0 |  68.3 |  62.5 | 113.1 |  75.2 |  68.8 |  81.7 | 110.6 |  85.6 |  63.1 | 103.3 | 106.0 |  93.8 |  55.0 | 113.1 | 109.8 |  98.9 |  74.9 |  69.6 | 107.0 |  59.4 | 107.2 |  57.9 |  56.2 |  89.8 | 119.8 |  75.3 | 102.0 |  75.4 | 107.2 | 104.0 | 115.5 |  62.5 |  74.5 |  88.5 |  63.2 |  71.2 | 113.8 | 103.8 | 105.5 | 105.8 | 104.2 |  59.9 | 106.1 | 117.8 | 116.1 |  77.3 | 109.4 | 113.9 |  94.0 | 106.9 |  63.5 |  69.5 |  64.6 |  91.5 | 105.3 | 106.2 |  62.0 |  91.8 |\n",
            "| 41  |  88.2 | 113.0 |  95.9 |  97.8 |  93.2 |  76.7 | 114.2 |  68.2 |  60.7 | 100.4 |  98.8 |  72.1 | 114.0 |  78.2 |  81.1 | 116.0 |  77.8 | 118.4 |  78.9 |  72.0 |  79.3 |  72.6 |  88.3 |  73.3 | 110.0 |  77.1 |  68.4 |  93.5 |  86.9 |  87.8 | 101.4 |  94.9 |  82.3 |  78.2 |  79.1 |  79.1 |  83.0 |  86.4 |  63.6 | 100.7 |  65.3 |   0.0 |  92.7 |  72.3 |  86.8 | 107.2 |  94.9 |  74.6 |  81.2 | 131.6 |  73.1 |  73.3 | 123.3 | 117.1 | 102.9 |  71.2 | 114.2 | 107.8 |  80.0 |  71.0 |  84.1 | 115.8 |  80.1 | 104.7 |  82.9 |  67.3 |  79.0 | 116.1 |  66.8 | 101.2 |  70.4 | 121.3 |  97.6 | 106.0 |  84.9 |  83.0 |  88.7 |  83.0 |  73.6 | 122.9 |  85.4 | 106.7 |  83.9 |  98.4 |  74.6 |  97.7 | 101.6 | 103.3 |  76.4 | 110.6 | 112.6 |  91.2 |  94.5 |  71.9 |  69.5 |  72.0 |  85.8 | 111.6 |  99.6 |  89.5 | 101.1 |\n",
            "| 42  |  70.5 |  86.7 |  68.9 |  67.7 |  78.0 |  98.5 |  82.4 |  90.9 |  98.9 |  76.3 |  88.0 |  97.9 |  85.4 | 124.4 |  90.2 |  88.5 | 103.3 |  92.6 |  71.0 |  90.5 |  87.2 |  80.9 | 117.2 |  88.4 |  76.8 |  91.8 |  86.5 |  87.4 |  67.1 |  73.6 |  87.7 |  92.5 |  82.6 |  79.1 | 113.1 | 112.2 | 117.6 | 104.8 |  95.3 |  74.4 | 108.0 |  92.7 |   0.0 |  94.3 | 115.3 |  78.1 | 109.0 | 110.9 |  92.7 |  81.7 |  85.0 | 108.2 |  89.0 |  81.0 |  74.7 | 121.6 |  75.0 |  79.2 |  92.5 |  98.3 |  95.8 |  91.1 | 116.7 |  86.9 | 112.2 | 104.5 |  81.7 |  87.6 |  95.1 |  80.0 |  94.2 |  97.9 |  79.0 |  77.7 |  94.8 | 119.1 |  90.6 | 110.4 |  94.2 |  88.1 |  79.7 |  88.3 |  68.2 |  69.7 | 119.2 |  75.5 |  67.8 |  89.9 | 103.5 |  77.7 |  83.4 |  69.4 |  63.5 | 120.4 | 112.1 | 113.8 |  81.8 |  97.3 |  87.3 | 107.0 |  96.6 |\n",
            "| 43  |  88.5 | 110.3 | 104.6 | 102.9 | 103.4 |  79.6 | 108.2 |  72.5 |  66.5 | 104.3 | 100.9 |  63.1 | 116.0 |  88.6 |  71.2 | 112.0 |  83.9 | 122.3 |  86.2 |  67.1 |  76.3 |  79.3 |  76.2 |  77.3 | 110.1 |  90.5 |  76.4 |  84.8 |  83.1 | 107.9 | 105.2 | 115.7 |  98.8 |  77.8 |  78.8 |  65.0 |  81.7 |  80.8 |  75.8 | 108.0 |  68.3 |  72.3 |  94.3 |   0.0 |  82.3 | 101.2 |  72.9 |  77.7 |  76.9 | 110.3 |  77.8 |  84.8 | 113.8 | 109.6 |  89.7 |  71.4 | 127.9 | 105.9 |  91.2 |  76.4 |  73.5 | 101.8 |  88.6 | 117.2 |  79.1 |  71.2 |  69.2 | 107.5 |  77.4 | 108.9 |  72.0 | 107.9 |  87.2 | 101.5 |  82.5 |  85.7 |  91.6 |  83.1 |  79.6 | 125.7 |  99.0 |  91.6 |  95.7 |  97.4 |  74.7 | 103.9 |  88.7 | 105.6 |  77.9 | 107.4 | 111.0 |  89.0 | 114.0 |  69.0 |  73.4 |  69.7 |  82.2 | 112.3 | 104.8 |  79.9 |  93.6 |\n",
            "| 44  | 109.3 | 104.7 | 116.3 | 110.6 | 112.0 |  54.5 | 107.3 |  73.0 |  68.4 | 113.8 | 109.5 |  66.4 |  96.1 |  40.3 | 101.2 |  88.5 |  74.6 |  93.7 | 110.3 |  94.6 | 106.2 |  95.6 |  40.3 | 100.3 | 107.2 | 114.0 |  95.4 | 109.4 | 109.6 | 116.8 | 109.3 | 116.4 | 103.1 | 104.5 |  40.5 |  73.0 |  36.2 |  65.4 |  60.6 | 108.6 |  62.5 |  86.8 | 115.3 |  82.3 |   0.0 | 111.4 |  74.0 |  41.5 | 106.5 | 101.7 |  94.1 |  47.5 |  89.4 |  91.1 | 111.0 |  47.6 |  99.1 | 102.8 | 115.5 |  60.4 |  81.8 | 103.5 |  35.4 | 103.2 |  42.4 |  48.3 |  88.0 | 109.7 |  99.7 | 103.4 |  69.2 | 100.5 | 111.7 | 116.6 |  70.6 |  38.0 | 109.6 |  34.2 |  85.9 |  96.1 | 117.9 | 104.8 | 103.2 | 123.8 |  42.8 | 107.4 | 117.2 | 101.0 |  97.1 |  98.0 |  95.8 | 110.9 | 113.7 |  48.8 |  53.1 |  51.1 | 106.5 | 106.0 | 111.5 |  43.9 |  77.0 |\n",
            "| 45  |  92.5 |  66.5 |  65.4 |  68.3 |  85.8 | 114.4 |  60.5 | 121.3 | 109.1 |  71.2 |  75.0 | 106.2 |  69.4 | 113.8 |  97.0 |  61.7 | 111.3 |  71.2 |  87.8 | 105.6 |  94.0 |  97.4 | 105.6 | 114.8 |  73.2 |  90.2 | 105.1 |  81.6 |  80.5 |  87.3 |  69.1 |  63.3 |  96.6 | 102.9 | 105.3 | 110.9 |  99.7 | 108.3 | 111.0 |  84.1 | 113.1 | 107.2 |  78.1 | 101.2 | 111.4 |   0.0 | 126.5 | 110.4 |  89.4 |  59.0 |  94.4 | 115.7 |  65.6 |  64.7 | 102.1 | 117.9 |  58.8 |  56.2 | 104.4 | 115.9 | 105.2 |  76.6 | 111.1 |  59.7 | 107.6 | 117.5 | 115.3 |  59.4 | 100.5 |  65.9 | 115.6 |  59.7 |  72.9 |  61.8 | 123.3 | 114.1 |  84.1 | 104.0 | 104.9 |  58.0 |  88.6 |  88.5 |  99.6 |  80.2 | 110.3 |  72.9 |  82.8 |  61.3 | 100.1 |  59.7 |  63.5 |  74.0 |  93.8 | 112.4 | 111.9 | 119.0 |  97.1 |  61.1 |  83.6 | 115.6 |  80.6 |\n",
            "| 46  |  87.5 |  91.9 | 116.0 | 106.4 | 111.6 |  65.1 | 120.4 |  68.8 |  83.0 | 107.3 |  98.2 |  80.7 | 115.4 |  77.5 |  77.1 | 103.8 |  68.1 | 110.0 |  92.7 |  77.3 |  88.1 |  89.6 |  74.8 |  76.6 |  91.1 |  88.3 |  90.2 |  77.4 |  94.5 | 105.6 | 107.3 | 111.7 |  81.9 |  70.9 |  74.4 |  62.1 |  83.9 |  66.7 |  82.5 |  97.4 |  75.2 |  94.9 | 109.0 |  72.9 |  74.0 | 126.5 |   0.0 |  65.9 |  79.8 | 105.2 |  78.5 |  80.3 | 112.7 | 117.2 |  84.9 |  72.9 | 115.5 | 114.8 |  80.2 |  80.3 |  64.5 |  98.0 |  83.5 | 109.6 |  73.6 |  70.5 |  72.8 | 118.9 |  92.5 | 112.6 |  75.9 | 104.1 |  90.7 | 104.2 |  63.4 |  72.0 |  78.0 |  84.7 |  80.3 | 117.9 |  99.9 |  81.0 |  78.2 | 108.5 |  78.4 | 106.9 |  90.5 | 112.1 |  73.3 | 120.5 | 109.2 |  99.3 |  95.8 |  66.7 |  69.0 |  68.0 |  93.5 | 111.7 |  93.5 |  67.8 |  86.8 |\n",
            "| 47  | 102.0 | 103.0 | 102.7 | 104.5 | 106.7 |  54.1 | 124.2 |  73.8 |  73.5 | 123.0 | 113.8 |  68.3 | 105.6 |  47.4 |  96.6 |  91.6 |  76.5 | 108.2 | 109.9 |  92.0 |  94.7 |  91.6 |  50.0 |  88.1 | 107.3 | 106.4 |  98.0 |  95.3 | 117.4 | 115.2 | 102.3 | 109.0 |  90.9 |  87.1 |  41.5 |  73.3 |  46.8 |  63.9 |  61.9 | 117.4 |  68.8 |  74.6 | 110.9 |  77.7 |  41.5 | 110.4 |  65.9 |   0.0 | 101.5 | 103.2 |  89.4 |  66.8 |  96.6 | 104.1 | 114.1 |  48.6 | 103.1 | 110.1 | 101.4 |  60.4 |  82.4 | 115.1 |  47.6 | 107.8 |  47.9 |  56.7 |  79.0 | 115.4 |  98.9 | 114.9 |  59.4 | 107.4 | 108.8 | 119.9 |  71.2 |  45.0 |  98.6 |  42.2 |  85.1 | 102.5 | 112.0 | 104.3 |  97.8 | 126.9 |  45.2 | 113.3 | 105.7 | 103.5 |  87.9 |  99.2 |  98.1 | 115.2 | 110.3 |  51.9 |  49.0 |  58.1 |  97.2 | 109.0 | 109.4 |  51.7 |  80.9 |\n",
            "| 48  |  78.6 |  82.9 | 101.6 |  84.3 |  97.9 |  91.5 |  99.2 |  93.2 |  86.4 |  86.1 |  84.3 |  82.3 | 116.0 |  99.9 |  63.0 | 110.3 |  74.6 | 106.8 |  78.4 |  69.1 |  93.8 |  71.0 |  97.8 |  70.0 | 102.2 |  85.3 |  81.4 |  73.7 |  80.2 |  82.3 | 100.4 |  84.3 |  76.8 |  71.2 |  95.0 |  91.5 |  97.5 |  73.9 |  81.6 |  78.6 |  81.7 |  81.2 |  92.7 |  76.9 | 106.5 |  89.4 |  79.8 | 101.5 |   0.0 | 101.1 |  69.2 |  95.3 | 114.1 | 105.0 |  90.5 |  90.7 | 114.6 | 113.8 |  72.7 |  86.9 |  71.9 |  90.1 | 105.3 |  94.1 |  94.7 |  91.0 |  85.4 |  97.5 |  68.5 |  89.7 |  83.6 |  93.0 |  84.2 |  93.3 |  76.4 |  99.4 |  70.4 |  95.4 |  84.1 | 106.3 |  79.1 |  88.6 |  91.5 |  89.6 |  97.8 |  83.7 |  95.8 | 112.2 |  64.7 | 123.1 | 114.9 |  90.4 | 100.6 |  96.2 |  86.5 |  89.2 |  79.7 | 103.3 | 104.5 | 103.8 |  97.1 |\n",
            "| 49  |  85.3 |  70.0 |  64.5 |  70.7 |  77.1 | 111.6 |  64.3 | 114.1 | 111.0 |  82.2 |  87.1 | 102.7 |  63.9 | 104.3 | 117.4 |  58.4 | 102.4 |  57.2 |  90.7 | 102.5 |  91.5 | 105.1 |  97.7 | 109.4 |  72.4 | 106.3 | 103.5 |  86.1 |  88.1 |  91.9 |  68.0 |  71.6 | 109.6 |  93.1 | 106.3 | 112.2 | 100.4 | 110.2 | 108.8 |  92.8 | 110.6 | 131.6 |  81.7 | 110.3 | 101.7 |  59.0 | 105.2 | 103.2 | 101.1 |   0.0 | 104.6 | 122.3 |  52.2 |  55.4 |  87.4 | 105.2 |  54.0 |  60.4 |  92.1 | 119.7 | 107.8 |  68.8 | 102.3 |  68.4 | 105.1 | 119.7 | 107.2 |  56.1 | 134.5 |  69.7 | 107.8 |  55.1 |  76.9 |  76.2 | 107.9 | 111.1 |  98.5 |  99.9 | 114.9 |  50.1 |  89.5 |  95.7 |  92.0 |  80.2 | 108.3 |  79.3 |  90.7 |  68.6 |  99.5 |  54.6 |  67.8 |  94.5 |  83.7 | 118.7 | 116.4 | 115.1 |  98.3 |  55.4 |  74.4 | 102.2 |  81.8 |\n",
            "| 50  |  81.1 |  88.9 |  99.6 |  89.0 |  99.0 |  93.6 |  99.4 |  83.1 |  79.1 | 109.7 |  76.6 |  94.0 | 109.6 |  93.5 |  81.0 | 118.1 |  71.4 | 107.9 |  80.2 |  74.6 |  76.0 |  65.7 |  98.8 |  91.1 | 101.9 |  75.4 |  72.6 |  68.7 |  80.8 | 101.1 | 111.2 | 109.5 |  65.9 |  86.8 |  93.7 |  88.9 |  95.0 |  79.1 |  78.4 |  88.8 |  85.6 |  73.1 |  85.0 |  77.8 |  94.1 |  94.4 |  78.5 |  89.4 |  69.2 | 104.6 |   0.0 |  82.3 | 117.8 | 104.7 | 100.1 |  97.4 | 100.6 |  97.8 |  74.8 |  90.7 |  73.5 | 106.3 | 105.3 | 101.0 |  90.5 |  90.8 |  70.9 | 109.8 |  79.6 |  87.0 |  82.5 |  99.2 |  85.5 |  88.9 |  92.2 |  97.6 |  75.7 |  93.8 |  71.8 | 114.5 |  84.9 |  92.9 |  67.0 |  98.4 | 101.6 | 103.1 |  83.3 |  96.3 |  65.6 | 108.8 | 108.8 |  75.1 |  97.4 |  93.2 |  80.9 |  86.8 |  69.2 | 100.6 |  96.8 |  90.8 |  97.7 |\n",
            "| 51  | 104.6 | 106.9 | 128.0 | 117.3 | 114.5 |  61.6 | 105.1 |  71.2 |  67.1 | 111.4 | 100.4 |  70.4 | 104.8 |  49.7 |  87.8 | 111.1 |  65.9 | 103.1 |  99.3 |  81.3 |  91.6 |  85.7 |  54.9 |  93.0 | 103.3 | 113.4 |  80.2 | 108.4 | 102.3 | 105.6 | 123.6 | 120.0 |  81.7 |  98.5 |  51.6 |  73.6 |  57.8 |  75.2 |  61.8 |  97.9 |  63.1 |  73.3 | 108.2 |  84.8 |  47.5 | 115.7 |  80.3 |  66.8 |  95.3 | 122.3 |  82.3 |   0.0 | 106.8 | 108.2 |  92.8 |  59.2 | 110.4 | 106.7 | 107.1 |  53.0 |  76.7 | 118.9 |  51.2 | 111.8 |  49.5 |  59.4 |  76.6 | 111.9 |  77.6 | 102.5 |  65.4 | 101.8 | 124.5 | 107.7 |  72.9 |  51.2 |  93.3 |  49.8 |  72.5 | 104.8 | 113.7 |  99.6 |  97.5 | 118.6 |  47.6 | 107.3 | 103.6 | 108.1 |  88.1 | 113.9 | 103.5 | 102.1 | 103.0 |  53.0 |  64.1 |  58.7 |  90.7 | 111.8 | 113.2 |  54.7 |  86.9 |\n",
            "| 52  |  98.2 |  77.7 |  77.2 |  67.7 |  82.6 | 106.6 |  56.9 | 118.7 | 109.1 |  84.4 | 100.3 | 110.4 |  45.1 |  93.1 | 117.1 |  52.2 | 117.4 |  45.3 | 104.5 | 111.5 | 103.7 | 116.4 |  97.5 | 102.6 |  69.3 | 100.2 | 110.2 | 101.6 |  99.7 |  87.5 |  56.5 |  67.6 | 113.4 | 109.7 |  99.2 | 115.8 |  86.1 | 108.9 | 112.4 |  97.2 | 103.3 | 123.3 |  89.0 | 113.8 |  89.4 |  65.6 | 112.7 |  96.6 | 114.1 |  52.2 | 117.8 | 106.8 |   0.0 |  43.5 |  92.6 | 105.9 |  49.9 |  57.8 | 109.1 | 113.7 | 119.0 |  74.7 |  88.0 |  58.7 |  96.9 | 104.1 | 108.0 |  61.8 | 111.2 |  76.4 | 119.5 |  58.1 |  87.7 |  82.6 | 109.9 |  92.5 | 104.5 |  91.6 | 111.6 |  43.5 |  86.9 |  84.9 | 120.0 |  72.4 |  95.1 |  84.7 |  87.0 |  59.4 | 105.4 |  46.8 |  45.6 |  87.0 |  90.5 | 112.1 | 107.7 | 108.2 | 104.4 |  58.3 |  67.9 |  96.5 |  76.7 |\n",
            "| 53  |  95.3 |  78.6 |  69.1 |  68.9 |  78.7 | 110.6 |  65.5 | 107.8 | 106.5 |  76.5 |  83.9 | 103.7 |  50.6 | 101.0 | 117.0 |  54.4 | 120.5 |  49.5 | 106.2 | 103.3 | 102.1 | 104.3 |  98.2 |  98.8 |  74.6 | 100.4 | 101.4 |  95.8 |  95.6 |  86.4 |  72.6 |  75.8 | 115.0 | 115.5 |  95.7 | 120.6 |  89.9 | 110.6 | 110.9 |  87.6 | 106.0 | 117.1 |  81.0 | 109.6 |  91.1 |  64.7 | 117.2 | 104.1 | 105.0 |  55.4 | 104.7 | 108.2 |  43.5 |   0.0 |  97.8 | 100.8 |  47.8 |  54.1 | 100.6 | 118.3 | 130.2 |  74.3 |  95.8 |  54.3 |  94.4 | 104.7 | 111.6 |  62.6 | 105.7 |  70.2 | 117.8 |  59.5 |  82.8 |  71.0 | 114.7 |  98.3 | 113.0 |  92.3 | 121.8 |  47.5 |  86.6 |  91.2 | 105.8 |  75.2 | 106.8 |  77.2 |  97.3 |  54.0 |  97.0 |  51.4 |  56.6 |  90.3 |  92.0 | 115.6 | 104.5 | 106.2 | 101.1 |  65.7 |  79.7 |  98.9 |  71.1 |\n",
            "| 54  |  81.4 |  91.5 |  82.2 |  96.4 |  74.9 | 101.6 |  84.7 |  79.0 | 104.2 |  86.0 |  99.0 |  99.0 | 100.7 | 115.3 |  80.3 | 100.2 |  91.7 |  87.4 |  83.5 |  85.8 |  73.3 |  96.6 | 104.7 |  77.7 |  77.5 |  89.2 |  84.8 |  74.4 |  89.1 |  68.5 |  94.7 |  94.6 |  81.0 |  75.4 | 112.7 |  87.6 | 113.2 | 108.6 |  97.0 |  66.8 |  93.8 | 102.9 |  74.7 |  89.7 | 111.0 | 102.1 |  84.9 | 114.1 |  90.5 |  87.4 | 100.1 |  92.8 |  92.6 |  97.8 |   0.0 | 101.7 |  94.6 |  88.8 |  74.8 |  93.6 |  84.6 |  85.8 | 104.1 |  99.6 | 100.3 |  97.9 |  81.3 |  88.8 |  92.7 |  86.7 |  93.9 |  91.5 |  89.0 |  81.5 |  91.2 | 109.7 |  67.9 | 113.9 |  86.4 |  89.7 |  91.8 |  70.7 |  88.4 |  79.4 | 100.3 |  77.5 |  81.2 |  91.7 |  92.2 |  90.1 |  90.2 |  85.8 |  70.6 | 106.7 | 113.3 | 110.7 |  69.6 |  90.5 |  77.7 |  96.1 | 101.1 |\n",
            "| 55  | 108.4 | 119.6 | 107.0 | 120.2 | 111.2 |  52.4 | 114.3 |  70.1 |  61.0 | 119.4 | 116.0 |  54.0 | 105.5 |  46.2 |  89.9 |  96.6 |  72.1 | 107.6 | 108.9 |  77.2 |  91.8 |  89.8 |  46.9 |  77.8 | 109.2 | 117.6 |  88.4 | 103.0 | 107.6 | 113.5 | 110.4 | 106.7 |  99.6 |  88.5 |  43.5 |  70.8 |  46.2 |  71.7 |  67.7 | 112.3 |  55.0 |  71.2 | 121.6 |  71.4 |  47.6 | 117.9 |  72.9 |  48.6 |  90.7 | 105.2 |  97.4 |  59.2 | 105.9 | 100.8 | 101.7 |   0.0 | 110.0 | 109.6 |  96.3 |  58.9 |  83.7 | 109.3 |  45.8 | 108.3 |  46.5 |  58.7 |  84.0 | 102.0 |  89.4 | 123.5 |  57.2 | 103.7 | 112.8 | 115.8 |  69.1 |  48.5 | 105.9 |  43.7 |  84.4 | 100.6 | 118.4 | 110.0 | 107.2 | 118.1 |  45.8 | 110.4 | 126.4 | 104.0 |  88.9 | 106.1 | 103.3 | 124.9 | 117.4 |  48.1 |  51.0 |  47.4 |  98.5 | 101.9 | 116.0 |  54.8 |  80.7 |\n",
            "| 56  |  89.3 |  74.1 |  61.6 |  64.6 |  82.0 | 102.0 |  57.3 | 108.2 | 106.9 |  77.2 |  86.8 | 110.2 |  52.0 | 100.6 | 114.8 |  56.5 | 112.6 |  48.5 | 101.0 | 117.3 |  96.6 | 111.5 | 104.8 | 103.5 |  69.2 |  93.5 | 107.5 |  87.4 |  86.0 |  83.9 |  65.4 |  69.5 | 103.2 | 109.8 | 104.7 | 113.9 |  92.8 | 111.8 | 118.4 |  86.5 | 113.1 | 114.2 |  75.0 | 127.9 |  99.1 |  58.8 | 115.5 | 103.1 | 114.6 |  54.0 | 100.6 | 110.4 |  49.9 |  47.8 |  94.6 | 110.0 |   0.0 |  57.6 | 103.1 | 127.2 | 118.0 |  73.5 |  99.4 |  51.7 | 101.4 | 107.4 | 107.4 |  64.8 | 117.2 |  69.8 | 110.9 |  67.3 |  77.4 |  69.7 | 118.1 | 102.1 |  95.5 |  97.0 | 107.9 |  44.9 |  86.9 |  90.4 |  96.3 |  64.7 | 105.2 |  86.3 |  90.8 |  55.8 | 112.7 |  45.2 |  50.3 |  85.5 |  79.4 | 114.9 | 105.0 | 114.4 | 103.6 |  54.4 |  78.0 |  99.1 |  76.3 |\n",
            "| 57  | 101.9 |  84.6 |  62.0 |  67.1 |  85.7 | 113.4 |  59.8 | 116.9 | 116.2 |  75.2 |  88.7 | 119.7 |  59.7 | 106.7 | 108.0 |  57.5 | 106.7 |  55.3 |  84.6 | 105.0 |  89.4 | 107.0 | 105.2 | 112.3 |  67.2 |  95.9 | 103.1 |  98.2 |  78.3 |  92.6 |  61.7 |  76.9 |  95.5 |  96.9 | 106.7 | 110.4 |  99.8 | 125.6 | 106.1 |  97.7 | 109.8 | 107.8 |  79.2 | 105.9 | 102.8 |  56.2 | 114.8 | 110.1 | 113.8 |  60.4 |  97.8 | 106.7 |  57.8 |  54.1 |  88.8 | 109.6 |  57.6 |   0.0 | 101.1 | 113.1 | 122.5 |  83.1 | 109.5 |  62.8 | 105.4 | 117.3 | 105.4 |  62.6 | 109.6 |  74.5 | 121.0 |  56.8 |  87.9 |  67.6 | 116.8 | 110.2 | 100.0 | 109.3 | 107.3 |  52.6 |  88.5 |  86.3 |  94.7 |  77.4 | 113.1 |  75.2 |  80.6 |  57.5 |  95.7 |  56.4 |  55.8 |  79.1 |  78.2 | 118.2 | 108.9 | 106.6 | 103.1 |  62.4 |  66.7 | 102.3 |  74.8 |\n",
            "| 58  |  68.3 |  92.0 |  81.4 |  97.7 |  80.0 | 103.1 | 101.6 |  83.9 | 100.4 |  90.6 |  79.6 |  99.2 | 108.4 |  98.9 |  86.8 | 109.0 |  79.1 |  93.9 |  92.0 |  72.6 |  85.5 |  74.7 | 110.1 |  80.5 | 100.6 |  69.5 |  68.5 |  67.2 |  97.5 |  70.5 | 100.5 |  84.9 |  85.5 |  77.6 | 103.5 |  79.3 | 107.5 |  91.2 |  88.9 |  87.6 |  98.9 |  80.0 |  92.5 |  91.2 | 115.5 | 104.4 |  80.2 | 101.4 |  72.7 |  92.1 |  74.8 | 107.1 | 109.1 | 100.6 |  74.8 |  96.3 | 103.1 | 101.1 |   0.0 |  86.8 |  84.8 | 100.9 | 105.8 | 100.1 |  99.7 |  94.3 |  91.6 |  96.9 |  84.5 |  87.7 |  91.3 |  95.6 |  77.5 |  82.7 |  84.3 | 106.3 |  82.3 | 114.8 |  82.8 | 103.9 |  73.2 |  90.0 |  68.8 |  84.7 | 102.5 |  86.1 |  86.9 |  96.2 |  75.8 | 102.5 | 104.4 |  91.3 |  82.8 |  99.9 |  95.5 |  94.5 |  70.4 |  91.3 |  76.8 | 110.4 | 107.6 |\n",
            "| 59  | 101.1 | 111.2 | 110.1 | 120.0 | 105.9 |  60.4 | 115.2 |  82.4 |  83.5 | 108.4 | 108.9 |  68.8 | 117.6 |  59.3 |  76.5 | 111.4 |  67.5 | 115.4 |  96.3 |  75.6 |  99.8 |  81.4 |  61.2 |  93.5 | 116.5 | 108.2 |  78.0 | 103.9 | 107.3 | 100.9 | 118.4 | 112.4 |  80.2 |  77.3 |  54.6 |  75.4 |  63.7 |  63.4 |  58.0 | 100.6 |  74.9 |  71.0 |  98.3 |  76.4 |  60.4 | 115.9 |  80.3 |  60.4 |  86.9 | 119.7 |  90.7 |  53.0 | 113.7 | 118.3 |  93.6 |  58.9 | 127.2 | 113.1 |  86.8 |   0.0 |  72.3 | 127.3 |  58.2 | 125.2 |  59.8 |  56.6 |  77.7 | 114.9 |  77.2 | 115.3 |  55.0 | 117.1 | 115.5 | 111.4 |  57.5 |  54.3 |  98.4 |  58.3 |  65.9 | 113.4 | 102.4 | 102.0 |  85.0 | 119.5 |  52.7 |  99.4 |  92.9 | 118.5 |  98.2 | 116.2 | 110.2 | 107.2 | 106.8 |  57.7 |  62.7 |  56.7 |  90.8 | 116.0 | 112.2 |  67.6 |  90.2 |\n",
            "| 60  |  86.0 |  84.4 | 121.7 | 106.7 | 101.5 |  67.1 | 103.7 |  64.2 |  75.2 |  99.7 |  94.8 |  84.5 | 107.9 |  82.7 |  85.2 | 111.3 |  69.7 | 113.2 |  90.2 |  72.5 |  88.8 |  77.5 |  81.2 |  94.6 | 102.7 |  83.8 |  81.1 |  85.9 |  84.9 |  91.7 | 119.2 | 100.7 |  78.5 |  76.9 |  90.3 |  68.4 |  91.8 |  69.6 |  71.1 |  83.5 |  69.6 |  84.1 |  95.8 |  73.5 |  81.8 | 105.2 |  64.5 |  82.4 |  71.9 | 107.8 |  73.5 |  76.7 | 119.0 | 130.2 |  84.6 |  83.7 | 118.0 | 122.5 |  84.8 |  72.3 |   0.0 |  93.7 |  85.6 | 117.2 |  80.7 |  73.8 |  90.1 | 109.7 |  82.5 | 100.9 |  70.1 | 107.5 |  85.3 |  92.7 |  68.6 |  90.3 |  78.0 |  86.2 |  64.6 | 116.1 |  93.2 |  91.3 |  86.0 | 103.2 |  81.4 | 104.8 |  91.7 | 116.6 |  93.6 | 120.8 | 119.9 |  84.6 |  94.2 |  72.3 |  72.0 |  81.3 |  83.9 | 109.3 | 107.6 |  80.0 |  91.4 |\n",
            "| 61  |  88.8 |  76.1 |  74.3 |  70.6 |  76.9 | 111.1 |  65.6 |  96.6 |  97.1 |  64.2 |  69.9 | 109.0 |  65.8 | 111.5 |  91.8 |  70.2 | 101.4 |  71.9 |  78.8 | 122.1 |  97.4 |  92.4 | 105.9 |  96.2 |  80.2 |  73.8 | 116.3 |  90.0 |  77.7 |  82.2 |  63.0 |  66.1 |  98.5 | 108.2 | 119.2 | 101.5 | 109.4 | 101.3 | 109.2 |  72.2 | 107.0 | 115.8 |  91.1 | 101.8 | 103.5 |  76.6 |  98.0 | 115.1 |  90.1 |  68.8 | 106.3 | 118.9 |  74.7 |  74.3 |  85.8 | 109.3 |  73.5 |  83.1 | 100.9 | 127.3 |  93.7 |   0.0 | 108.2 |  65.5 | 121.0 | 114.1 | 108.6 |  68.2 |  99.8 |  68.9 | 111.0 |  66.2 |  72.7 |  76.7 | 103.7 | 114.2 |  85.9 | 109.8 | 116.7 |  69.4 |  71.9 |  79.6 |  96.5 |  66.9 | 118.3 |  72.1 |  92.9 |  76.6 |  97.9 |  67.2 |  76.9 |  80.7 |  74.8 | 106.7 | 116.8 | 107.8 |  93.0 |  69.1 |  75.5 | 104.6 |  93.2 |\n",
            "| 62  | 114.1 | 115.7 | 107.6 | 114.2 | 110.3 |  56.8 | 104.7 |  75.7 |  69.6 | 113.7 | 107.1 |  60.7 |  96.2 |  31.0 |  97.9 |  86.0 |  80.7 |  91.4 | 110.5 |  86.0 | 106.6 | 100.6 |  38.3 |  90.7 | 110.7 | 108.3 |  95.4 | 113.4 | 120.0 | 106.9 | 110.2 | 104.7 | 102.0 | 105.3 |  40.6 |  72.1 |  33.1 |  71.5 |  66.0 | 105.8 |  59.4 |  80.1 | 116.7 |  88.6 |  35.4 | 111.1 |  83.5 |  47.6 | 105.3 | 102.3 | 105.3 |  51.2 |  88.0 |  95.8 | 104.1 |  45.8 |  99.4 | 109.5 | 105.8 |  58.2 |  85.6 | 108.2 |   0.0 | 104.3 |  39.5 |  47.0 |  94.8 | 101.2 |  94.2 | 106.8 |  63.1 | 101.1 | 118.4 | 115.7 |  67.4 |  35.0 | 106.2 |  34.6 |  83.8 |  97.6 | 124.7 | 109.6 | 110.1 | 114.3 |  35.7 | 113.4 | 121.3 | 105.3 | 101.3 |  98.0 |  90.2 | 113.9 | 113.6 |  51.6 |  64.8 |  50.3 |  96.8 |  96.0 | 111.2 |  45.9 |  83.7 |\n",
            "| 63  |  86.2 |  67.5 |  75.4 |  65.7 |  83.8 | 109.5 |  61.0 | 104.7 | 105.2 |  64.5 |  84.3 | 109.1 |  63.0 | 103.1 | 107.0 |  54.6 | 125.7 |  61.9 |  90.0 | 106.1 | 105.6 | 109.7 | 105.7 | 109.2 |  76.7 |  84.4 | 117.2 |  87.2 |  83.3 |  81.5 |  58.2 |  55.5 | 108.9 | 115.9 | 103.5 | 122.9 |  99.6 | 115.4 | 108.9 |  78.5 | 107.2 | 104.7 |  86.9 | 117.2 | 103.2 |  59.7 | 109.6 | 107.8 |  94.1 |  68.4 | 101.0 | 111.8 |  58.7 |  54.3 |  99.6 | 108.3 |  51.7 |  62.8 | 100.1 | 125.2 | 117.2 |  65.5 | 104.3 |   0.0 | 106.9 | 105.7 | 117.2 |  70.3 | 104.6 |  74.8 | 112.3 |  55.1 |  86.4 |  67.2 | 113.9 | 102.9 |  92.7 | 108.2 | 117.4 |  50.0 |  78.1 |  85.3 |  99.1 |  64.3 | 109.4 |  84.4 |  88.0 |  51.2 |  97.6 |  54.4 |  50.9 |  89.4 |  88.1 | 118.5 | 111.6 | 115.2 | 102.8 |  59.6 |  66.0 | 114.1 |  87.8 |\n",
            "| 64  | 107.2 | 106.5 | 108.8 | 110.8 | 113.0 |  52.9 | 114.3 |  70.3 |  76.8 | 112.4 | 100.2 |  58.6 | 105.0 |  40.4 |  96.9 |  91.2 |  83.5 |  96.0 | 116.3 |  80.8 |  97.3 |  99.6 |  40.9 |  91.0 | 112.2 | 113.9 |  86.9 | 102.8 | 124.6 | 116.5 | 116.3 | 118.1 |  95.1 | 100.2 |  39.2 |  70.5 |  39.9 |  65.6 |  71.8 | 110.1 |  57.9 |  82.9 | 112.2 |  79.1 |  42.4 | 107.6 |  73.6 |  47.9 |  94.7 | 105.1 |  90.5 |  49.5 |  96.9 |  94.4 | 100.3 |  46.5 | 101.4 | 105.4 |  99.7 |  59.8 |  80.7 | 121.0 |  39.5 | 106.9 |   0.0 |  50.0 |  89.1 | 107.9 |  90.0 | 105.3 |  68.1 | 100.0 | 121.3 | 106.7 |  74.0 |  47.8 |  96.5 |  36.3 |  89.1 |  97.4 | 124.7 | 113.6 | 108.0 | 114.0 |  44.3 | 111.3 | 111.5 | 101.3 |  92.0 | 106.6 |  96.1 | 106.7 | 107.7 |  49.2 |  57.5 |  58.2 |  92.9 |  98.0 | 118.2 |  48.0 |  79.4 |\n",
            "| 65  |  98.9 | 110.0 | 112.2 | 113.0 | 112.5 |  54.3 | 115.8 |  64.9 |  70.0 | 104.3 | 111.6 |  57.2 | 113.9 |  56.9 |  84.9 |  98.5 |  82.0 | 102.9 | 100.4 |  75.1 | 104.4 |  99.0 |  56.7 |  88.0 | 117.2 |  92.1 |  80.8 |  95.1 | 104.9 | 101.9 | 116.3 | 112.5 | 107.3 |  89.4 |  53.2 |  66.4 |  52.1 |  66.8 |  67.0 | 101.3 |  56.2 |  67.3 | 104.5 |  71.2 |  48.3 | 117.5 |  70.5 |  56.7 |  91.0 | 119.7 |  90.8 |  59.4 | 104.1 | 104.7 |  97.9 |  58.7 | 107.4 | 117.3 |  94.3 |  56.6 |  73.8 | 114.1 |  47.0 | 105.7 |  50.0 |   0.0 |  87.0 | 125.6 |  89.8 | 103.1 |  64.6 | 122.8 | 100.4 | 108.5 |  68.2 |  49.5 |  97.4 |  57.9 |  73.6 | 119.6 | 106.2 | 107.0 |  93.8 | 103.6 |  47.1 | 109.5 | 112.4 | 108.2 |  99.0 | 108.4 | 103.0 |  99.6 | 114.9 |  52.1 |  59.1 |  55.5 | 103.2 | 106.8 | 106.9 |  60.7 |  86.0 |\n",
            "| 66  |  74.7 |  99.2 | 100.2 |  93.8 |  91.6 |  89.9 | 106.1 |  83.0 |  78.5 | 107.7 | 100.1 |  74.9 | 111.3 |  91.6 |  75.7 | 114.6 |  72.6 | 106.9 |  79.3 |  86.6 |  63.6 |  67.1 |  93.1 |  67.8 |  92.7 |  99.2 |  85.7 |  83.7 |  78.8 |  90.4 |  98.0 | 126.3 |  73.7 |  72.2 |  87.9 |  84.4 |  89.4 |  81.7 |  87.0 | 102.7 |  89.8 |  79.0 |  81.7 |  69.2 |  88.0 | 115.3 |  72.8 |  79.0 |  85.4 | 107.2 |  70.9 |  76.6 | 108.0 | 111.6 |  81.3 |  84.0 | 107.4 | 105.4 |  91.6 |  77.7 |  90.1 | 108.6 |  94.8 | 117.2 |  89.1 |  87.0 |   0.0 | 100.5 |  84.6 | 102.3 |  72.6 | 116.9 |  97.4 | 113.1 |  83.7 |  81.9 |  91.1 |  84.3 |  72.9 | 116.5 |  93.2 |  86.5 |  79.8 |  95.5 |  83.8 | 103.3 |  83.0 | 105.5 |  72.9 | 114.8 | 104.0 |  84.7 |  85.5 |  81.5 |  83.5 |  88.9 |  76.1 | 107.4 | 104.1 |  86.2 |  93.7 |\n",
            "| 67  |  98.5 |  83.5 |  66.6 |  83.4 |  79.9 | 117.4 |  56.5 | 120.4 | 103.4 |  70.6 |  77.3 | 107.5 |  54.5 | 101.4 | 109.0 |  54.2 | 107.9 |  58.1 |  87.5 |  96.9 |  87.6 |  88.1 | 103.9 |  94.0 |  70.6 |  98.7 | 109.8 |  95.6 |  85.6 |  73.7 |  72.2 |  62.4 |  99.9 | 104.8 | 110.7 | 108.1 | 100.6 | 122.2 | 118.3 |  86.0 | 119.8 | 116.1 |  87.6 | 107.5 | 109.7 |  59.4 | 118.9 | 115.4 |  97.5 |  56.1 | 109.8 | 111.9 |  61.8 |  62.6 |  88.8 | 102.0 |  64.8 |  62.6 |  96.9 | 114.9 | 109.7 |  68.2 | 101.2 |  70.3 | 107.9 | 125.6 | 100.5 |   0.0 | 104.4 |  83.1 | 111.3 |  60.8 |  82.7 |  62.3 | 117.7 | 102.1 | 111.1 | 101.1 | 108.1 |  54.8 |  90.8 |  93.3 | 109.6 |  71.9 | 110.9 |  84.1 |  93.9 |  61.6 |  96.2 |  70.5 |  64.9 |  83.5 |  81.5 | 111.9 | 109.7 | 113.7 |  91.2 |  55.0 |  82.7 | 105.4 |  86.9 |\n",
            "| 68  |  91.4 |  99.9 | 110.8 |  95.3 |  88.2 |  81.1 |  98.8 |  84.9 |  76.9 |  88.0 |  79.2 |  82.6 | 103.6 |  92.0 |  61.8 | 121.8 |  86.6 | 114.3 |  91.3 |  77.5 |  90.7 |  79.5 |  98.6 |  65.5 |  94.8 |  72.3 |  71.3 |  96.5 |  93.5 |  88.4 | 101.1 |  93.8 |  64.5 |  91.4 |  89.9 |  83.8 |  95.9 |  68.4 |  90.1 |  74.8 |  75.3 |  66.8 |  95.1 |  77.4 |  99.7 | 100.5 |  92.5 |  98.9 |  68.5 | 134.5 |  79.6 |  77.6 | 111.2 | 105.7 |  92.7 |  89.4 | 117.2 | 109.6 |  84.5 |  77.2 |  82.5 |  99.8 |  94.2 | 104.6 |  90.0 |  89.8 |  84.6 | 104.4 |   0.0 | 106.0 |  90.7 | 107.1 |  93.5 | 100.7 |  77.2 |  90.0 |  78.3 |  92.3 |  70.3 | 111.1 |  80.4 |  79.6 |  93.2 |  91.9 |  90.4 |  86.5 |  81.6 | 101.7 |  69.8 | 114.1 | 107.8 |  76.7 |  96.7 |  82.5 |  83.6 |  81.6 |  75.0 | 114.3 | 108.6 |  92.7 |  98.8 |\n",
            "| 69  |  85.5 |  61.8 |  69.7 |  70.2 |  71.8 | 130.6 |  83.3 |  97.4 | 108.1 |  72.1 |  63.7 | 105.4 |  74.7 | 111.0 |  96.9 |  81.9 | 100.4 |  67.1 |  83.8 | 111.3 |  82.7 |  89.2 | 101.3 | 111.2 |  83.6 |  77.2 |  88.6 |  81.0 |  80.9 |  68.3 |  76.0 |  84.7 | 102.6 |  97.6 | 108.9 | 100.6 | 101.9 | 102.6 |  99.3 |  83.4 | 102.0 | 101.2 |  80.0 | 108.9 | 103.4 |  65.9 | 112.6 | 114.9 |  89.7 |  69.7 |  87.0 | 102.5 |  76.4 |  70.2 |  86.7 | 123.5 |  69.8 |  74.5 |  87.7 | 115.3 | 100.9 |  68.9 | 106.8 |  74.8 | 105.3 | 103.1 | 102.3 |  83.1 | 106.0 |   0.0 | 121.3 |  75.0 |  82.8 |  69.7 | 111.4 | 115.4 |  77.6 | 103.7 | 109.5 |  78.0 |  77.0 |  87.4 |  86.8 |  74.1 | 111.4 |  67.1 |  92.4 |  85.0 |  94.3 |  77.6 |  87.5 |  74.1 |  67.8 | 105.1 | 115.2 | 117.7 |  87.0 |  69.7 |  65.8 | 113.2 |  89.8 |\n",
            "| 70  |  89.7 | 108.6 | 104.8 | 106.5 | 102.8 |  61.5 | 108.1 |  70.3 |  75.3 | 112.7 | 107.8 |  65.7 | 113.7 |  62.0 |  82.1 | 107.4 |  72.9 | 116.6 |  96.0 |  71.0 |  88.7 |  82.7 |  59.5 |  82.4 | 123.9 | 105.6 |  91.5 |  96.3 |  94.1 | 104.1 | 120.2 | 108.9 |  80.7 |  79.8 |  60.9 |  80.0 |  70.5 |  77.6 |  58.6 |  98.6 |  75.4 |  70.4 |  94.2 |  72.0 |  69.2 | 115.6 |  75.9 |  59.4 |  83.6 | 107.8 |  82.5 |  65.4 | 119.5 | 117.8 |  93.9 |  57.2 | 110.9 | 121.0 |  91.3 |  55.0 |  70.1 | 111.0 |  63.1 | 112.3 |  68.1 |  64.6 |  72.6 | 111.3 |  90.7 | 121.3 |   0.0 | 107.0 | 107.5 | 109.2 |  63.5 |  64.1 |  96.9 |  60.7 |  69.6 | 112.5 | 103.4 | 100.0 |  83.8 | 108.9 |  55.5 | 121.9 |  95.0 | 118.4 |  96.4 | 107.0 | 107.0 | 121.8 | 110.5 |  72.9 |  66.4 |  61.2 |  79.5 | 110.6 | 123.2 |  62.2 |  91.7 |\n",
            "| 71  |  95.8 |  72.0 |  80.6 |  66.5 |  94.6 | 113.4 |  59.1 | 113.2 | 115.9 |  83.0 |  76.9 | 120.6 |  56.4 |  96.0 | 108.3 |  61.5 | 102.2 |  61.3 |  95.8 | 103.5 |  96.3 |  97.3 |  97.7 | 110.2 |  73.0 | 103.2 | 120.2 | 100.6 |  88.3 |  95.7 |  71.8 |  62.7 |  97.8 | 112.8 |  99.0 | 114.7 |  98.4 | 120.2 | 106.2 |  96.5 | 107.2 | 121.3 |  97.9 | 107.9 | 100.5 |  59.7 | 104.1 | 107.4 |  93.0 |  55.1 |  99.2 | 101.8 |  58.1 |  59.5 |  91.5 | 103.7 |  67.3 |  56.8 |  95.6 | 117.1 | 107.5 |  66.2 | 101.1 |  55.1 | 100.0 | 122.8 | 116.9 |  60.8 | 107.1 |  75.0 | 107.0 |   0.0 |  89.1 |  65.5 | 111.4 | 107.8 |  91.1 |  98.9 | 120.3 |  49.7 |  82.3 |  82.9 | 109.8 |  81.6 | 103.5 |  82.5 |  84.5 |  66.6 |  92.5 |  59.1 |  54.6 |  92.7 |  89.7 | 113.3 | 115.0 | 102.4 |  93.7 |  57.9 |  71.2 | 101.1 |  90.6 |\n",
            "| 72  |  73.5 |  71.3 |  82.5 |  77.3 |  77.3 | 101.0 |  77.4 |  96.9 | 100.5 |  82.6 |  83.6 | 105.7 |  84.3 | 126.3 |  89.2 |  84.8 |  92.0 |  87.4 |  88.2 |  87.7 |  86.7 |  78.5 | 125.3 |  81.0 |  68.2 |  72.3 |  88.6 |  77.1 |  67.4 |  78.7 |  88.2 |  73.1 |  95.9 |  83.1 | 117.4 |  93.6 | 115.8 |  96.2 | 113.7 |  80.8 | 104.0 |  97.6 |  79.0 |  87.2 | 111.7 |  72.9 |  90.7 | 108.8 |  84.2 |  76.9 |  85.5 | 124.5 |  87.7 |  82.8 |  89.0 | 112.8 |  77.4 |  87.9 |  77.5 | 115.5 |  85.3 |  72.7 | 118.4 |  86.4 | 121.3 | 100.4 |  97.4 |  82.7 |  93.5 |  82.8 | 107.5 |  89.1 |   0.0 |  75.1 | 109.1 | 118.0 |  87.3 | 118.4 |  85.6 |  97.1 |  66.5 |  75.5 |  78.8 |  86.1 | 113.9 |  68.9 |  87.7 |  76.6 |  92.9 |  81.4 |  89.6 |  70.6 |  93.0 | 107.6 |  97.6 | 107.6 | 103.0 |  86.7 |  85.7 | 113.3 |  99.5 |\n",
            "| 73  | 101.8 |  71.9 |  70.1 |  88.6 |  87.5 | 116.3 |  78.9 | 100.4 | 112.8 |  68.0 |  71.2 | 122.1 |  66.4 | 115.5 |  94.7 |  70.2 | 110.6 |  77.7 |  89.6 |  87.3 |  88.8 |  93.3 | 106.3 | 111.1 |  72.8 |  77.5 |  99.3 |  77.0 |  86.8 |  72.1 |  93.4 |  67.8 | 100.5 |  96.1 | 107.3 | 103.3 | 114.5 | 122.7 | 115.2 |  75.5 | 115.5 | 106.0 |  77.7 | 101.5 | 116.6 |  61.8 | 104.2 | 119.9 |  93.3 |  76.2 |  88.9 | 107.7 |  82.6 |  71.0 |  81.5 | 115.8 |  69.7 |  67.6 |  82.7 | 111.4 |  92.7 |  76.7 | 115.7 |  67.2 | 106.7 | 108.5 | 113.1 |  62.3 | 100.7 |  69.7 | 109.2 |  65.5 |  75.1 |   0.0 | 115.9 | 113.2 |  89.2 | 113.4 | 100.5 |  72.4 |  75.2 |  89.6 |  91.0 |  70.5 | 116.1 |  89.3 |  82.6 |  66.2 |  98.4 |  69.4 |  75.6 |  73.6 |  81.7 | 109.1 | 106.9 | 118.2 |  88.9 |  65.0 |  69.6 | 104.8 |  90.5 |\n",
            "| 74  |  95.4 | 110.4 | 115.9 | 104.2 | 108.6 |  57.0 | 118.6 |  75.0 |  73.1 |  99.7 | 110.3 |  71.9 | 113.8 |  65.4 |  68.8 | 113.4 |  60.6 | 113.5 |  92.6 |  77.5 | 116.2 |  96.0 |  64.0 |  83.7 | 105.2 |  93.9 |  85.3 | 101.3 |  94.1 |  96.1 | 112.3 | 109.8 |  86.7 |  70.4 |  63.3 |  67.2 |  71.5 |  57.1 |  60.1 |  90.6 |  62.5 |  84.9 |  94.8 |  82.5 |  70.6 | 123.3 |  63.4 |  71.2 |  76.4 | 107.9 |  92.2 |  72.9 | 109.9 | 114.7 |  91.2 |  69.1 | 118.1 | 116.8 |  84.3 |  57.5 |  68.6 | 103.7 |  67.4 | 113.9 |  74.0 |  68.2 |  83.7 | 117.7 |  77.2 | 111.4 |  63.5 | 111.4 | 109.1 | 115.9 |   0.0 |  68.1 |  93.3 |  71.3 |  63.6 | 110.7 |  99.9 |  94.5 |  83.6 | 102.8 |  66.7 | 110.6 | 100.3 | 142.4 |  82.6 | 115.8 | 116.2 | 104.6 |  97.7 |  74.4 |  73.5 |  60.6 |  86.4 | 123.4 | 108.2 |  60.1 |  90.3 |\n",
            "| 75  | 115.0 | 110.6 | 110.6 | 122.0 | 113.7 |  59.6 | 108.9 |  84.8 |  72.8 | 112.2 | 108.9 |  64.8 | 100.9 |  37.4 |  86.0 |  89.9 |  79.2 |  96.6 | 108.8 |  84.3 | 105.1 |  97.8 |  40.5 |  86.6 | 108.5 | 106.2 |  93.1 |  99.2 | 115.0 | 105.2 | 107.7 | 106.7 |  96.9 |  97.3 |  38.2 |  70.3 |  35.7 |  66.1 |  68.7 | 102.9 |  74.5 |  83.0 | 119.1 |  85.7 |  38.0 | 114.1 |  72.0 |  45.0 |  99.4 | 111.1 |  97.6 |  51.2 |  92.5 |  98.3 | 109.7 |  48.5 | 102.1 | 110.2 | 106.3 |  54.3 |  90.3 | 114.2 |  35.0 | 102.9 |  47.8 |  49.5 |  81.9 | 102.1 |  90.0 | 115.4 |  64.1 | 107.8 | 118.0 | 113.2 |  68.1 |   0.0 | 105.2 |  39.7 |  78.4 | 100.5 | 121.9 |  96.5 | 103.2 | 116.5 |  40.0 | 117.0 | 108.8 | 103.5 |  95.0 | 106.2 |  94.0 | 119.5 | 120.3 |  50.0 |  50.0 |  47.9 | 101.1 | 104.9 | 107.1 |  48.2 |  78.9 |\n",
            "| 76  |  92.1 |  74.9 |  83.4 |  82.1 |  87.6 |  94.0 |  94.1 |  83.9 | 100.5 |  89.6 |  80.1 |  95.2 | 115.4 | 108.6 |  60.5 | 113.3 |  84.1 | 100.4 |  75.0 |  95.2 |  76.1 |  96.8 |  98.2 |  85.2 |  81.9 |  70.9 |  84.7 |  63.1 |  92.9 |  89.5 |  84.3 |  82.2 |  62.9 |  74.6 | 104.8 |  88.2 | 103.9 |  83.2 |  93.5 |  69.9 |  88.5 |  88.7 |  90.6 |  91.6 | 109.6 |  84.1 |  78.0 |  98.6 |  70.4 |  98.5 |  75.7 |  93.3 | 104.5 | 113.0 |  67.9 | 105.9 |  95.5 | 100.0 |  82.3 |  98.4 |  78.0 |  85.9 | 106.2 |  92.7 |  96.5 |  97.4 |  91.1 | 111.1 |  78.3 |  77.6 |  96.9 |  91.1 |  87.3 |  89.2 |  93.3 | 105.2 |   0.0 | 102.9 |  91.0 |  99.6 |  88.5 |  71.5 |  83.1 |  90.5 |  96.5 |  77.9 |  77.4 |  96.1 |  82.5 |  97.7 |  96.5 |  84.7 |  84.9 |  89.9 | 101.1 |  99.5 |  81.7 |  92.0 |  80.9 |  97.3 | 100.4 |\n",
            "| 77  | 116.3 | 106.0 | 105.7 | 104.5 | 111.7 |  55.1 | 110.7 |  81.2 |  70.7 | 116.4 | 104.8 |  59.4 |  95.9 |  38.1 |  93.5 |  93.6 |  76.1 |  98.2 | 110.5 |  89.9 |  98.8 |  93.0 |  36.2 |  90.2 | 108.8 | 122.0 |  94.5 | 111.5 | 115.4 | 115.9 | 110.0 | 109.5 |  96.3 |  97.7 |  36.1 |  80.9 |  33.5 |  64.8 |  67.3 | 112.1 |  63.2 |  83.0 | 110.4 |  83.1 |  34.2 | 104.0 |  84.7 |  42.2 |  95.4 |  99.9 |  93.8 |  49.8 |  91.6 |  92.3 | 113.9 |  43.7 |  97.0 | 109.3 | 114.8 |  58.3 |  86.2 | 109.8 |  34.6 | 108.2 |  36.3 |  57.9 |  84.3 | 101.1 |  92.3 | 103.7 |  60.7 |  98.9 | 118.4 | 113.4 |  71.3 |  39.7 | 102.9 |   0.0 |  88.6 |  95.0 | 117.1 | 113.5 | 111.8 | 117.6 |  37.7 | 111.5 | 117.3 | 107.4 |  96.1 |  99.9 |  97.7 | 115.4 | 114.3 |  48.5 |  54.0 |  52.8 |  99.6 | 100.3 | 122.1 |  42.7 |  81.4 |\n",
            "| 78  |  88.0 | 105.0 | 118.0 | 117.8 |  97.5 |  74.8 | 102.3 |  90.3 |  74.6 | 112.9 | 112.2 |  85.6 | 107.8 |  82.3 |  66.1 | 116.1 |  65.4 | 117.8 |  99.9 |  70.9 |  92.0 |  90.2 |  86.3 |  83.7 | 100.0 |  79.9 |  70.4 |  82.8 |  81.7 |  89.6 | 119.2 | 112.5 |  77.7 |  72.3 |  78.3 |  64.1 |  83.1 |  67.0 |  79.0 |  86.3 |  71.2 |  73.6 |  94.2 |  79.6 |  85.9 | 104.9 |  80.3 |  85.1 |  84.1 | 114.9 |  71.8 |  72.5 | 111.6 | 121.8 |  86.4 |  84.4 | 107.9 | 107.3 |  82.8 |  65.9 |  64.6 | 116.7 |  83.8 | 117.4 |  89.1 |  73.6 |  72.9 | 108.1 |  70.3 | 109.5 |  69.6 | 120.3 |  85.6 | 100.5 |  63.6 |  78.4 |  91.0 |  88.6 |   0.0 | 115.9 |  93.3 |  82.1 |  80.7 | 102.4 |  75.6 | 111.4 |  87.7 | 117.3 |  84.1 | 111.0 | 116.1 |  83.4 | 106.0 |  85.3 |  71.5 |  81.8 |  74.3 | 109.8 | 105.2 |  76.6 |  92.9 |\n",
            "| 79  |  93.3 |  76.2 |  69.8 |  73.4 |  81.0 | 104.3 |  54.6 | 115.7 | 109.2 |  73.9 |  92.1 | 115.6 |  51.6 |  94.1 | 116.3 |  53.0 | 109.9 |  50.8 | 105.3 | 120.2 | 106.2 | 112.3 |  97.4 | 110.6 |  75.8 | 105.7 | 111.3 |  92.7 |  99.6 |  86.5 |  59.8 |  64.1 | 100.5 | 112.1 |  99.5 | 114.9 |  91.8 | 110.6 | 105.8 |  85.5 | 113.8 | 122.9 |  88.1 | 125.7 |  96.1 |  58.0 | 117.9 | 102.5 | 106.3 |  50.1 | 114.5 | 104.8 |  43.5 |  47.5 |  89.7 | 100.6 |  44.9 |  52.6 | 103.9 | 113.4 | 116.1 |  69.4 |  97.6 |  50.0 |  97.4 | 119.6 | 116.5 |  54.8 | 111.1 |  78.0 | 112.5 |  49.7 |  97.1 |  72.4 | 110.7 | 100.5 |  99.6 |  95.0 | 115.9 |   0.0 |  89.2 |  89.8 | 111.4 |  74.3 | 102.3 |  79.6 |  88.4 |  55.7 | 104.4 |  46.4 |  52.4 |  96.9 |  79.9 | 116.3 | 106.0 | 115.0 |  96.4 |  58.5 |  77.0 |  99.5 |  72.1 |\n",
            "| 80  |  74.5 |  70.3 |  84.2 |  62.2 |  69.5 | 105.9 |  85.8 | 104.5 | 108.3 |  69.6 |  80.5 | 116.8 |  77.5 | 115.6 |  90.0 |  98.2 |  95.8 |  90.8 |  76.7 |  94.6 |  88.4 |  84.7 | 128.4 |  89.7 |  84.5 |  71.8 |  87.0 |  94.7 |  79.4 |  79.5 |  72.3 |  66.1 |  88.1 |  80.3 | 119.4 | 113.8 | 128.5 |  98.3 | 105.1 |  87.8 | 103.8 |  85.4 |  79.7 |  99.0 | 117.9 |  88.6 |  99.9 | 112.0 |  79.1 |  89.5 |  84.9 | 113.7 |  86.9 |  86.6 |  91.8 | 118.4 |  86.9 |  88.5 |  73.2 | 102.4 |  93.2 |  71.9 | 124.7 |  78.1 | 124.7 | 106.2 |  93.2 |  90.8 |  80.4 |  77.0 | 103.4 |  82.3 |  66.5 |  75.2 |  99.9 | 121.9 |  88.5 | 117.1 |  93.3 |  89.2 |   0.0 |  90.4 |  78.7 |  66.6 | 113.7 |  71.9 |  69.5 |  82.8 |  81.9 |  79.2 |  95.3 |  73.1 |  71.3 | 113.0 | 104.2 | 112.2 |  94.7 |  83.7 |  71.0 | 116.5 | 116.5 |\n",
            "| 81  |  84.3 |  72.4 |  92.5 |  88.7 |  75.4 | 110.9 |  78.7 |  97.0 | 111.0 |  95.2 |  91.4 | 113.6 |  83.0 | 113.5 |  71.2 |  90.2 | 100.1 |  90.0 | 102.3 |  96.3 |  78.7 |  88.5 | 103.2 |  76.8 |  74.4 |  76.7 |  96.4 |  75.5 |  71.1 |  69.3 |  79.4 |  87.5 |  75.6 |  86.3 | 105.4 |  87.1 | 103.9 |  93.4 |  98.5 |  72.7 | 105.5 | 106.7 |  88.3 |  91.6 | 104.8 |  88.5 |  81.0 | 104.3 |  88.6 |  95.7 |  92.9 |  99.6 |  84.9 |  91.2 |  70.7 | 110.0 |  90.4 |  86.3 |  90.0 | 102.0 |  91.3 |  79.6 | 109.6 |  85.3 | 113.6 | 107.0 |  86.5 |  93.3 |  79.6 |  87.4 | 100.0 |  82.9 |  75.5 |  89.6 |  94.5 |  96.5 |  71.5 | 113.5 |  82.1 |  89.8 |  90.4 |   0.0 |  91.3 |  90.6 | 104.7 |  76.7 |  65.9 |  92.2 |  95.6 |  91.7 |  83.8 |  96.0 |  89.9 | 108.4 |  92.9 |  98.4 |  78.6 | 100.4 |  71.5 | 104.6 |  86.7 |\n",
            "| 82  |  66.4 |  83.4 |  85.4 |  93.7 |  78.9 |  91.6 | 101.4 |  82.1 |  94.6 |  81.2 |  69.9 |  87.3 | 112.5 | 106.5 |  84.3 | 107.3 |  79.6 | 105.9 |  64.9 |  83.1 |  83.5 |  79.2 | 104.3 |  98.5 |  91.6 |  75.8 |  69.9 |  75.2 |  76.2 |  92.2 | 100.1 | 102.7 |  73.1 |  75.3 | 101.6 |  89.7 | 116.7 |  82.2 |  86.1 |  74.1 | 105.8 |  83.9 |  68.2 |  95.7 | 103.2 |  99.6 |  78.2 |  97.8 |  91.5 |  92.0 |  67.0 |  97.5 | 120.0 | 105.8 |  88.4 | 107.2 |  96.3 |  94.7 |  68.8 |  85.0 |  86.0 |  96.5 | 110.1 |  99.1 | 108.0 |  93.8 |  79.8 | 109.6 |  93.2 |  86.8 |  83.8 | 109.8 |  78.8 |  91.0 |  83.6 | 103.2 |  83.1 | 111.8 |  80.7 | 111.4 |  78.7 |  91.3 |   0.0 |  96.2 | 110.8 |  80.2 |  69.7 |  96.2 |  84.8 |  99.7 | 109.2 |  83.3 |  74.0 |  97.1 | 100.5 |  94.6 |  87.6 | 103.8 |  86.4 | 101.5 | 100.9 |\n",
            "| 83  |  79.0 |  90.6 |  68.0 |  65.2 |  82.7 | 108.7 |  63.6 | 103.7 |  99.6 |  62.5 |  81.4 | 103.0 |  67.0 | 107.9 |  89.9 |  79.8 | 116.9 |  69.6 |  80.1 |  99.2 |  91.4 | 104.0 | 115.6 |  88.5 |  91.1 |  73.1 |  93.3 |  90.0 |  74.8 |  73.3 |  59.3 |  72.3 | 104.2 |  97.4 | 123.3 | 101.5 | 109.7 | 110.6 | 118.1 |  88.3 | 104.2 |  98.4 |  69.7 |  97.4 | 123.8 |  80.2 | 108.5 | 126.9 |  89.6 |  80.2 |  98.4 | 118.6 |  72.4 |  75.2 |  79.4 | 118.1 |  64.7 |  77.4 |  84.7 | 119.5 | 103.2 |  66.9 | 114.3 |  64.3 | 114.0 | 103.6 |  95.5 |  71.9 |  91.9 |  74.1 | 108.9 |  81.6 |  86.1 |  70.5 | 102.8 | 116.5 |  90.5 | 117.6 | 102.4 |  74.3 |  66.6 |  90.6 |  96.2 |   0.0 | 114.8 |  90.4 |  73.9 |  78.7 | 104.7 |  68.9 |  70.0 |  71.9 |  66.5 | 115.7 | 115.6 | 111.1 |  85.2 |  60.7 |  66.7 | 113.4 |  99.6 |\n",
            "| 84  | 107.3 | 108.7 | 114.9 | 112.3 | 111.9 |  52.6 | 109.4 |  74.8 |  69.4 | 112.7 | 119.1 |  59.5 | 108.1 |  38.6 |  89.4 | 104.0 |  73.3 | 101.4 | 110.1 |  82.0 |  98.5 | 102.6 |  40.6 |  90.2 | 111.5 | 113.5 |  88.0 | 105.6 | 113.7 | 108.4 | 109.3 | 105.5 | 103.2 |  90.0 |  40.8 |  66.4 |  40.6 |  72.7 |  61.0 | 113.9 |  59.9 |  74.6 | 119.2 |  74.7 |  42.8 | 110.3 |  78.4 |  45.2 |  97.8 | 108.3 | 101.6 |  47.6 |  95.1 | 106.8 | 100.3 |  45.8 | 105.2 | 113.1 | 102.5 |  52.7 |  81.4 | 118.3 |  35.7 | 109.4 |  44.3 |  47.1 |  83.8 | 110.9 |  90.4 | 111.4 |  55.5 | 103.5 | 113.9 | 116.1 |  66.7 |  40.0 |  96.5 |  37.7 |  75.6 | 102.3 | 113.7 | 104.7 | 110.8 | 114.8 |   0.0 | 117.3 | 110.6 | 109.1 |  95.6 | 100.8 |  99.0 | 120.7 | 120.1 |  46.1 |  54.8 |  52.9 | 101.5 | 106.1 | 112.6 |  48.7 |  89.2 |\n",
            "| 85  |  77.2 |  72.8 |  70.7 |  72.6 |  66.5 | 106.9 |  78.3 | 106.0 | 113.9 |  70.9 |  73.7 | 102.5 |  86.1 | 122.0 |  89.1 |  82.7 |  96.0 |  82.2 |  78.7 | 107.0 |  84.5 |  80.7 | 118.7 |  84.8 |  71.1 |  91.8 |  84.5 |  90.6 |  80.1 |  77.7 |  69.3 |  74.9 |  79.5 |  86.2 | 113.7 | 105.7 | 114.1 |  94.5 | 102.6 |  75.3 | 106.1 |  97.7 |  75.5 | 103.9 | 107.4 |  72.9 | 106.9 | 113.3 |  83.7 |  79.3 | 103.1 | 107.3 |  84.7 |  77.2 |  77.5 | 110.4 |  86.3 |  75.2 |  86.1 |  99.4 | 104.8 |  72.1 | 113.4 |  84.4 | 111.3 | 109.5 | 103.3 |  84.1 |  86.5 |  67.1 | 121.9 |  82.5 |  68.9 |  89.3 | 110.6 | 117.0 |  77.9 | 111.5 | 111.4 |  79.6 |  71.9 |  76.7 |  80.2 |  90.4 | 117.3 |   0.0 |  82.5 |  77.8 |  92.7 |  84.1 |  88.7 |  79.7 |  66.8 | 105.6 | 109.5 | 107.9 |  99.4 |  87.6 |  77.4 | 120.6 |  97.5 |\n",
            "| 86  |  72.5 |  75.8 |  86.0 |  77.9 |  74.5 | 103.4 |  77.2 | 108.4 | 116.9 |  80.9 |  77.9 | 110.5 |  85.6 | 110.3 |  81.9 |  94.4 | 108.1 |  96.7 |  81.8 |  91.2 |  80.0 |  95.0 | 110.4 |  96.0 |  89.5 |  78.7 |  82.0 |  85.3 |  82.4 |  93.5 |  72.6 |  89.3 |  69.6 |  83.5 | 112.0 |  98.5 | 121.1 |  93.0 | 102.9 |  85.7 | 117.8 | 101.6 |  67.8 |  88.7 | 117.2 |  82.8 |  90.5 | 105.7 |  95.8 |  90.7 |  83.3 | 103.6 |  87.0 |  97.3 |  81.2 | 126.4 |  90.8 |  80.6 |  86.9 |  92.9 |  91.7 |  92.9 | 121.3 |  88.0 | 111.5 | 112.4 |  83.0 |  93.9 |  81.6 |  92.4 |  95.0 |  84.5 |  87.7 |  82.6 | 100.3 | 108.8 |  77.4 | 117.3 |  87.7 |  88.4 |  69.5 |  65.9 |  69.7 |  73.9 | 110.6 |  82.5 |   0.0 |  84.5 |  96.3 |  82.1 |  84.5 |  78.8 |  72.3 | 112.4 | 104.1 | 106.8 |  78.1 |  91.6 |  71.4 | 107.3 | 102.8 |\n",
            "| 87  |  89.4 |  73.0 |  70.0 |  78.2 |  74.5 | 108.7 |  56.8 | 101.3 | 104.1 |  72.0 |  79.1 | 107.3 |  66.5 | 107.2 | 120.5 |  61.0 | 124.4 |  58.6 |  91.1 | 103.1 |  85.3 |  99.6 | 113.0 | 105.5 |  70.9 |  87.4 | 102.0 |  83.2 |  99.3 |  90.4 |  66.1 |  64.3 |  99.2 | 118.9 | 111.4 | 119.6 | 104.5 | 119.8 | 118.2 |  81.3 | 116.1 | 103.3 |  89.9 | 105.6 | 101.0 |  61.3 | 112.1 | 103.5 | 112.2 |  68.6 |  96.3 | 108.1 |  59.4 |  54.0 |  91.7 | 104.0 |  55.8 |  57.5 |  96.2 | 118.5 | 116.6 |  76.6 | 105.3 |  51.2 | 101.3 | 108.2 | 105.5 |  61.6 | 101.7 |  85.0 | 118.4 |  66.6 |  76.6 |  66.2 | 142.4 | 103.5 |  96.1 | 107.4 | 117.3 |  55.7 |  82.8 |  92.2 |  96.2 |  78.7 | 109.1 |  77.8 |  84.5 |   0.0 |  95.5 |  53.1 |  51.3 |  79.7 |  90.2 | 107.1 | 107.3 | 117.9 | 103.1 |  57.6 |  74.9 | 111.9 |  84.6 |\n",
            "| 88  |  84.1 |  90.8 | 103.9 |  88.7 |  88.5 |  94.8 | 113.1 |  85.7 |  74.4 |  91.9 |  83.4 |  86.0 | 111.5 |  93.8 |  82.5 | 112.2 |  72.7 | 105.3 |  76.9 |  73.9 |  81.0 |  81.5 |  96.3 |  70.7 |  85.2 |  80.9 |  77.7 |  76.1 |  96.7 |  94.1 |  98.6 |  97.6 |  76.3 |  77.0 |  88.8 |  85.3 | 100.4 |  84.2 |  81.6 |  86.6 |  77.3 |  76.4 | 103.5 |  77.9 |  97.1 | 100.1 |  73.3 |  87.9 |  64.7 |  99.5 |  65.6 |  88.1 | 105.4 |  97.0 |  92.2 |  88.9 | 112.7 |  95.7 |  75.8 |  98.2 |  93.6 |  97.9 | 101.3 |  97.6 |  92.0 |  99.0 |  72.9 |  96.2 |  69.8 |  94.3 |  96.4 |  92.5 |  92.9 |  98.4 |  82.6 |  95.0 |  82.5 |  96.1 |  84.1 | 104.4 |  81.9 |  95.6 |  84.8 | 104.7 |  95.6 |  92.7 |  96.3 |  95.5 |   0.0 | 111.8 | 115.2 |  82.8 |  92.2 |  93.0 |  89.3 |  94.1 |  74.8 | 108.9 |  95.7 |  87.0 |  98.5 |\n",
            "| 89  |  93.9 |  87.0 |  60.6 |  71.3 |  81.1 | 105.3 |  59.9 | 109.5 | 104.0 |  82.9 |  94.0 | 112.7 |  54.5 | 102.7 | 108.6 |  61.1 | 116.1 |  67.0 |  98.6 | 119.0 | 101.1 | 115.5 | 102.2 | 113.3 |  76.9 |  91.4 | 109.3 |  91.0 |  96.4 |  95.9 |  61.8 |  66.9 | 111.2 | 113.1 | 100.1 | 113.7 |  96.2 | 115.7 | 110.2 |  91.0 | 109.4 | 110.6 |  77.7 | 107.4 |  98.0 |  59.7 | 120.5 |  99.2 | 123.1 |  54.6 | 108.8 | 113.9 |  46.8 |  51.4 |  90.1 | 106.1 |  45.2 |  56.4 | 102.5 | 116.2 | 120.8 |  67.2 |  98.0 |  54.4 | 106.6 | 108.4 | 114.8 |  70.5 | 114.1 |  77.6 | 107.0 |  59.1 |  81.4 |  69.4 | 115.8 | 106.2 |  97.7 |  99.9 | 111.0 |  46.4 |  79.2 |  91.7 |  99.7 |  68.9 | 100.8 |  84.1 |  82.1 |  53.1 | 111.8 |   0.0 |  47.8 |  86.9 |  88.4 | 113.8 | 115.2 | 110.9 |  95.3 |  56.9 |  66.9 |  97.6 |  88.4 |\n",
            "| 90  |  95.0 |  88.2 |  68.7 |  67.4 |  99.6 | 102.9 |  49.7 | 108.9 | 111.6 |  83.7 |  89.6 | 111.5 |  57.5 |  94.9 | 108.7 |  49.6 | 115.9 |  50.8 |  96.5 | 115.6 | 102.6 | 102.2 | 104.2 | 104.1 |  71.5 |  97.3 | 129.0 |  99.2 |  92.8 |  90.7 |  62.8 |  67.7 |  98.8 | 123.1 |  99.4 | 116.6 |  86.9 | 119.4 | 115.9 |  96.2 | 113.9 | 112.6 |  83.4 | 111.0 |  95.8 |  63.5 | 109.2 |  98.1 | 114.9 |  67.8 | 108.8 | 103.5 |  45.6 |  56.6 |  90.2 | 103.3 |  50.3 |  55.8 | 104.4 | 110.2 | 119.9 |  76.9 |  90.2 |  50.9 |  96.1 | 103.0 | 104.0 |  64.9 | 107.8 |  87.5 | 107.0 |  54.6 |  89.6 |  75.6 | 116.2 |  94.0 |  96.5 |  97.7 | 116.1 |  52.4 |  95.3 |  83.8 | 109.2 |  70.0 |  99.0 |  88.7 |  84.5 |  51.3 | 115.2 |  47.8 |   0.0 |  82.5 |  91.0 | 107.0 | 115.6 | 100.4 | 100.9 |  52.5 |  74.2 | 102.7 |  79.1 |\n",
            "| 91  |  79.9 |  87.4 |  89.3 |  76.3 |  92.5 | 102.4 |  81.6 |  96.5 |  91.3 |  77.0 |  70.3 | 103.3 |  83.5 | 116.8 |  85.5 |  87.7 |  93.1 |  88.2 |  71.8 |  99.7 |  90.9 |  80.7 | 125.3 |  96.1 |  72.2 |  67.1 |  91.1 |  88.7 |  78.2 |  87.7 |  88.5 |  94.3 |  82.8 |  99.0 | 115.3 |  94.4 | 112.5 |  91.6 | 117.0 |  83.0 |  94.0 |  91.2 |  69.4 |  89.0 | 110.9 |  74.0 |  99.3 | 115.2 |  90.4 |  94.5 |  75.1 | 102.1 |  87.0 |  90.3 |  85.8 | 124.9 |  85.5 |  79.1 |  91.3 | 107.2 |  84.6 |  80.7 | 113.9 |  89.4 | 106.7 |  99.6 |  84.7 |  83.5 |  76.7 |  74.1 | 121.8 |  92.7 |  70.6 |  73.6 | 104.6 | 119.5 |  84.7 | 115.4 |  83.4 |  96.9 |  73.1 |  96.0 |  83.3 |  71.9 | 120.7 |  79.7 |  78.8 |  79.7 |  82.8 |  86.9 |  82.5 |   0.0 |  71.4 | 100.5 | 117.4 | 114.5 |  83.6 |  78.7 |  85.8 | 108.2 |  99.0 |\n",
            "| 92  |  71.9 |  80.0 |  70.2 |  69.9 |  67.3 | 103.8 |  88.4 |  89.0 | 106.1 |  66.5 |  71.9 | 110.3 |  77.3 | 108.0 | 100.9 |  84.3 |  95.7 |  80.1 |  73.9 | 110.3 |  76.9 |  84.6 | 114.9 |  88.2 |  76.8 |  82.9 |  90.1 |  96.4 |  79.6 |  68.8 |  69.8 |  88.4 |  75.2 |  83.0 | 119.2 |  95.5 | 120.5 | 103.9 | 101.1 |  85.0 | 106.9 |  94.5 |  63.5 | 114.0 | 113.7 |  93.8 |  95.8 | 110.3 | 100.6 |  83.7 |  97.4 | 103.0 |  90.5 |  92.0 |  70.6 | 117.4 |  79.4 |  78.2 |  82.8 | 106.8 |  94.2 |  74.8 | 113.6 |  88.1 | 107.7 | 114.9 |  85.5 |  81.5 |  96.7 |  67.8 | 110.5 |  89.7 |  93.0 |  81.7 |  97.7 | 120.3 |  84.9 | 114.3 | 106.0 |  79.9 |  71.3 |  89.9 |  74.0 |  66.5 | 120.1 |  66.8 |  72.3 |  90.2 |  92.2 |  88.4 |  91.0 |  71.4 |   0.0 | 107.3 | 111.4 | 117.0 |  81.7 |  84.2 |  71.6 | 106.5 | 101.1 |\n",
            "| 93  | 108.3 | 106.9 | 112.2 | 119.8 | 118.4 |  54.2 | 121.2 |  66.0 |  62.2 | 106.8 |  93.7 |  54.6 | 110.0 |  51.4 |  83.6 | 106.8 |  69.3 | 110.4 |  97.6 |  88.9 |  88.6 |  85.0 |  51.2 |  90.6 | 102.8 | 102.3 |  84.2 |  99.7 | 107.5 | 111.9 | 110.2 | 110.5 |  96.5 |  92.2 |  50.9 |  57.6 |  53.3 |  63.3 |  75.3 | 111.9 |  63.5 |  71.9 | 120.4 |  69.0 |  48.8 | 112.4 |  66.7 |  51.9 |  96.2 | 118.7 |  93.2 |  53.0 | 112.1 | 115.6 | 106.7 |  48.1 | 114.9 | 118.2 |  99.9 |  57.7 |  72.3 | 106.7 |  51.6 | 118.5 |  49.2 |  52.1 |  81.5 | 111.9 |  82.5 | 105.1 |  72.9 | 113.3 | 107.6 | 109.1 |  74.4 |  50.0 |  89.9 |  48.5 |  85.3 | 116.3 | 113.0 | 108.4 |  97.1 | 115.7 |  46.1 | 105.6 | 112.4 | 107.1 |  93.0 | 113.8 | 107.0 | 100.5 | 107.3 |   0.0 |  56.8 |  50.0 | 106.4 | 104.7 | 109.8 |  60.0 |  85.1 |\n",
            "| 94  | 105.5 | 101.1 | 110.4 | 112.6 | 103.9 |  55.3 | 115.6 |  77.1 |  70.4 | 113.0 | 113.5 |  75.7 | 101.6 |  54.8 |  87.8 | 101.5 |  73.6 | 108.0 | 113.6 |  75.7 |  91.4 |  86.4 |  57.2 |  78.3 | 110.3 |  99.7 |  76.0 |  92.3 |  98.3 | 101.2 | 108.1 | 105.2 |  90.2 |  77.9 |  55.8 |  63.6 |  54.7 |  64.7 |  62.0 | 108.7 |  69.5 |  69.5 | 112.1 |  73.4 |  53.1 | 111.9 |  69.0 |  49.0 |  86.5 | 116.4 |  80.9 |  64.1 | 107.7 | 104.5 | 113.3 |  51.0 | 105.0 | 108.9 |  95.5 |  62.7 |  72.0 | 116.8 |  64.8 | 111.6 |  57.5 |  59.1 |  83.5 | 109.7 |  83.6 | 115.2 |  66.4 | 115.0 |  97.6 | 106.9 |  73.5 |  50.0 | 101.1 |  54.0 |  71.5 | 106.0 | 104.2 |  92.9 | 100.5 | 115.6 |  54.8 | 109.5 | 104.1 | 107.3 |  89.3 | 115.2 | 115.6 | 117.4 | 111.4 |  56.8 |   0.0 |  53.9 | 101.9 | 118.7 | 110.4 |  57.8 |  77.8 |\n",
            "| 95  | 112.0 | 127.0 | 108.2 | 108.0 | 127.4 |  53.0 | 105.7 |  78.7 |  68.1 | 115.3 | 101.0 |  66.8 | 104.8 |  44.9 |  78.3 | 103.3 |  62.9 | 104.2 |  95.8 |  77.4 | 102.7 |  85.6 |  52.0 |  79.9 | 113.5 | 102.4 |  84.4 | 110.9 |  96.2 | 118.3 | 108.7 | 106.2 |  89.7 |  89.9 |  51.8 |  63.5 |  48.7 |  68.5 |  61.9 | 115.4 |  64.6 |  72.0 | 113.8 |  69.7 |  51.1 | 119.0 |  68.0 |  58.1 |  89.2 | 115.1 |  86.8 |  58.7 | 108.2 | 106.2 | 110.7 |  47.4 | 114.4 | 106.6 |  94.5 |  56.7 |  81.3 | 107.8 |  50.3 | 115.2 |  58.2 |  55.5 |  88.9 | 113.7 |  81.6 | 117.7 |  61.2 | 102.4 | 107.6 | 118.2 |  60.6 |  47.9 |  99.5 |  52.8 |  81.8 | 115.0 | 112.2 |  98.4 |  94.6 | 111.1 |  52.9 | 107.9 | 106.8 | 117.9 |  94.1 | 110.9 | 100.4 | 114.5 | 117.0 |  50.0 |  53.9 |   0.0 | 103.0 | 110.5 | 107.7 |  52.7 |  84.5 |\n",
            "| 96  |  74.3 | 102.0 |  87.1 | 101.2 |  77.7 | 109.3 |  96.9 |  86.5 |  86.1 | 107.0 |  79.6 |  96.1 |  97.8 |  95.3 |  77.0 | 104.4 |  89.8 | 107.6 | 100.6 |  87.9 |  77.3 |  76.3 |  95.1 |  78.3 | 109.8 |  74.6 |  82.8 |  68.0 |  98.8 |  78.3 | 104.4 | 109.6 |  69.3 |  96.7 |  93.4 |  79.4 |  97.1 |  85.6 |  84.5 |  77.2 |  91.5 |  85.8 |  81.8 |  82.2 | 106.5 |  97.1 |  93.5 |  97.2 |  79.7 |  98.3 |  69.2 |  90.7 | 104.4 | 101.1 |  69.6 |  98.5 | 103.6 | 103.1 |  70.4 |  90.8 |  83.9 |  93.0 |  96.8 | 102.8 |  92.9 | 103.2 |  76.1 |  91.2 |  75.0 |  87.0 |  79.5 |  93.7 | 103.0 |  88.9 |  86.4 | 101.1 |  81.7 |  99.6 |  74.3 |  96.4 |  94.7 |  78.6 |  87.6 |  85.2 | 101.5 |  99.4 |  78.1 | 103.1 |  74.8 |  95.3 | 100.9 |  83.6 |  81.7 | 106.4 | 101.9 | 103.0 |   0.0 |  94.3 |  92.1 |  92.7 |  99.8 |\n",
            "| 97  |  98.0 |  80.8 |  61.8 |  76.4 |  87.7 | 114.8 |  58.9 | 119.0 | 113.9 |  83.1 |  76.4 | 105.4 |  57.1 |  95.7 | 106.0 |  54.5 | 113.8 |  55.7 |  92.7 | 107.5 |  85.1 | 106.4 | 101.9 | 109.6 |  82.2 |  89.2 | 111.2 |  91.5 |  93.5 |  92.8 |  63.2 |  66.8 | 104.2 | 110.8 | 106.2 | 110.9 |  93.6 | 112.5 | 130.8 |  99.4 | 105.3 | 111.6 |  97.3 | 112.3 | 106.0 |  61.1 | 111.7 | 109.0 | 103.3 |  55.4 | 100.6 | 111.8 |  58.3 |  65.7 |  90.5 | 101.9 |  54.4 |  62.4 |  91.3 | 116.0 | 109.3 |  69.1 |  96.0 |  59.6 |  98.0 | 106.8 | 107.4 |  55.0 | 114.3 |  69.7 | 110.6 |  57.9 |  86.7 |  65.0 | 123.4 | 104.9 |  92.0 | 100.3 | 109.8 |  58.5 |  83.7 | 100.4 | 103.8 |  60.7 | 106.1 |  87.6 |  91.6 |  57.6 | 108.9 |  56.9 |  52.5 |  78.7 |  84.2 | 104.7 | 118.7 | 110.5 |  94.3 |   0.0 |  67.8 | 108.4 |  87.4 |\n",
            "| 98  |  91.4 |  77.9 |  70.2 |  82.9 |  77.6 | 124.3 |  80.6 | 106.4 | 111.3 |  83.9 |  88.8 | 121.1 |  69.1 | 108.8 |  94.0 |  77.0 | 108.9 |  76.5 |  81.2 |  98.4 |  81.7 | 103.0 | 108.8 | 103.1 |  73.1 |  72.2 |  90.8 |  79.1 |  82.6 |  73.4 |  57.5 |  67.4 | 105.3 |  86.5 | 112.5 |  98.5 | 110.2 | 116.4 | 105.1 |  92.4 | 106.2 |  99.6 |  87.3 | 104.8 | 111.5 |  83.6 |  93.5 | 109.4 | 104.5 |  74.4 |  96.8 | 113.2 |  67.9 |  79.7 |  77.7 | 116.0 |  78.0 |  66.7 |  76.8 | 112.2 | 107.6 |  75.5 | 111.2 |  66.0 | 118.2 | 106.9 | 104.1 |  82.7 | 108.6 |  65.8 | 123.2 |  71.2 |  85.7 |  69.6 | 108.2 | 107.1 |  80.9 | 122.1 | 105.2 |  77.0 |  71.0 |  71.5 |  86.4 |  66.7 | 112.6 |  77.4 |  71.4 |  74.9 |  95.7 |  66.9 |  74.2 |  85.8 |  71.6 | 109.8 | 110.4 | 107.7 |  92.1 |  67.8 |   0.0 | 113.9 | 102.7 |\n",
            "| 99  | 122.9 | 117.6 | 106.8 | 109.7 | 113.4 |  49.5 | 111.7 |  75.8 |  70.5 | 112.9 | 108.8 |  73.8 |  99.1 |  47.0 |  85.2 |  96.9 |  69.8 | 101.8 | 103.5 |  84.3 | 102.6 | 108.3 |  40.1 |  86.8 | 102.7 | 104.4 |  91.4 | 104.8 | 114.4 | 120.0 | 116.1 | 114.9 |  89.5 |  88.4 |  47.0 |  66.0 |  50.8 |  70.3 |  67.0 | 100.9 |  62.0 |  89.5 | 107.0 |  79.9 |  43.9 | 115.6 |  67.8 |  51.7 | 103.8 | 102.2 |  90.8 |  54.7 |  96.5 |  98.9 |  96.1 |  54.8 |  99.1 | 102.3 | 110.4 |  67.6 |  80.0 | 104.6 |  45.9 | 114.1 |  48.0 |  60.7 |  86.2 | 105.4 |  92.7 | 113.2 |  62.2 | 101.1 | 113.3 | 104.8 |  60.1 |  48.2 |  97.3 |  42.7 |  76.6 |  99.5 | 116.5 | 104.6 | 101.5 | 113.4 |  48.7 | 120.6 | 107.3 | 111.9 |  87.0 |  97.6 | 102.7 | 108.2 | 106.5 |  60.0 |  57.8 |  52.7 |  92.7 | 108.4 | 113.9 |   0.0 |  84.2 |\n",
            "| SEP | 101.0 |  89.2 |  90.0 |  95.1 | 102.0 |  86.7 |  86.1 |  89.8 |  87.8 |  93.5 |  97.8 |  88.3 |  77.2 |  84.6 |  98.6 |  70.7 |  90.6 |  72.1 | 108.8 |  99.8 |  97.5 |  97.1 |  76.7 |  95.8 |  89.2 |  99.3 |  94.8 |  84.2 |  93.9 |  93.4 |  90.2 |  99.8 |  92.4 |  92.5 |  79.4 |  86.6 |  74.4 |  85.0 |  89.0 |  93.8 |  91.8 | 101.1 |  96.6 |  93.6 |  77.0 |  80.6 |  86.8 |  80.9 |  97.1 |  81.8 |  97.7 |  86.9 |  76.7 |  71.1 | 101.1 |  80.7 |  76.3 |  74.8 | 107.6 |  90.2 |  91.4 |  93.2 |  83.7 |  87.8 |  79.4 |  86.0 |  93.7 |  86.9 |  98.8 |  89.8 |  91.7 |  90.6 |  99.5 |  90.5 |  90.3 |  78.9 | 100.4 |  81.4 |  92.9 |  72.1 | 116.5 |  86.7 | 100.9 |  99.6 |  89.2 |  97.5 | 102.8 |  84.6 |  98.5 |  88.4 |  79.1 |  99.0 | 101.1 |  85.1 |  77.8 |  84.5 |  99.8 |  87.4 | 102.7 |  84.2 |   0.0 |\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_8d_pairwise_angles(model):\n",
        "    \"\"\"\n",
        "    Calculates the pairwise angles (in degrees) between all token vectors\n",
        "    in the original 8D embedding space.\n",
        "    \"\"\"\n",
        "    print(f\"Calculating pairwise angles between all vectors in the original {D_MODEL}D space...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Get original 8D embeddings\n",
        "    w_e = model.embed.W_E.detach().cpu().numpy()\n",
        "    w_u = model.unembed.W_U.T.detach().cpu().numpy()\n",
        "    labels = [str(d) for d in DIGITS] + ['SEP']\n",
        "\n",
        "    # --- Calculate for W_E ---\n",
        "    # cosine_similarity(X, Y) computes (X @ Y.T) / (norm(X) * norm(Y))\n",
        "    # The result is the cosine of the angle between vectors.\n",
        "    cos_sim_e = cosine_similarity(w_e)\n",
        "    # Clip values to be within [-1, 1] to avoid domain errors with arccos due to floating point inaccuracies\n",
        "    cos_sim_e = np.clip(cos_sim_e, -1.0, 1.0)\n",
        "    # The angle is the arccosine of the similarity\n",
        "    angles_e_rad = np.arccos(cos_sim_e)\n",
        "    angles_e_deg = np.rad2deg(angles_e_rad)\n",
        "    df_e = pd.DataFrame(angles_e_deg, index=labels, columns=labels)\n",
        "\n",
        "    # --- Calculate for W_U ---\n",
        "    cos_sim_u = cosine_similarity(w_u)\n",
        "    cos_sim_u = np.clip(cos_sim_u, -1.0, 1.0)\n",
        "    angles_u_rad = np.arccos(cos_sim_u)\n",
        "    angles_u_deg = np.rad2deg(angles_u_rad)\n",
        "    df_u = pd.DataFrame(angles_u_deg, index=labels, columns=labels)\n",
        "\n",
        "    print(f\"\\n--- Pairwise Angles (Degrees) for W_E (Embeddings) in {D_MODEL}D ---\")\n",
        "    # Use a specific float format for better readability\n",
        "    print(df_e.to_markdown(floatfmt=\".1f\"))\n",
        "\n",
        "    print(f\"\\n--- Pairwise Angles (Degrees) for W_U (Unembeddings) in {D_MODEL}D ---\")\n",
        "    print(df_u.to_markdown(floatfmt=\".1f\"))\n",
        "\n",
        "    return df_e, df_u\n",
        "\n",
        "# Calculate and display the pairwise angles from the 8D space\n",
        "df_e_angles_8d, df_u_angles_8d = calculate_8d_pairwise_angles(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Permutation-Invariant Spacing Analysis for W_E (Embeddings) ---\n",
            "Mean neighbor angle: 68.99°\n",
            "Std Dev of neighbor angles: 4.66°\n",
            "\n",
            "--- Permutation-Invariant Spacing Analysis for W_U (Unembeddings) ---\n",
            "Mean neighbor angle: 56.36°\n",
            "Std Dev of neighbor angles: 10.39°\n"
          ]
        }
      ],
      "source": [
        "def analyze_spacing_invariant(df_angles, name):\n",
        "    \"\"\"\n",
        "    Analyzes vector spacing in a permutation-invariant way by checking\n",
        "    the angles to the two nearest neighbors for each digit vector.\n",
        "    \"\"\"\n",
        "    # We only care about the digit embeddings for this analysis\n",
        "    digit_angles_df = df_angles.iloc[:len(DIGITS), :len(DIGITS)].copy()\n",
        "    \n",
        "    all_neighbor_angles = []\n",
        "    \n",
        "    print(f\"\\n--- Permutation-Invariant Spacing Analysis for {name} ---\")\n",
        "    # print(\"This method finds the angles to the two nearest neighbors for each digit vector.\")\n",
        "\n",
        "    for i in range(len(DIGITS)):\n",
        "        if i in []:\n",
        "            print(f\"Skipping digit {DIGITS[i]} (index {i}) for analysis.\")\n",
        "            continue\n",
        "        # Get angles from digit i to all other digits\n",
        "        angles_from_i = digit_angles_df.iloc[i].drop(digit_angles_df.columns[i])\n",
        "        # Sort to find the two smallest angles\n",
        "        sorted_angles = angles_from_i.sort_values()\n",
        "        # The two nearest neighbors\n",
        "        all_neighbor_angles.extend(sorted_angles.iloc[:2].values)\n",
        "        # print(f\"Digit {DIGITS[i]} has nearest neighbours: {sorted_angles.index[0]} ({sorted_angles.iloc[0]:.2f}°) & {sorted_angles.index[1]} ({sorted_angles.iloc[1]:.2f}°)\")\n",
        "\n",
        "    neighbor_angles = np.array(all_neighbor_angles)\n",
        "    \n",
        "    print(f\"Mean neighbor angle: {neighbor_angles.mean():.2f}°\")\n",
        "    print(f\"Std Dev of neighbor angles: {neighbor_angles.std():.2f}°\")\n",
        "\n",
        "analyze_spacing_invariant(df_e_angles_8d, \"W_E (Embeddings)\")\n",
        "analyze_spacing_invariant(df_u_angles_8d, \"W_U (Unembeddings)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0 Ablation\n",
            "--- Attention Pattern Change (Head 0) ---\n",
            "BEFORE Ablation:\n",
            "[[1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.42332 0.57668 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 1.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.43446 0.56554 0.00000]]\n",
            "AFTER Ablation:\n",
            "[[0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.00000 0.00000 0.00000]]\n",
            "---------------------------------------------\n",
            "\n",
            "--- Performance Metrics (on last 2 digits) ---\n",
            "             | Original   | Ablated   \n",
            "---------------------------------------------\n",
            "Loss:        | 0.234      | 30.616    \n",
            "Accuracy:    | 0.911      | 0.010     \n",
            "---------------------------------------------\n",
            "\n",
            "--- Prediction Comparison (Sample 0) ---\n",
            "Original sequence:   [ 80  52 100 100 100]\n",
            "Original prediction: [52 52 53 80 52]\n",
            "Ablated prediction:  [52 90 79 16 24]\n",
            "---------------------------------------------\n",
            "--- Attention Pattern Change (Head 0) ---\n",
            "BEFORE Ablation:\n",
            "[[1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.42332 0.57668 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 1.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.43446 0.56554 0.00000]]\n",
            "AFTER Ablation:\n",
            "[[0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.00000 0.00000 0.00000]]\n",
            "---------------------------------------------\n",
            "\n",
            "--- Performance Metrics (on last 2 digits) ---\n",
            "             | Original   | Ablated   \n",
            "---------------------------------------------\n",
            "Loss:        | 0.234      | 30.616    \n",
            "Accuracy:    | 0.911      | 0.010     \n",
            "---------------------------------------------\n",
            "\n",
            "--- Prediction Comparison (Sample 0) ---\n",
            "Original sequence:   [ 80  52 100 100 100]\n",
            "Original prediction: [52 52 53 80 52]\n",
            "Ablated prediction:  [52 90 79 16 24]\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# try setting specific attention positions to zero\n",
        "layer_to_ablate = 0 # output digits do nothijg in layer 0\n",
        "print(f\"Layer {layer_to_ablate} Ablation\")\n",
        "# Define which specific attention position you want to zero out\n",
        "key_pos, query_pos = 3,4 # (top left = [0,0]. query is the row, key is the column)\n",
        "\n",
        "def specific_attention_ablation_hook(\n",
        "    pattern, # Shape: [batch, head_index, query_pos, key_pos]\n",
        "    hook\n",
        "):    \n",
        "    # Set specific attention weight to 0\n",
        "    with torch.no_grad():\n",
        "        # This print statement will only run once if the validation set is processed as a single batch\n",
        "        print(\"--- Attention Pattern Change (Head 0) ---\")\n",
        "        print(f'BEFORE Ablation:\\n{pattern[0, head_index_to_ablate, :, :].cpu().numpy()}')\n",
        "\n",
        "        pattern[:, head_index_to_ablate, :2, :] = 0.0\n",
        "        # pattern[:, head_index_to_ablate, 3:4, :] = 0.0\n",
        "        pattern[:, head_index_to_ablate, query_pos, key_pos] = 0.0\n",
        "        # pattern[:, head_index_to_ablate, 2, :2] = 0.5\n",
        "        \n",
        "        print(f'AFTER Ablation:\\n{pattern[0, head_index_to_ablate, :, :].cpu().numpy()}')\n",
        "        print(\"-\" * 45)\n",
        "    \n",
        "    return pattern\n",
        "\n",
        "# Get the attention pattern hook name\n",
        "attn_pattern_hook_name = utils.get_act_name(\"pattern\", layer_to_ablate)\n",
        "\n",
        "# --- Calculate Ablated Loss on last 2 digits ---\n",
        "with torch.no_grad():\n",
        "    ablated_logits = model.run_with_hooks(\n",
        "        val_inputs,\n",
        "        return_type=\"logits\",  # Get logits instead of loss\n",
        "        fwd_hooks=[(attn_pattern_hook_name, specific_attention_ablation_hook)],\n",
        "    )\n",
        "    # Slice to get logits for the last two positions\n",
        "    output_logits_ablated = ablated_logits[:, LIST_LEN+1:]\n",
        "    # Calculate the loss\n",
        "    ablated_loss = loss_fn(\n",
        "        output_logits_ablated.reshape(-1, VOCAB), output_targets.reshape(-1)\n",
        "    )\n",
        "\n",
        "    # Calculate accuracy\n",
        "    ablated_predictions = ablated_logits.argmax(dim=-1)\n",
        "    ablated_output_predictions = ablated_predictions[:, LIST_LEN+1:]\n",
        "    ablated_accuracy = (ablated_output_predictions == output_targets).float().mean()\n",
        "\n",
        "print(\"\\n--- Performance Metrics (on last 2 digits) ---\")\n",
        "print(f\"{'':<12} | {'Original':<10} | {'Ablated':<10}\")\n",
        "print(\"-\" * 45)\n",
        "print(f\"{'Loss:':<12} | {original_loss.item():<10.3f} | {ablated_loss.item():<10.3f}\")\n",
        "print(f\"{'Accuracy:':<12} | {original_accuracy.item():<10.3f} | {ablated_accuracy.item():<10.3f}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "# Get the predicted tokens from the ablated logits\n",
        "ablated_predictions = ablated_logits.argmax(dim=-1)\n",
        "\n",
        "print(f\"\\n--- Prediction Comparison (Sample {sample_idx}) ---\")\n",
        "print(f\"Original sequence:   {val_inputs[sample_idx].cpu().numpy()}\")\n",
        "print(f\"Original prediction: {original_predictions[sample_idx].cpu().numpy()}\")\n",
        "print(f\"Ablated prediction:  {ablated_predictions[sample_idx].cpu().numpy()}\")\n",
        "print(\"-\" * 45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Analysis of 2000 Failure Cases ---\n",
            "Showing the first 10 incorrect predictions:\n",
            "\n",
            "Example 1 (Index: 0):\n",
            "  Input Digits:     [80 52]\n",
            "  Correct Output:   [80 52]\n",
            "  Predicted Output: [16 24] <--- ERROR\n",
            "\n",
            "Example 2 (Index: 1):\n",
            "  Input Digits:     [ 3 66]\n",
            "  Correct Output:   [ 3 66]\n",
            "  Predicted Output: [16 24] <--- ERROR\n",
            "\n",
            "Example 3 (Index: 2):\n",
            "  Input Digits:     [90 79]\n",
            "  Correct Output:   [90 79]\n",
            "  Predicted Output: [16 24] <--- ERROR\n",
            "\n",
            "Example 4 (Index: 3):\n",
            "  Input Digits:     [ 8 41]\n",
            "  Correct Output:   [ 8 41]\n",
            "  Predicted Output: [16 24] <--- ERROR\n",
            "\n",
            "Example 5 (Index: 4):\n",
            "  Input Digits:     [79 20]\n",
            "  Correct Output:   [79 20]\n",
            "  Predicted Output: [16 24] <--- ERROR\n",
            "\n",
            "Example 6 (Index: 5):\n",
            "  Input Digits:     [23 44]\n",
            "  Correct Output:   [23 44]\n",
            "  Predicted Output: [16 24] <--- ERROR\n",
            "\n",
            "Example 7 (Index: 6):\n",
            "  Input Digits:     [72 29]\n",
            "  Correct Output:   [72 29]\n",
            "  Predicted Output: [16 24] <--- ERROR\n",
            "\n",
            "Example 8 (Index: 7):\n",
            "  Input Digits:     [93 38]\n",
            "  Correct Output:   [93 38]\n",
            "  Predicted Output: [16 24] <--- ERROR\n",
            "\n",
            "Example 9 (Index: 8):\n",
            "  Input Digits:     [41 49]\n",
            "  Correct Output:   [41 49]\n",
            "  Predicted Output: [16 24] <--- ERROR\n",
            "\n",
            "Example 10 (Index: 9):\n",
            "  Input Digits:     [ 7 29]\n",
            "  Correct Output:   [ 7 29]\n",
            "  Predicted Output: [16 24] <--- ERROR\n"
          ]
        }
      ],
      "source": [
        "# --- Analyze Failure Cases ---\n",
        "# Find indices where the ablated prediction is incorrect\n",
        "is_incorrect = (ablated_output_predictions != output_targets).any(dim=1)\n",
        "error_indices = torch.where(is_incorrect)[0]\n",
        "\n",
        "print(f\"\\n--- Analysis of {len(error_indices)} Failure Cases ---\")\n",
        "if len(error_indices) > 0:\n",
        "    # Limit the number of printed examples for readability\n",
        "    n_examples_to_show = min(10, len(error_indices))\n",
        "    print(f\"Showing the first {n_examples_to_show} incorrect predictions:\")\n",
        "    \n",
        "    for i in range(n_examples_to_show):\n",
        "        idx = error_indices[i]\n",
        "        full_sequence = val_inputs[idx].cpu().numpy()\n",
        "        input_digits = full_sequence[:LIST_LEN]\n",
        "        correct_output = val_targets[idx, LIST_LEN+1:].cpu().numpy()\n",
        "        predicted_output = ablated_predictions[idx, LIST_LEN+1:].cpu().numpy()\n",
        "        \n",
        "        print(f\"\\nExample {i+1} (Index: {idx}):\")\n",
        "        print(f\"  Input Digits:     {input_digits}\")\n",
        "        print(f\"  Correct Output:   {correct_output}\")\n",
        "        print(f\"  Predicted Output: {predicted_output} <--- ERROR\")\n",
        "else:\n",
        "    print(\"No incorrect predictions found after ablation.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
