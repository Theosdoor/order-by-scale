{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a10051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "import umap\n",
    "\n",
    "import einops\n",
    "import pandas as pd, itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n",
    "\n",
    "from data import get_dataset\n",
    "\n",
    "# Configure plotly to use static rendering if widgets fail\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "float_formatter = \"{:.5f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2fe0b",
   "metadata": {},
   "source": [
    "# Data and Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e336a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- parameters ----------\n",
    "LIST_LEN = 2 # [d1, d2]\n",
    "SEQ_LEN = LIST_LEN * 2 + 1 # [d1, d2, SEP, o1, o2]\n",
    "\n",
    "N_DIGITS = 100\n",
    "DIGITS = list(range(N_DIGITS)) # 100 digits from 0 to 99\n",
    "MASK = N_DIGITS # special masking token for o1 and o2\n",
    "SEP = N_DIGITS + 1 # special seperator token for the model to think about the input (+1 to avoid confusion with the last digit)\n",
    "VOCAB = len(DIGITS) + 2  # + the special tokens\n",
    "\n",
    "D_MODEL = 128\n",
    "N_HEAD = 1\n",
    "N_LAYER = 2\n",
    "USE_LN = False # use layer norm in model\n",
    "USE_BIAS = False # use bias in model\n",
    "FREEZE_WV = True # no value matrix in attn \n",
    "FREEZE_WO = True # no output matrix in attn (i.e. attn head can only copy inputs to outputs)\n",
    "\n",
    "LEARNING_RATE = 1e-3 # default 1e-3\n",
    "WEIGHT_DECAY = 0.01 # default 0.01\n",
    "MAX_TRAIN_STEPS = 50_000 # max training steps\n",
    "USE_CHECKPOINTING = False # whether to use checkpointing for training\n",
    "\n",
    "MODEL_NAME = f'{N_LAYER}layer_{N_DIGITS}dig_{D_MODEL}d'\n",
    "MODEL_PATH = \"artifacts/\" + MODEL_NAME + \".pt\"\n",
    "\n",
    "DEV = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98163fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- mask ----------\n",
    "# attention mask for [d1, d2, SEP, o1, o2] looks like this:\n",
    "# -    d1    d2    SEP    o1    o2   (keys)\n",
    "# d1  -inf  -inf   -inf  -inf  -inf\n",
    "# d2   0    -inf   -inf  -inf  -inf\n",
    "# SEP  0      0    -inf  -inf  -inf\n",
    "# o1  -inf  -inf    0    -inf   -inf\n",
    "# o2  -inf  -inf    0      0    -inf\n",
    "# (queries)\n",
    "\n",
    "mask_bias = torch.triu(torch.ones(SEQ_LEN, SEQ_LEN) * float(\"-inf\")) # upper triangular bias mask (lead_diag & above = -inf, rest = 0)\n",
    "mask_bias[0, 0] = 0. # don't want a full row of -inf! otherwise we get nan erros & training breaks\n",
    "mask_bias[LIST_LEN+1:, :LIST_LEN] = float(\"-inf\") # stop output tokens from attending to input tokens\n",
    "mask_bias = mask_bias.unsqueeze(0).unsqueeze(0) # (1,1,T,T) broadcastable across batch and heads\n",
    "\n",
    "# L0: keep outputs self-only and allow SEP->digits; avoid all -inf rows\n",
    "mask_bias_l0 = mask_bias.clone()\n",
    "mask_bias_l0[..., LIST_LEN+1:, :] = float(\"-inf\") # block all for outputs\n",
    "idx = torch.arange(LIST_LEN+1, SEQ_LEN)  # re-enable self for outputs\n",
    "mask_bias_l0[..., idx, idx] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- dataset ----------\n",
    "train_ds, val_ds = get_dataset(\n",
    "    list_len=LIST_LEN, \n",
    "    n_digits=N_DIGITS, \n",
    "    train_split=0.8,\n",
    "    mask_tok=MASK, # use MASK as mask token\n",
    "    sep_tok=SEP, # use SEP as separator token\n",
    "    )\n",
    "\n",
    "train_batch_size = min(128, len(train_ds))\n",
    "val_batch_size = min(256, len(val_ds))\n",
    "train_dl = DataLoader(train_ds, train_batch_size, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, val_batch_size, drop_last=False)\n",
    "\n",
    "print(\"Input:\", train_ds[0][0])\n",
    "print(\"Target:\", train_ds[0][1])\n",
    "print(f\"Train dataset size: {len(train_ds)}, Validation dataset size: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c14b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- config helper ----------\n",
    "def attach_custom_mask(model):\n",
    "    def _mask(scores, hook=None):\n",
    "        # scores: (batch, heads, Q, K)\n",
    "        return scores + mask_bias.to(scores.device)\n",
    "    \n",
    "    def _mask_l0(scores, hook=None):\n",
    "        # layer-0 special mask: o1/o2 only self; SEP can read d1/d2\n",
    "        return scores + mask_bias_l0.to(scores.device)\n",
    "    \n",
    "    # Completely suppress attention for oi in L0 (safe: zero pattern rows, not -inf scores)\n",
    "    def _zero_o_rows(pattern, hook=None):\n",
    "        # pattern: [B, H, Q, K]\n",
    "        start_o = LIST_LEN+1 # first o_i index\n",
    "        if start_o < SEQ_LEN:\n",
    "            pattern = pattern.clone()\n",
    "            pattern[..., start_o:SEQ_LEN, :] = 0.0\n",
    "        return pattern\n",
    "    \n",
    "    # register the same mask hook on every layer\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        if i == 0:\n",
    "            block.attn.hook_attn_scores.add_perma_hook(_mask_l0, dir=\"fwd\")\n",
    "            block.attn.hook_pattern.add_perma_hook(_zero_o_rows, dir=\"fwd\")\n",
    "        else:\n",
    "            block.attn.hook_attn_scores.add_perma_hook(_mask, dir=\"fwd\")\n",
    "\n",
    "\n",
    "def strip_bias(m):\n",
    "    for mod in m.modules():\n",
    "        if hasattr(mod, \"bias\") and mod.bias is not None:\n",
    "            mod.bias.requires_grad_(False)\n",
    "            torch.nn.init.zeros_(mod.bias)\n",
    "\n",
    "    # remove biases from attention layers\n",
    "    attn_biases = ['b_Q', 'b_K', 'b_V', 'b_O']\n",
    "    for block in m.blocks:\n",
    "        for b in attn_biases:\n",
    "            mod = getattr(block.attn, b, None)\n",
    "            if mod is not None:\n",
    "                mod.requires_grad_(False)\n",
    "                torch.nn.init.zeros_(mod)\n",
    "\n",
    "    # remove unembed bias\n",
    "    if hasattr(m, \"unembed\") and hasattr(m.unembed, \"b_U\") and m.unembed.b_U is not None:\n",
    "        m.unembed.b_U.requires_grad_(False)\n",
    "        torch.nn.init.zeros_(m.unembed.b_U)\n",
    "\n",
    "def set_WV_identity_and_freeze(model, d_model):\n",
    "    with torch.no_grad():\n",
    "        # Create a stack of identity-like matrices for W_V\n",
    "        # Each matrix is of shape (d_model, d_head)\n",
    "        # We take the first d_head columns of the d_model x d_model identity matrix\n",
    "        identity_slice = torch.eye(d_model, model.cfg.d_head)\n",
    "        # Repeat for each head\n",
    "        W_V_identity = identity_slice.unsqueeze(0).repeat(model.cfg.n_heads, 1, 1)\n",
    "        \n",
    "        for block in model.blocks:\n",
    "            block.attn.W_V.copy_(W_V_identity)\n",
    "            block.attn.W_V.requires_grad = False\n",
    "\n",
    "def set_WO_identity_and_freeze(model, d_model):\n",
    "    with torch.no_grad():\n",
    "        # Create a stack of identity-like matrices for W_O\n",
    "        # Each matrix is of shape (d_head, d_model)\n",
    "        # We take the first d_head rows of the d_model x d_model identity matrix\n",
    "        identity_slice = torch.eye(model.cfg.d_head, d_model)\n",
    "        # Repeat for each head\n",
    "        W_O_identity = identity_slice.unsqueeze(0).repeat(model.cfg.n_heads, 1, 1)\n",
    "\n",
    "        for block in model.blocks:\n",
    "            block.attn.W_O.copy_(W_O_identity)\n",
    "            block.attn.W_O.requires_grad = False\n",
    "\n",
    "\n",
    "def make_model(n_layers=N_LAYER, n_heads=N_HEAD, d_model=D_MODEL, ln=USE_LN, use_bias=USE_BIAS, freeze_wv=FREEZE_WV, freeze_wo=FREEZE_WO):\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers = n_layers,\n",
    "        n_heads = n_heads,\n",
    "        d_model = d_model,\n",
    "        d_head = d_model//n_heads,\n",
    "        n_ctx=SEQ_LEN,\n",
    "        d_vocab=VOCAB,\n",
    "        attn_only=True, # no MLP!\n",
    "        normalization_type=(\"LN\" if ln else None),\n",
    "    )\n",
    "    model = HookedTransformer(cfg).to(DEV)\n",
    "    if freeze_wv:\n",
    "        set_WV_identity_and_freeze(model, d_model)\n",
    "    if freeze_wo:\n",
    "        set_WO_identity_and_freeze(model, d_model)\n",
    "    if not use_bias:\n",
    "        strip_bias(model)\n",
    "    \n",
    "    attach_custom_mask(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a96f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Model saving / loading helpers ------\n",
    "def save_model(model, path = MODEL_PATH):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(path = MODEL_PATH, device = DEV):\n",
    "    print(\"Loading model from\", path)\n",
    "    model = make_model()\n",
    "    model.load_state_dict(\n",
    "        torch.load(path, map_location=device)\n",
    "    )  # map weights to target device\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- utilities ----------\n",
    "def accuracy(m):\n",
    "    m.eval()\n",
    "    hits = tots = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dl:\n",
    "            logits = m(inputs.to(DEV))[:, LIST_LEN+1:]  # (batch, 2, vocab)\n",
    "            preds = logits.argmax(-1)\n",
    "            hits += (preds == targets[:, LIST_LEN+1:].to(DEV)).sum().item()\n",
    "            tots += preds.numel()\n",
    "    return hits / tots\n",
    "\n",
    "\n",
    "def train(m, max_steps=10_000, early_stop_acc=0.999, checkpoints=False, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, verbose=True):\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr, weight_decay=weight_decay)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "    dl = itertools.cycle(train_dl)  # infinite iterator\n",
    "    for step in tqdm(range(max_steps), desc=\"Training\"):\n",
    "        inputs, targets = next(dl)\n",
    "        # get logits/loss for output tokens only\n",
    "        logits = m(inputs.to(DEV))[:, LIST_LEN+1:].reshape(-1, VOCAB) \n",
    "        loss = ce(logits, targets[:, LIST_LEN+1:].reshape(-1).to(DEV))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        if (step + 1) % 100 == 0:\n",
    "            acc = accuracy(m)\n",
    "            if acc > early_stop_acc:\n",
    "                print(f\"Early stopping at step {step + 1} with accuracy {acc:.2%} >= {early_stop_acc:.2%}\")\n",
    "                break\n",
    "            update_every = max(min(10_000, max_steps//20), 1000)\n",
    "            if verbose and (step+1) % update_every == 0:\n",
    "                print(f\"Step {step + 1}, Loss: {loss.item():.4f}, Accuracy: {acc:.2%}\")\n",
    "            if checkpoints and (step+1) % 50_000 == 0:\n",
    "                save_model(m, MODEL_PATH)\n",
    "            \n",
    "    print(f\"Final accuracy: {accuracy(m):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca679443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD existing or train and SAVE new model\n",
    "load_existing = True  # Set to False to always train a new model\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and load_existing:\n",
    "    model = load_model(MODEL_PATH, device=DEV)\n",
    "else:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        MODEL_PATH = MODEL_PATH.replace(\".pt\", \"_new.pt\")\n",
    "        print(f\"Model path already exists. Saving new model to {MODEL_PATH}\")\n",
    "    print(\"Training model\")\n",
    "    model = make_model()\n",
    "    train(model, max_steps=MAX_TRAIN_STEPS, checkpoints=USE_CHECKPOINTING)\n",
    "    save_model(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40a903",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ab5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "head_index_to_ablate = 0 # fixed\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Check loss on validation set\n",
    "val_inputs = val_ds.tensors[0].to(DEV)\n",
    "val_targets = val_ds.tensors[1].to(DEV)\n",
    "sample_idx = 1  # Use the xth sample in the validation set for comparing predictions\n",
    "sample_list = val_inputs[sample_idx].cpu().numpy()\n",
    "\n",
    "# --- Calculate Original Loss on last 2 digits ---\n",
    "with torch.no_grad():\n",
    "    original_logits, cache = model.run_with_cache(val_inputs, return_type=\"logits\")\n",
    "    output_logits = original_logits[:, LIST_LEN+1:] # Slice to get logits for the last two positions\n",
    "    output_targets = val_targets[:, LIST_LEN+1:] # Slice to get the target tokens\n",
    "    \n",
    "    original_loss = loss_fn(output_logits.reshape(-1, VOCAB), output_targets.reshape(-1)) # Calculate the loss\n",
    "    # Calculate accuracy\n",
    "    original_predictions = original_logits.argmax(dim=-1) \n",
    "    original_output_predictions = original_predictions[:, LIST_LEN+1:]\n",
    "    original_accuracy = (original_output_predictions == output_targets).float().mean()\n",
    "\n",
    "print(f\"Original loss: {original_loss.item()}\")\n",
    "print(f\"Original accuracy: {original_accuracy.item()}\")\n",
    "print(f\"Sample sequence {sample_idx}: {sample_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75a4d5",
   "metadata": {},
   "source": [
    "## Attention Analysis and Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f643805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ------ Fig 1 ------\n",
    "\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "import matplotlib.patheffects as pe\n",
    "from matplotlib.path import Path as MplPath  # NEW: inspect curve type\n",
    "\n",
    "# Use a single validation example \n",
    "example = val_inputs[sample_idx].unsqueeze(0).to(DEV)\n",
    "\n",
    "# Run and cache activations\n",
    "_, cache = model.run_with_cache(example, return_type=\"logits\")\n",
    "\n",
    "# Collect attention patterns per layer -> shape [L, Q, K]\n",
    "att = (\n",
    "    torch.stack(\n",
    "        [cache[f\"blocks.{layer}.attn.hook_pattern\"] for layer in range(model.cfg.n_layers)],\n",
    "        dim=0,\n",
    "    )\n",
    "    .cpu()\n",
    "    .numpy()\n",
    "    .squeeze()\n",
    ")\n",
    "\n",
    "# prune arrows (these ones don't have any effect on the output)\n",
    "if N_LAYER == 2:\n",
    "    att[0][:2] = 0. * att[0][:2]\n",
    "    att[1][:3] = 0. * att[1][:3]\n",
    "elif N_LAYER == 3:\n",
    "    ablate = {\n",
    "        0: [(4, 2), (3, 2), (4, 3), (0, 0), (1, 0)],\n",
    "        1: [(3, 2), (4, 3), (0, 0), (1, 0), (2, 1)],\n",
    "        2: [(0, 0), (1, 0), (2, 0), (2, 1), (3, 0), (4, 0), (4, 1), (4, 2), (4, 3)],\n",
    "    }\n",
    "    # Vectorized assignment using numpy indexing (robust for single/multiple pairs)\n",
    "    for layer, pairs in ablate.items():\n",
    "        if not pairs:\n",
    "            continue\n",
    "        arr = np.array(pairs, dtype=int)  # shape (n_pairs, 2)\n",
    "        qs = arr[:, 0]\n",
    "        ks = arr[:, 1]\n",
    "        att[layer, qs, ks] = 0.0\n",
    "\n",
    "# Collect residual stream (embed + post-resid after each layer)\n",
    "resid_keys = [\"hook_embed\"] + [f\"blocks.{l}.hook_resid_post\" for l in range(model.cfg.n_layers)]\n",
    "resid_values = torch.stack([cache[k] for k in resid_keys], dim=0)  # [L+1, 1, seq, d_model]\n",
    "\n",
    "# Get W_U (compatibly)\n",
    "W_U = getattr(model, \"W_U\", model.unembed.W_U)\n",
    "\n",
    "# Logit lens: decode most likely token at each position after each layer\n",
    "position_tokens = (resid_values @ W_U).squeeze(1).argmax(-1)  # [L+1, seq]\n",
    "\n",
    "L, N, _ = att.shape\n",
    "\n",
    "# Layout\n",
    "x_positions = np.arange(L + 1)  # columns: input + after each layer\n",
    "y_positions = np.arange(N)[::-1]  # top token index = 0\n",
    "\n",
    "# Styling for publication (vector-safe fonts for PDF/SVG)\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.8, 4.2), dpi=300)\n",
    "\n",
    "# Node roles and colors\n",
    "position_names = [\"d1\", \"d2\", \"SEP\", \"o1\", \"o2\"]\n",
    "roles = [\"input\", \"input\", \"sep\", \"output\", \"output\"]\n",
    "role_colors = {\"input\": \"#4C78A8\", \"sep\": \"#F58518\", \"output\": \"#54A24B\"}\n",
    "node_colors = [role_colors[r] for r in roles]\n",
    "\n",
    "# Draw nodes and labels (keep these on top of arrows)\n",
    "for lx in range(L + 1):\n",
    "    xs = np.full(N, lx)\n",
    "    ax.scatter(xs, y_positions, s=180, c=node_colors,\n",
    "               edgecolor=\"black\", linewidth=0.6, zorder=4)\n",
    "    # left: position label (only in first col); right: decoded token id (every col)\n",
    "    for i, y in enumerate(y_positions):\n",
    "        if lx == 0:\n",
    "            ax.text(lx - 0.14, y, position_names[i], va=\"center\", ha=\"right\",\n",
    "                    fontsize=9, color=\"#334155\", zorder=5)\n",
    "        ax.text(lx + 0.14, y, str(position_tokens[lx, i].item()),\n",
    "                va=\"center\", ha=\"left\", fontsize=9, fontweight=\"bold\",\n",
    "                color=\"black\", zorder=5)\n",
    "\n",
    "# Residual stream (dotted straight arrows)\n",
    "for lx in range(L):\n",
    "    for y in y_positions:\n",
    "        arrow = FancyArrowPatch((lx, y), (lx + 1, y),\n",
    "                                arrowstyle=\"-\", mutation_scale=8,\n",
    "                                lw=1.0, linestyle=(0, (2, 2)), color=\"#94A3B8\",\n",
    "                                alpha=0.7, zorder=1, clip_on=False)\n",
    "        ax.add_patch(arrow)\n",
    "\n",
    "# --- Per-edge Δaccuracy placeholders ---\n",
    "# Fill this dict with your measured changes in percentage points (pp).\n",
    "# Key: (layer, query_idx, key_idx) where layer is 0-indexed.\n",
    "# Example: delta_acc_pp[(0, 2, 0)] = -1.7  # ablating L0: q=SEP, k=d1 lowers acc by 1.7 pp\n",
    "if N_LAYER == 2:\n",
    "    delta_acc_pp = {\n",
    "        # (l, q, k): value_in_pp,\n",
    "        (0, 2, 0): -87.4, # sep --> d1\n",
    "        (0, 2, 1): -74.3, # sep --> d2\n",
    "        (0, 3, 2): -0.2, # o1 --> sep\n",
    "        (0, 4, 2): -0.1, # o2 --> sep\n",
    "        (1, 3, 2): -48.6, # o1 --> sep (L1)\n",
    "        (1, 4, 2): -42.4, # o2 --> sep (L1)\n",
    "        (1, 4, 3): -39.0, # o2 --> o1 (L1)\n",
    "    }\n",
    "elif N_LAYER == 3:\n",
    "    delta_acc_pp = {\n",
    "        (0, 2, 0): -1.3,\n",
    "        (0, 2, 1): -49.6,\n",
    "        (1, 2, 0): -49.6,\n",
    "        (1, 4, 2): -49.5,\n",
    "        (2, 3, 2): -49.5,\n",
    "    }\n",
    "\n",
    "def format_delta_pp(val):\n",
    "    # Less obtrusive: short text, no \"Δacc:\" prefix\n",
    "    if val is None:\n",
    "        return \"—\"\n",
    "    sign = \"+\" if val >= 0 else \"−\"\n",
    "    return f\"{sign}{abs(val):.1f}%\"\n",
    "\n",
    "# Attention edges (curved; width/alpha ~ weight)\n",
    "threshold = 0.04  # ignore tiny weights\n",
    "arrow_color = \"#DC2626\"   # red\n",
    "arrow_alpha = 0.35        # translucent to avoid obscuring text\n",
    "\n",
    "# Label controls\n",
    "label_threshold = 0.04     # only label edges above this weight\n",
    "show_placeholder = False   # set True to show \"—\" for missing entries\n",
    "label_offset = 0.12        # distance of label from the edge midpoint\n",
    "\n",
    "CURVE_STRENGTH = 0.0  # try 0.04–0.08; set 0.0 if you want perfectly straight\n",
    "\n",
    "def edge_style(w):\n",
    "    lw = 0.6 + 2.0 * np.sqrt(float(w))\n",
    "    alpha = arrow_alpha\n",
    "    return lw, alpha\n",
    "\n",
    "def angle_in_display(ax, x0, y0, x1, y1):\n",
    "    # Compute angle in screen space so rotation matches visual slope despite axis scales\n",
    "    X0, Y0 = ax.transData.transform((x0, y0))\n",
    "    X1, Y1 = ax.transData.transform((x1, y1))\n",
    "    return np.degrees(np.arctan2(Y1 - Y0, X1 - X0))\n",
    "\n",
    "for l in range(L):\n",
    "    for q in range(N):\n",
    "        for k in range(N):\n",
    "            w = att[l, q, k]\n",
    "            if w <= threshold:\n",
    "                continue\n",
    "\n",
    "            x0, y0 = l,      y_positions[k]\n",
    "            x1, y1 = l + 1, y_positions[q]\n",
    "            dy = y1 - y0\n",
    "\n",
    "            # Curvature and style\n",
    "            rad = np.sign(dy) * CURVE_STRENGTH * (min(abs(dy), 2) / 2.0)\n",
    "            lw, alpha = edge_style(w)\n",
    "\n",
    "            arrow = FancyArrowPatch(\n",
    "                (x0, y0), (x1, y1),\n",
    "                connectionstyle=f\"arc3,rad={rad}\",\n",
    "                arrowstyle=\"->\", mutation_scale=8,\n",
    "                lw=lw, color=arrow_color, alpha=alpha,\n",
    "                zorder=2, shrinkA=8, shrinkB=8,\n",
    "                joinstyle=\"round\", capstyle=\"round\",\n",
    "                clip_on=False,\n",
    "            )\n",
    "            ax.add_patch(arrow)\n",
    "\n",
    "            # Check if a label should be drawn\n",
    "            delta_val = delta_acc_pp.get((l, q, k))\n",
    "            if (delta_val is None and not show_placeholder) or (w < label_threshold):\n",
    "                continue\n",
    "            \n",
    "            label_text = format_delta_pp(delta_val)\n",
    "\n",
    "            # --- NEW ROBUST LABEL PLACEMENT LOGIC ---\n",
    "\n",
    "            # 1. Calculate the angle of the straight line between nodes in display space\n",
    "            angle_deg = angle_in_display(ax, x0, y0, x1, y1) -8\n",
    "            angle_rad = np.deg2rad(angle_deg)\n",
    "\n",
    "            # 2. Calculate the simple midpoint of the straight line in data space\n",
    "            mid_data = np.array([(x0 + x1) / 2.0, (y0 + y1) / 2.0])\n",
    "\n",
    "            # 3. Calculate a perpendicular \"nudge\" vector in display space\n",
    "            #    This vector points perpendicularly outwards from the line on the screen\n",
    "            perp_vec_disp = np.array([-np.sin(angle_rad), np.cos(angle_rad)])\n",
    "            \n",
    "            # 4. Define how far to nudge the label in pixels.\n",
    "            #    This is proportional to the curvature `rad`.\n",
    "            #    This \"magic number\" controls the strength of the effect. Tune if needed.\n",
    "            offset_strength_px = -400.0 \n",
    "            pixel_offset = rad * offset_strength_px\n",
    "\n",
    "            # 5. Apply the nudge in display space for visual correctness\n",
    "            mid_disp = ax.transData.transform(mid_data)\n",
    "            label_pos_disp = mid_disp + pixel_offset * perp_vec_disp\n",
    "            \n",
    "            # 6. Transform the final label position back to data space\n",
    "            lx, ly = ax.transData.inverted().transform(label_pos_disp)\n",
    "\n",
    "            # 7. Annotate at the final calculated position\n",
    "            ann = ax.annotate(\n",
    "                label_text,\n",
    "                xy=(lx, ly),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=7,\n",
    "                color=\"#111827\",\n",
    "                rotation=angle_deg, # Rotate to match the chord\n",
    "                rotation_mode=\"anchor\",\n",
    "                zorder=4.2,\n",
    "                clip_on=False,\n",
    "            )\n",
    "            ann.set_path_effects([pe.withStroke(linewidth=2.0, foreground=\"white\", alpha=0.85)])\n",
    "         \n",
    "# Axes cosmetics\n",
    "ax.set_xlim(-0.5, L + 0.5)\n",
    "ax.set_ylim(-0.5, N - 0.5)\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels([\"Input\"] + [f\"After L{l+1}\" for l in range(L)])\n",
    "ax.set_yticks([])  # we draw our own labels\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Legend (update attention color to red)\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker=\"o\", color=\"w\", label=\"Inputs\",\n",
    "           markerfacecolor=role_colors[\"input\"], markeredgecolor=\"black\", markersize=7),\n",
    "    Line2D([0], [0], marker=\"o\", color=\"w\", label=\"SEP\",\n",
    "           markerfacecolor=role_colors[\"sep\"], markeredgecolor=\"black\", markersize=7),\n",
    "    Line2D([0], [0], marker=\"o\", color=\"w\", label=\"Outputs\",\n",
    "           markerfacecolor=role_colors[\"output\"], markeredgecolor=\"black\", markersize=7),\n",
    "    Line2D([0], [0], linestyle=(0, (2, 2)), color=\"#94A3B8\", lw=1.2, label=\"Residual\"),\n",
    "    Line2D([0], [0], color=arrow_color, lw=2, label=\"Attention\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, frameon=False, ncol=5,\n",
    "          loc=\"upper center\", bbox_to_anchor=(0.5, -0.12))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8076f7d",
   "metadata": {},
   "source": [
    "### Mean Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbed750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mean Attention Patterns ---\n",
    "\n",
    "all_pats = [[] for _ in range(model.cfg.n_layers)]\n",
    "for inputs, _ in val_dl:\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(inputs.to(DEV))\n",
    "    for l in range(model.cfg.n_layers):\n",
    "        pat = cache[\"pattern\", l][:, 0]  # (batch, Q, K)\n",
    "        all_pats[l].append(pat)\n",
    "all_pats = [torch.cat(pats, dim=0) for pats in all_pats]\n",
    "\n",
    "for l, pats in enumerate(all_pats):\n",
    "    identical = torch.allclose(pats, pats[0].expand_as(pats))\n",
    "    print(f\"Layer {l}: all attention patterns identical? {'✅' if identical else '❌'}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    avg_pats = [\n",
    "        torch.zeros(SEQ_LEN, SEQ_LEN, device=DEV) for _ in range(model.cfg.n_layers)\n",
    "    ]\n",
    "    n = 0\n",
    "    for inputs, _ in val_dl:\n",
    "        _, cache = model.run_with_cache(inputs.to(DEV))\n",
    "        for l in range(model.cfg.n_layers):\n",
    "            avg_pats[l] += cache[\"pattern\", l][:, 0].sum(0)\n",
    "        n += inputs.shape[0]\n",
    "    avg_pats = [p / n for p in avg_pats]\n",
    "\n",
    "\n",
    "# Create a deep copy of the model to avoid modifying the original\n",
    "model_with_avg_attn = copy.deepcopy(model)\n",
    "\n",
    "def mk_hook(avg):\n",
    "    logits = (avg + 1e-12).log()  # log-prob so softmax≈avg, ε avoids -∞\n",
    "\n",
    "    def f(scores, hook):\n",
    "        return logits.unsqueeze(0).unsqueeze(0).expand_as(scores)\n",
    "\n",
    "    return f\n",
    "\n",
    "for l in range(model_with_avg_attn.cfg.n_layers):\n",
    "    model_with_avg_attn.blocks[l].attn.hook_attn_scores.add_hook(\n",
    "        mk_hook(avg_pats[l]), dir=\"fwd\"\n",
    "    )\n",
    "\n",
    "print(\"Accuracy with avg-attn:\", accuracy(model_with_avg_attn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1487c",
   "metadata": {},
   "source": [
    "Using the mean attention pattern destroys performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0131356",
   "metadata": {},
   "source": [
    "Earlier research found that RNNs use a fixed attention pattern where the embeddings are projected into an \"onion ring\" pattern. However, in our model, the attention pattern to each of the input tokens is roughly normally distributed, for each of the positions (though always summing to 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1576dbb",
   "metadata": {},
   "source": [
    "### Ablation of specific attn edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09876eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Ablation of Specific Attention Edges ----\n",
    "\n",
    "renorm_rows = False # whether to renormalize rows after ablation\n",
    "ablate_in_l0 = [\n",
    "                (4,3),\n",
    "                (0,0),\n",
    "                (1,0)\n",
    "                ]\n",
    "ablate_in_l1 = [\n",
    "                (0,0),\n",
    "                (1,0),\n",
    "                (2,0),\n",
    "                (2,1),\n",
    "                ]\n",
    "\n",
    "ablate_in_l2 = [(0,0),(1,0),(2,0), (2,1), (3,0),  (4,0), (4,1), (4,2), (4,3)]\n",
    "\n",
    "# Try ablating multiple layer attention patterns at same time\n",
    "def build_qk_mask(positions=None, queries=None, keys=None, seq_len=SEQ_LEN):\n",
    "    \"\"\"\n",
    "    Create a boolean mask of shape (seq_len, seq_len) where True means \"ablate this (q,k)\".\n",
    "    You can pass:\n",
    "      - positions: list of (q, k) tuples\n",
    "      - or queries: iterable of q, and keys: iterable of k (outer-product mask)\n",
    "    \"\"\"\n",
    "    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
    "    if positions is not None:\n",
    "        for q, k in positions:\n",
    "            mask[q, k] = True\n",
    "    else:\n",
    "        if queries is None:\n",
    "            queries = range(seq_len)\n",
    "        if keys is None:\n",
    "            keys = range(seq_len)\n",
    "        for q in queries:\n",
    "            mask[q, keys] = True\n",
    "    return mask\n",
    "\n",
    "def make_pattern_hook(mask_2d: torch.Tensor, head_index=None, set_to=0.0, renorm=True, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Returns a fwd hook for the 'pattern' activation that:\n",
    "      - sets masked entries to set_to (default 0.0)\n",
    "      - optionally renormalizes rows so they sum to 1 again (per head, per batch, per query row)\n",
    "    Args:\n",
    "      mask_2d: Bool tensor [Q, K]\n",
    "      head_index: int to affect a single head, or None to affect all heads\n",
    "      set_to: value to write into masked entries (usually 0.0)\n",
    "      renorm: whether to renormalize rows after masking\n",
    "    \"\"\"\n",
    "    mask_2d = mask_2d.detach()\n",
    "\n",
    "    def hook(pattern, hook):\n",
    "        # pattern: [batch, n_heads, Q, K]\n",
    "        B, H, Q, K = pattern.shape\n",
    "        m4_all = mask_2d.to(pattern.device).view(1, 1, Q, K)  # broadcastable\n",
    "        # Keep a copy for safe fallback in renorm\n",
    "        pre = pattern.clone()\n",
    "        print(f\"\\nLayer {hook.layer()} Ablation\")\n",
    "        print(f'BEFORE Ablation:\\n{pattern[sample_idx, head_index, :, :].cpu().numpy()}')\n",
    "        # print(f'Mask:\\n{m4_all[0, 0, :, :].cpu().numpy()}')\n",
    "        \n",
    "\n",
    "        if head_index is None:\n",
    "            pattern = torch.where(m4_all, torch.as_tensor(set_to, device=pattern.device), pattern)\n",
    "        else:\n",
    "            m3 = m4_all.squeeze(1)  # [1, Q, K]\n",
    "            ph = pattern[:, head_index]  # [B, Q, K]\n",
    "            ph = torch.where(m3, torch.as_tensor(set_to, device=pattern.device), ph)\n",
    "            pattern[:, head_index] = ph\n",
    "\n",
    "        if renorm:\n",
    "            # Renormalize only rows whose query index has any masked key\n",
    "            rows_to_fix = mask_2d.any(dim=-1)  # [Q]\n",
    "            if rows_to_fix.any():\n",
    "                rows_idx = rows_to_fix.nonzero(as_tuple=False).squeeze(-1)  # [Nr]\n",
    "                heads = range(H) if head_index is None else [head_index]\n",
    "                for h in heads:\n",
    "                    # p: [B, Nr, K]\n",
    "                    p = pattern[:, h, rows_idx, :]\n",
    "                    s = p.sum(dim=-1, keepdim=True).clamp_min(eps)   # [B, Nr, 1]\n",
    "                    pattern[:, h, rows_idx, :] = p / s\n",
    "\n",
    "        print(f'AFTER Ablation:\\n{pattern[sample_idx, head_index, :, :].cpu().numpy()}')\n",
    "        return pattern\n",
    "\n",
    "    return hook\n",
    "\n",
    "# Example usage:\n",
    "# Define what to ablate per layer:\n",
    "# - As explicit (q,k) pairs\n",
    "# - Or as queries/keys sets (outer-product)\n",
    "layers_to_ablate = {\n",
    "    0: build_qk_mask(positions=ablate_in_l0, seq_len=SEQ_LEN),\n",
    "    1: build_qk_mask(positions=ablate_in_l1, seq_len=SEQ_LEN),\n",
    "    # 2: build_qk_mask(positions=ablate_in_l2, seq_len=SEQ_LEN),\n",
    "}\n",
    "\n",
    "# Apply to a single head or all heads\n",
    "head = None  # Set to None to affect all heads, or specify a head index (e.g., 0)\n",
    "\n",
    "# Build hooks\n",
    "fwd_hooks = []\n",
    "for layer_idx, mask in layers_to_ablate.items():\n",
    "    hook_name = utils.get_act_name(\"pattern\", layer_idx)\n",
    "    fwd_hooks.append((hook_name, make_pattern_hook(mask, head_index=head, set_to=0.0, renorm=renorm_rows)))\n",
    "\n",
    "# Run with hooks and evaluate on last two positions\n",
    "with torch.no_grad():\n",
    "    logits_multi = model.run_with_hooks(val_inputs, return_type=\"logits\", fwd_hooks=fwd_hooks)\n",
    "\n",
    "output_logits_multi = logits_multi[:, LIST_LEN+1:]\n",
    "ablated_output_predictions = output_logits_multi.argmax(dim=-1)\n",
    "output_targets = val_targets[:, LIST_LEN+1:]\n",
    "\n",
    "ablated_loss = loss_fn(output_logits_multi.reshape(-1, VOCAB), val_targets[:, LIST_LEN+1:].reshape(-1))\n",
    "ablated_acc = (ablated_output_predictions == val_targets[:, LIST_LEN+1:]).float().mean()\n",
    "\n",
    "print(\"\\n--- Performance Metrics ---\")\n",
    "print(f\"Multi-layer attention ablation -> Loss: {ablated_loss.item():.3f}, Acc: {ablated_acc.item():.3f}\")\n",
    "\n",
    "# Optional: inspect a sample\n",
    "idx = sample_idx\n",
    "print(\"Sample sequence:\", val_inputs[idx].cpu().numpy())\n",
    "print(\"Original:\", original_predictions[idx].cpu().numpy())\n",
    "print(\"Ablated: \", logits_multi.argmax(dim=-1)[idx].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse errors from the ablated model wrt o_2\n",
    "\n",
    "def analyze_o2_errors(preds, targets, inputs=None, top_k=10):\n",
    "    \"\"\"\n",
    "    preds, targets: [B, 2] (o1, o2)\n",
    "    inputs: [B, 5] full token seq [d1, d2, SEP, o1, o2] (optional)\n",
    "    \"\"\"\n",
    "    assert preds.shape == targets.shape and preds.ndim == 2 and preds.shape[1] == 2\n",
    "    B = preds.shape[0]\n",
    "    o1_pred, o2_pred = preds[:, 0], preds[:, 1]\n",
    "    o1_true, o2_true = targets[:, 0], targets[:, 1]\n",
    "\n",
    "    o2_wrong_mask = (o2_pred != o2_true)\n",
    "    num_o2_wrong = int(o2_wrong_mask.sum().item())\n",
    "    if num_o2_wrong == 0:\n",
    "        print(\"No o2 errors.\")\n",
    "        return\n",
    "\n",
    "    # Relations to o1\n",
    "    dupes_rate = (o2_pred[o2_wrong_mask] == o1_pred[o2_wrong_mask]).float().mean().item()\n",
    "    equals_o1_true_rate = (o2_pred[o2_wrong_mask] == o1_true[o2_wrong_mask]).float().mean().item()\n",
    "    o1_correct_given_o2_wrong = (o1_pred[o2_wrong_mask] == o1_true[o2_wrong_mask]).float().mean().item()\n",
    "\n",
    "    # Relations to inputs (if provided)\n",
    "    if inputs is not None:\n",
    "        d1, d2 = inputs[:, 0], inputs[:, 1]\n",
    "        eq_d1 = int((o2_pred[o2_wrong_mask] == d1[o2_wrong_mask]).sum().item())\n",
    "        eq_d2 = int((o2_pred[o2_wrong_mask] == d2[o2_wrong_mask]).sum().item())\n",
    "    else:\n",
    "        eq_d1 = eq_d2 = None\n",
    "\n",
    "    # Frequency of o2 predictions when wrong\n",
    "    vals = o2_pred[o2_wrong_mask]\n",
    "    counts = torch.bincount(vals, minlength=VOCAB).cpu()\n",
    "    top_idx = counts.argsort(descending=True)[:top_k]\n",
    "        \n",
    "\n",
    "    def tok_label(t):\n",
    "        t = int(t)\n",
    "        if t < N_DIGITS: return f\"{t}\"\n",
    "        if t == MASK: return \"MASK\"\n",
    "        if t == SEP: return \"SEP\"\n",
    "        return f\"tok{t}\"\n",
    "\n",
    "    print(f\"o2 wrong: {num_o2_wrong}/{B} ({num_o2_wrong/B:.2%})\")\n",
    "    print(f\"P(o2_pred == o1_pred | o2 wrong): {dupes_rate:.2%}\")\n",
    "    print(f\"P(o2_pred == o1_true | o2 wrong): {equals_o1_true_rate:.2%}\")\n",
    "    print(f\"P(o1 correct | o2 wrong): {o1_correct_given_o2_wrong:.2%}\")\n",
    "    if eq_d1 is not None:\n",
    "        print(f\"P(o2_pred == d1 | o2 wrong): {eq_d1/num_o2_wrong:.2%} ({eq_d1})\")\n",
    "        print(f\"P(o2_pred == d2 | o2 wrong): {eq_d2/num_o2_wrong:.2%} ({eq_d2})\")\n",
    "\n",
    "    print(\"\\nTop o2 predictions when wrong:\")\n",
    "    for t in top_idx.tolist():\n",
    "        c = int(counts[t].item())\n",
    "        if c == 0: continue\n",
    "        print(f\"  {tok_label(t):>4}: {c} ({c/num_o2_wrong:.2%})\")\n",
    "\n",
    "    # Show a few concrete examples\n",
    "    show = min(5, num_o2_wrong)\n",
    "    idxs = torch.nonzero(o2_wrong_mask).squeeze(-1)[:show].cpu().tolist()\n",
    "    print(\"\\nExamples (d1,d2) -> (o1_true,o2_true) | (o1_pred,o2_pred):\")\n",
    "    for i in idxs:\n",
    "        if inputs is not None:\n",
    "            d1i, d2i = int(inputs[i, 0]), int(inputs[i, 1])\n",
    "            left = f\"({d1i},{d2i}) -> ({int(o1_true[i])},{int(o2_true[i])})\"\n",
    "        else:\n",
    "            left = f\"-> ({int(o1_true[i])},{int(o2_true[i])})\"\n",
    "        print(f\"  {left} | ({int(o1_pred[i])},{int(o2_pred[i])})\")\n",
    "\n",
    "analyze_o2_errors(ablated_output_predictions, output_targets, inputs=val_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd1f0d",
   "metadata": {},
   "source": [
    "## Circuit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- constants for equation ----\n",
    "# Get attention patterns for both layers on the validation set\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, cache = model.run_with_cache(val_inputs)\n",
    "    dig_logits = logits[:,:,:-2]  # exclude SEP and mask token logits\n",
    "    b_size = dig_logits.shape[0]  # batch size\n",
    "\n",
    "# get required attention values\n",
    "alpha = cache[\"pattern\", 0][:, head_index_to_ablate]  # Layer 0\n",
    "beta = cache[\"pattern\", 1][:, head_index_to_ablate]   # Layer 1\n",
    "alpha_sep_d1 = alpha[:,2, 0].unsqueeze(-1)  # SEP -> d1\n",
    "alpha_sep_d2 = alpha[:,2, 1].unsqueeze(-1)  # SEP -> d2\n",
    "beta_o2_o1 = beta[:,-1, -2].unsqueeze(-1)\n",
    "beta_o2_sep = beta[:,-1, 2].unsqueeze(-1) # beta_o2_o1 + beta_o2_sep = 1.0\n",
    "\n",
    "# Weights and embeddings\n",
    "W_E = model.W_E.detach()  # (vocab, d_model)\n",
    "W_pos = model.W_pos.detach()  # (seq_len, d_model)\n",
    "W_U = model.unembed.W_U.detach()  # (d_model, vocab)\n",
    "\n",
    "# Input tokens for d1, d2\n",
    "d1_tok = val_inputs[:, 0]\n",
    "d2_tok = val_inputs[:, 1]\n",
    "\n",
    "# get embeds\n",
    "big_d1 = W_E[d1_tok] + W_pos[0,:]  # d1 embedding (d_model)\n",
    "big_d2 = W_E[d2_tok] + W_pos[1,:] # d2 embedding (d_model)\n",
    "pos_o1 = W_pos[-2,:]  # o1 position (d_model)\n",
    "pos_o2 = W_pos[-1,:]  # o2 position (d_model)\n",
    "mask_embed = W_E[MASK]  # (d_model)\n",
    "sep_embed = W_E[SEP] + W_pos[2,:]  # SEP token embedding (d_model)\n",
    "\n",
    "# get shapes right\n",
    "pos_o1 = pos_o1.expand(b_size, -1)\n",
    "pos_o2 = pos_o2.expand(b_size, -1)\n",
    "mask_embed = mask_embed.expand(b_size, -1)\n",
    "sep_embed = sep_embed.expand(b_size, -1)\n",
    "\n",
    "# print(\"mask_embed shape:\", mask_embed.shape)\n",
    "# print(\"sep_embed shape:\", sep_embed.shape)\n",
    "# print(\"pos_o1 shape:\", pos_o1.shape)\n",
    "# print(\"pos_o2 shape:\", pos_o2.shape)\n",
    "# print(\"big_d1 shape:\", big_d1.shape)\n",
    "# print(\"big_d2 shape:\", big_d2.shape)\n",
    "# print(\"alpha_sep_d1 shape:\", alpha_sep_d1.shape)\n",
    "# print(\"alpha_sep_d2 shape:\", alpha_sep_d2.shape)\n",
    "# print(\"beta_o2_o1 shape:\", beta_o2_o1.shape)\n",
    "# print(\"beta_o2_sep shape:\", beta_o2_sep.shape)\n",
    "\n",
    "# verify by reconstructing logits\n",
    "\n",
    "l_o1 = (mask_embed + sep_embed + pos_o1 + alpha_sep_d1*big_d1+ alpha_sep_d2*big_d2) @ W_U  # logits for o1 (d_model)\n",
    "l_o1_digits = l_o1[:, :N_DIGITS]\n",
    "patched_o1_logits = l_o1_digits.argmax(dim=-1)\n",
    "acc_patched_o1 = (patched_o1_logits == val_targets[:, -2]).float().mean().item()\n",
    "\n",
    "l_o2 = ((1+beta_o2_o1)*mask_embed + beta_o2_o1*pos_o1 + pos_o2 + beta_o2_sep*(alpha_sep_d1*big_d1 + alpha_sep_d2*big_d2 + sep_embed)) @ W_U  # logits for o2 (d_model)\n",
    "l_o2_digits = l_o2[:, :N_DIGITS]\n",
    "patched_o2_logits = l_o2_digits.argmax(dim=-1)\n",
    "acc_patched_o2 = (patched_o2_logits == val_targets[:, -1]).float().mean().item()\n",
    "\n",
    "\n",
    "# Compare reconstructed logits to model logits for o2\n",
    "with torch.no_grad():\n",
    "    model_o2_logits = logits[:, -1, :N_DIGITS]  # [B, N_DIGITS]\n",
    "    l2_diff = ((l_o2_digits - model_o2_logits).norm(dim=-1).mean().item())\n",
    "    print(f\"Mean L2 diff between reconstructed and model o2 logits: {l2_diff:.4f}\")\n",
    "\n",
    "print(f'\\nReconstructed accuracy: {(acc_patched_o1 + acc_patched_o2) / 2.0:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcc224",
   "metadata": {},
   "source": [
    "Great! This suggests we have successfully reconstructed the logits as the accuracy is the same using the reconstructed logits to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logit Difference Calculation ---\n",
    "# scale constants\n",
    "scaled_pos_o1 = -beta_o2_sep * pos_o1\n",
    "scaled_big_d1 = -beta_o2_o1 * alpha_sep_d1 * big_d1\n",
    "scaled_big_d2 = -beta_o2_o1 * alpha_sep_d2 * big_d2\n",
    "scaled_sep_embed = -beta_o2_o1 * sep_embed\n",
    "scaled_mask_embed = beta_o2_o1 * mask_embed\n",
    "\n",
    "# logit_o2 - logit_o1\n",
    "logit_diff = pos_o2 + scaled_pos_o1 + scaled_big_d1 + scaled_big_d2 + scaled_sep_embed + scaled_mask_embed\n",
    "logit_diff = (logit_diff @ W_U )[:,:-2]  # exclude sep and mask token logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1995c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Fig 2 - Logit Difference Contributions -----\n",
    "\n",
    "# get embeds in d_model space\n",
    "term_pos_o2 = W_pos[-1,:].expand(b_size, -1)\n",
    "term_pos_o1 = -beta_o2_sep * W_pos[-2,:].expand(b_size, -1)\n",
    "term_big_d1 = -beta_o2_o1 * alpha_sep_d1 * (W_E[d1_tok] + W_pos[0,:])\n",
    "term_big_d2 = -beta_o2_o1 * alpha_sep_d2 * (W_E[d2_tok] + W_pos[1,:])\n",
    "term_sep = -beta_o2_o1 * (W_E[SEP] + W_pos[2,:]).expand(b_size, -1)\n",
    "term_mask = beta_o2_o1 * W_E[MASK].expand(b_size, -1)\n",
    "\n",
    "\n",
    "# Define number of digits from logit shape\n",
    "N_DIGITS = dig_logits.shape[-1]\n",
    "\n",
    "# Helper to project d_model vectors to logit space for N_DIGITS\n",
    "def unembed_digits(x):\n",
    "    return (x @ W_U)[:, :N_DIGITS]\n",
    "\n",
    "# Unembed all our terms into logit-space contributions\n",
    "base_o1_digits = logits[:, -2, :N_DIGITS]\n",
    "contrib_pos_o2 = unembed_digits(term_pos_o2)\n",
    "contrib_pos_o1 = unembed_digits(term_pos_o1)\n",
    "contrib_big_d1 = unembed_digits(term_big_d1)\n",
    "contrib_big_d2 = unembed_digits(term_big_d2)\n",
    "contrib_sep = unembed_digits(term_sep)\n",
    "contrib_mask = unembed_digits(term_mask)\n",
    "\n",
    "# Helper to gather the specific logits for d1 and d2 from a logit matrix\n",
    "def gather_pair_cols(logits, d1, d2):\n",
    "    # logits: [B, N_DIGITS], d1: [B], d2: [B]\n",
    "    d1_logits = logits.gather(-1, d1.unsqueeze(-1))\n",
    "    d2_logits = logits.gather(-1, d2.unsqueeze(-1))\n",
    "    return torch.cat([d1_logits, d2_logits], dim=-1) # -> [B, 2]\n",
    "\n",
    "# Define the mask for the specific cases we're analyzing (where target_o2 is d2)\n",
    "# This assumes `val_targets` (shape [batch, seq_len]) is defined.\n",
    "tgt_o2 = val_targets[:, -1]\n",
    "mask_unique = (d1_tok != d2_tok) & ((tgt_o2 == d1_tok) | (tgt_o2 == d2_tok))\n",
    "m = mask_unique & (tgt_o2 == d2_tok)\n",
    "\n",
    "# Calculate the final scalar values required for the plot\n",
    "base_diff_d2_d1 = (\n",
    "    gather_pair_cols(base_o1_digits[m], d1_tok[m], d2_tok[m])[:, 1] - \n",
    "    gather_pair_cols(base_o1_digits[m], d1_tok[m], d2_tok[m])[:, 0]\n",
    ").mean().cpu()\n",
    "\n",
    "contribs_list = [contrib_pos_o2, contrib_pos_o1, contrib_big_d1, contrib_big_d2, contrib_sep, contrib_mask]\n",
    "diff_contributions = []\n",
    "for c in contribs_list:\n",
    "    c_d2 = gather_pair_cols(c[m], d1_tok[m], d2_tok[m])[:, 1]\n",
    "    c_d1 = gather_pair_cols(c[m], d1_tok[m], d2_tok[m])[:, 0]\n",
    "    diff_contributions.append((c_d2 - c_d1).mean().item())\n",
    "\n",
    "final_calc_diff = base_diff_d2_d1 + sum(diff_contributions)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "\n",
    "# Labels for the plot, matching the paper's notation\n",
    "base_name = r'$\\ell_{o_1}$ (base)'\n",
    "contrib_names = [\n",
    "    r'$P(o_2)$', r'$-\\beta_{o_2 \\to s} P(o_1)$', r'$-\\beta_{o_2 \\to o_1}\\alpha_{s \\to d_1}D_1$',\n",
    "    r'$-\\beta_{o_2 \\to o_1}\\alpha_{s \\to d_2}D_2$', r'$-\\beta_{o_2 \\to o_1}S$', r'$+\\beta_{o_2 \\to o_1}E(m)$'\n",
    "]\n",
    "\n",
    "# Combine base and contribution data for plotting\n",
    "all_names = [base_name] + contrib_names\n",
    "all_values = [base_diff_d2_d1.item()] + diff_contributions\n",
    "step_plot_values = np.insert(np.cumsum(all_values), 0, base_diff_d2_d1.item())\n",
    "colors = ['grey'] + ['#377eb8' if v > 0 else '#e41a1c' for v in diff_contributions]\n",
    "\n",
    "# Plot the main horizontal bars\n",
    "bars = ax.barh(all_names, all_values, color=colors, alpha=0.9, zorder=2)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add text annotations for the value of each bar\n",
    "for bar, value in zip(bars, all_values):\n",
    "    ha = 'left' if value >= 0 else 'right'\n",
    "    offset = 0.5\n",
    "    ax.text(bar.get_width() + (offset if value >= 0 else -offset), bar.get_y() + bar.get_height()/2,\n",
    "            f'{value:.2f}', va='center', ha=ha, fontsize=10)\n",
    "\n",
    "# Create and configure the top axis for the cumulative sum step plot\n",
    "ax_top = ax.twiny()\n",
    "y_steps = np.arange(len(all_names) + 1) - 0.5\n",
    "step_line, = ax_top.step(step_plot_values, y_steps, color='dimgray', where='post', \n",
    "                         linestyle='--', linewidth=1.5, label=r'Cumulative $\\Delta\\ell$')\n",
    "# ax_top.set_xlabel(r\"Cumulative $\\Delta\\ell$\", fontsize=11, color='dimgray')\n",
    "ax_top.tick_params(axis='x', colors='dimgray')\n",
    "\n",
    "# Manually draw the final sum line for robustness\n",
    "y_limits = ax.get_ylim()\n",
    "final_line, = ax.plot([final_calc_diff, final_calc_diff], y_limits, \n",
    "                       color='purple', linestyle='--', linewidth=2, \n",
    "                       label=r'Final $\\Delta\\ell$', zorder=10)\n",
    "ax.text(final_calc_diff, y_limits[1] - 0.2, f' {final_calc_diff:.2f}', color='purple', \n",
    "        ha='left', va='bottom', fontsize=10)\n",
    "\n",
    "# Create a combined legend for elements from both axes\n",
    "handles = [step_line, final_line]\n",
    "ax.legend(handles=handles, frameon=False, loc=\"lower left\", fontsize=10)\n",
    "\n",
    "# --- Final Aesthetics and Layout ---\n",
    "ax.set_xlabel(r\"Contribution to $\\Delta\\ell$ (Negative favors $d_1$ $\\leftarrow$ | $\\rightarrow$ Positive favors $d_2$)\", fontsize=12)\n",
    "ax.grid(axis='x', linestyle=':', alpha=0.6, zorder=1)\n",
    "ax.axvline(0, color='dimgray', linestyle='-', linewidth=1.2, zorder=1)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['bottom'].set_position(('outward', 5))\n",
    "ax.spines['left'].set_position(('outward', 5))\n",
    "ax_top.spines['top'].set_position(('outward', 5))\n",
    "ax_top.spines['right'].set_visible(False)\n",
    "ax_top.tick_params(axis='y', right=False, labelright=False)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Set axis limits and apply tight layout\n",
    "ax.set_xlim(left=-25, right=55)\n",
    "ax_top.set_xlim(ax.get_xlim()) \n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2dfc1e",
   "metadata": {},
   "source": [
    "## Linearly Separable Unembedding Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566cad6",
   "metadata": {},
   "source": [
    "The projections of the last and second-to-last token vectors onto the positional dimensions are linearly separable, revealing that this composition of positional encodings is key to separating the tokens back out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have a cache to use in the analysis\n",
    "with torch.no_grad():\n",
    "    logits, cache = model.run_with_cache(val_inputs, return_type=\"logits\")\n",
    "\n",
    "# Residuals: [batch, seq, d_model] or [seq, d_model]\n",
    "resid = cache[\"resid_post\", 1].detach().cpu().numpy()\n",
    "if resid.ndim == 2:  # no batch\n",
    "    resid = resid[None, ...]\n",
    "if resid.shape[1] < 2:\n",
    "    raise ValueError(\"Need sequence length >= 2 to compare -1 and -2.\")\n",
    "\n",
    "# Last and second-to-last token vectors\n",
    "vecs_last = resid[:, -1, :]  # [B, d_model] -> o2 (last)\n",
    "vecs_prev = resid[:, -2, :]  # [B, d_model] -> o1 (second-last)\n",
    "\n",
    "# Positional matrix and projections\n",
    "W_pos = model.W_pos.detach().cpu().numpy()\n",
    "projs_last = (vecs_last @ W_pos.T)[:]\n",
    "projs_prev = (vecs_prev @ W_pos.T)[:]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Least-squares projection onto span(W_pos) before unembedding\n",
    "    # projs_last/prev are (B, n_pos) = vec @ W_pos.T\n",
    "    W_pos_t = model.W_pos.cpu()                         # [n_pos, d_model]\n",
    "    G = W_pos_t @ W_pos_t.T                             # [n_pos, n_pos]\n",
    "    G_inv = torch.linalg.pinv(G)                        # robust pseudo-inverse\n",
    "\n",
    "    last_coeffs = torch.from_numpy(projs_last).to(W_pos_t.dtype) @ G_inv  # (B, n_pos)\n",
    "    prev_coeffs = torch.from_numpy(projs_prev).to(W_pos_t.dtype) @ G_inv  # (B, n_pos)\n",
    "\n",
    "    last_proj = last_coeffs @ W_pos_t                   # (B, d_model)\n",
    "    prev_proj = prev_coeffs @ W_pos_t                   # (B, d_model)\n",
    "\n",
    "    last_pos_projs_unembed = last_proj @ model.W_U.cpu()  # (B, vocab)\n",
    "    prev_pos_projs_unembed = prev_proj @ model.W_U.cpu()  # (B, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SVM analysis ---\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# X: points, y: labels\n",
    "X = np.vstack([projs_last, projs_prev])\n",
    "y = np.hstack([np.ones(len(projs_last)), -np.ones(len(projs_prev))])\n",
    "\n",
    "# hard-margin SVM via large C\n",
    "clf = SVC(kernel=\"linear\", C=1e6).fit(X, y)\n",
    "\n",
    "separable = clf.score(X, y) > 0.99\n",
    "print(\"Linearly separable:\", separable)\n",
    "\n",
    "if separable:\n",
    "    w = clf.coef_[0]\n",
    "    b = clf.intercept_[0]\n",
    "    margin = 1.0 / np.linalg.norm(w)\n",
    "    print(\"Margin:\", margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe6cff",
   "metadata": {},
   "source": [
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623af6c6",
   "metadata": {},
   "source": [
    "## D - Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Fig 4 - Attn histogram -----\n",
    "\n",
    "# Extract attention weights (reuse existing cache)\n",
    "attn_sep_d1 = cache[\"pattern\", 0].squeeze()[:, 2, 0].cpu().numpy()\n",
    "attn_sep_d2 = cache[\"pattern\", 0].squeeze()[:, 2, 1].cpu().numpy()\n",
    "attn_o2_sep = cache[\"pattern\", 1].squeeze()[:, -1, 2].cpu().numpy()\n",
    "attn_o2_o1  = cache[\"pattern\", 1].squeeze()[:, -1, 3].cpu().numpy()\n",
    "\n",
    "# Compute global x-range across all histograms\n",
    "all_vals = np.concatenate([attn_sep_d1, attn_sep_d2, attn_o2_sep, attn_o2_o1])\n",
    "x_min, x_max = float(all_vals.min()), float(all_vals.max())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Row 1\n",
    "ax = axes[0, 0]\n",
    "ax.hist(attn_sep_d1, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Attention to d1 (pos 0)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Histogram: SEP → d1 (Layer 0)\")\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.hist(attn_sep_d2, bins=30, color=\"salmon\", edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Attention to d2 (pos 1)\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_title(\"Histogram: SEP → d2 (Layer 0)\")\n",
    "\n",
    "# Row 2\n",
    "ax = axes[1, 0]\n",
    "ax.hist(attn_o2_sep, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Attention to SEP (pos 2)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Histogram: o2 → SEP (Layer 1)\")\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.hist(attn_o2_o1, bins=30, color=\"salmon\", edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Attention to o1 (pos 3)\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_title(\"Histogram: o2 → o1 (Layer 1)\")\n",
    "\n",
    "# Apply the shared x-limits to all axes\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "\n",
    "# Make y-axes the same within each row\n",
    "ymax_row0 = max(axes[0, 0].get_ylim()[1], axes[0, 1].get_ylim()[1])\n",
    "ymax_row1 = max(axes[1, 0].get_ylim()[1], axes[1, 1].get_ylim()[1])\n",
    "axes[0, 0].set_ylim(0, ymax_row0)\n",
    "axes[0, 1].set_ylim(0, ymax_row0)\n",
    "# axes[0,1].set_yticks([])\n",
    "axes[1, 0].set_ylim(0, ymax_row1)\n",
    "axes[1, 1].set_ylim(0, ymax_row1)\n",
    "# axes[1,1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Fig 5 - SEP attn vs accuracy ------\n",
    "\n",
    "# Run model on all validation data\n",
    "all_inputs = val_ds.tensors[0].to(DEV)\n",
    "all_targets = val_ds.tensors[1].to(DEV)\n",
    "with torch.no_grad():\n",
    "    all_logits, all_cache = model.run_with_cache(all_inputs, return_type=\"logits\")\n",
    "\n",
    "# Correctness per sample (both outputs correct)\n",
    "all_predictions = all_logits.argmax(dim=-1)[:, -LIST_LEN:]\n",
    "all_correct = (all_predictions == all_targets[:, -LIST_LEN:]).all(dim=-1).cpu().numpy()\n",
    "\n",
    "# Extract attention SCORES: Layer 0, q=SEP (2), k∈{d1=0, d2=1}\n",
    "scores_l0 = all_cache[\"attn_scores\", 0][:, 0]  # [B, Q, K] (head=0)\n",
    "attn_vals = scores_l0[:, 2, [0, 1]].detach().cpu().numpy()  # [B, 2]\n",
    "x, y = attn_vals[:, 0], attn_vals[:, 1]\n",
    "ok = all_correct.astype(bool)\n",
    "x_ok, y_ok = x[ok], y[ok]\n",
    "x_bad, y_bad = x[~ok], y[~ok]\n",
    "\n",
    "# Axes limits (robust to outliers)\n",
    "x_lo, x_hi = np.percentile(x, [1, 99])\n",
    "y_lo, y_hi = np.percentile(y, [1, 99])\n",
    "pad_x = 0.06 * max(1e-6, x_hi - x_lo)\n",
    "pad_y = 0.06 * max(1e-6, y_hi - y_lo)\n",
    "\n",
    "# Figure (no marginals, no title)\n",
    "fig, ax_sc = plt.subplots(figsize=(6.8, 6.2), dpi=300)\n",
    "\n",
    "c_ok, c_bad = \"#1f77b4\", \"#DC2626\"  # blue, red\n",
    "ms, a_ok, a_bad = 24, 0.35, 0.55\n",
    "\n",
    "# Scatter\n",
    "if len(x_ok):\n",
    "    ax_sc.scatter(x_ok, y_ok, s=ms, c=c_ok, alpha=a_ok, edgecolors=\"none\", label=f\"Correct\")\n",
    "if len(x_bad):\n",
    "    ax_sc.scatter(x_bad, y_bad, s=ms, c=c_bad, alpha=a_bad, edgecolors=\"none\", label=f\"Incorrect\") # (n={len(x_bad)})\n",
    "\n",
    "# Reference lines at 0\n",
    "ax_sc.axvline(0, color=\"#94A3B8\", lw=0.8, ls=\"--\", alpha=0.8)\n",
    "ax_sc.axhline(0, color=\"#94A3B8\", lw=0.8, ls=\"--\", alpha=0.8)\n",
    "\n",
    "# Labels, limits, legend\n",
    "ax_sc.set_xlabel(\"SEP → d1 attention score (L0)\")\n",
    "ax_sc.set_ylabel(\"SEP → d2 attention score (L0)\")\n",
    "ax_sc.set_xlim(x_lo - pad_x, x_hi + pad_x)\n",
    "ax_sc.set_ylim(y_lo - pad_y, y_hi + pad_y)\n",
    "ax_sc.grid(True, linestyle=\":\", alpha=0.8)\n",
    "ax_sc.legend(frameon=True, loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "acc = all_correct.mean()\n",
    "print(f\"Total samples: {len(all_correct)}\")\n",
    "print(f\"Correct predictions: {all_correct.sum()}\")\n",
    "print(f\"Accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d732f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Fig 6 - scatter plot ----\n",
    "\n",
    "# Indices of incorrect samples\n",
    "incorrect_idx = np.where(~all_correct)[0]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    all_inputs[incorrect_idx, 0].detach().cpu().numpy(),\n",
    "    all_inputs[incorrect_idx, 1].detach().cpu().numpy(),\n",
    "    c=\"blue\",\n",
    "    alpha=0.5,\n",
    "    label=\"Incorrect\",\n",
    ")\n",
    "plt.xlabel(\"d1 value\")\n",
    "plt.ylabel(\"d2 value\")\n",
    "plt.title(\"Scatter plot of incorrect predictions by d1 and d2 values\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Fig 7 - Performance on fixed attn patterns -----\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "def fixed_attention_score_hook(pattern, hook=None, attn_0=None, attn_1=None):\n",
    "    # pattern: [B, H, Q, K]\n",
    "    assert (attn_0 is not None) and (attn_1 is not None), \"attn_0 and attn_1 must be specified\"\n",
    "    p0 = torch.as_tensor(attn_0, dtype=pattern.dtype, device=pattern.device)\n",
    "    p1 = torch.as_tensor(attn_1, dtype=pattern.dtype, device=pattern.device)\n",
    "    s = (p0 + p1).clamp_min(1e-12)\n",
    "\n",
    "    # Clone and overwrite the SEP (q=2) row to be a valid distribution over {d1=0, d2=1}\n",
    "    pattern = pattern.clone()\n",
    "    pattern[..., 2, :] = 0.0\n",
    "    pattern[..., 2, 0] = p0 / s\n",
    "    pattern[..., 2, 1] = p1 / s\n",
    "    return pattern\n",
    "\n",
    "def sequence_accuracy_from_logits(logits, targets):\n",
    "    # logits: [B, T, V], targets: [B, T]\n",
    "    preds = logits.argmax(dim=-1)[:, -LIST_LEN:]          # only o1, o2\n",
    "    targs = targets[:, -LIST_LEN:]\n",
    "    return (preds == targs).all(dim=-1).float().mean().item()\n",
    "\n",
    "# Inputs/targets to evaluate on\n",
    "if \"all_inputs\" not in locals() or \"all_targets\" not in locals():\n",
    "    all_inputs = val_ds.tensors[0].to(DEV)\n",
    "    all_targets = val_ds.tensors[1].to(DEV)\n",
    "\n",
    "steps = 100  # NOTE: 100x100 grid can be slow; reduce for quicker runs\n",
    "scores = torch.linspace(0, 1, steps=steps)\n",
    "cache_file = \"fixed_attention_perfs.pkl\"\n",
    "\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        perfs = pickle.load(f)\n",
    "else:\n",
    "    perfs = torch.zeros((steps, steps), dtype=torch.float32)\n",
    "    for i, j in tqdm(itertools.product(range(steps), range(steps)), total=steps**2):\n",
    "        logits = model.run_with_hooks(\n",
    "            all_inputs,\n",
    "            return_type=\"logits\",\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    \"blocks.0.attn.hook_pattern\",\n",
    "                    partial(\n",
    "                        fixed_attention_score_hook,\n",
    "                        attn_0=scores[i].item(),\n",
    "                        attn_1=scores[j].item(),\n",
    "                    ),\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        perf = sequence_accuracy_from_logits(logits, all_targets)\n",
    "        perfs[i, steps - j - 1] = perf  # flip Y so higher j is up\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(perfs, f)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(perfs.T.cpu(), cmap=\"viridis\", aspect=\"auto\", vmin=0.0)\n",
    "plt.colorbar(label=\"Sequence accuracy\")\n",
    "plt.xlabel(\"SEP → d1 attention (L0)\")\n",
    "plt.ylabel(\"SEP → d2 attention (L0)\")\n",
    "plt.title(\"Validation performance for fixed attention scores\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
