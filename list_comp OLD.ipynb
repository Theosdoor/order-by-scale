{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3H1R__HCc54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch, random\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "# from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Configure plotly to use static rendering if widgets fail\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"notebook\"\n",
        "\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "float_formatter = \"{:.5f}\".format\n",
        "np.set_printoptions(formatter={'float_kind':float_formatter})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdUT-1P-_Gsu"
      },
      "source": [
        "How does this model work?\n",
        "\n",
        "super basic two layer transformer with no MLP or even a value matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "1f65592add8048388731ac71645b9a22"
          ]
        },
        "id": "rlhNtRpvCh8v",
        "outputId": "ff423a77-9474-4960-d3e6-7a7e9c5e07c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from artifacts/len_2_v1.pt\n",
            "Moving model to device:  cpu\n"
          ]
        }
      ],
      "source": [
        "# DIGITS = list(range(10))\n",
        "# LIST_LEN = 2\n",
        "# SPECIAL = 10\n",
        "# VOCAB = 11\n",
        "# SEQ_LEN = LIST_LEN * 2 + 1\n",
        "# D_MODEL = 16\n",
        "# N_HEAD = 1\n",
        "# N_LAYER = 2 # 2 layers each with single attn head\n",
        "# FREEZE_WV = True # no value matrix in attn (i.e. attn head can only copy inputs to outputs)\n",
        "# MODEL_PATH = \"artifacts/len_2_v1.pt\"\n",
        "\n",
        "# class DigitDataset(Dataset):\n",
        "#     def __init__(self, n):\n",
        "#         self.data = [[random.randint(0, 9) for _ in range(LIST_LEN)] for _ in range(n)]\n",
        "#         # come up with 'size' lots of sequences of random digits (each seq len 2)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # [d1, d2, 10, d1, d2], where 10 is a special separator token\n",
        "#         seq = self.data[idx]\n",
        "#         tok = seq + [SPECIAL] + seq\n",
        "#         return torch.tensor(tok, dtype=torch.long)\n",
        "\n",
        "# def build_mask(n: int, lead_diag=1) -> torch.Tensor:\n",
        "#     # create attention pattern for a sequence of length n\n",
        "#     # rows are queries, columns are keys\n",
        "#     # float(-inf) means \"ignore this position\" (i.e. becomes 0 in later softmax - see 3b1b video)\n",
        "#     m = torch.triu(torch.ones(n, n) * float(\"-inf\"), diagonal=lead_diag) # prevents attending to future tokens NOTE\n",
        "#     m[LIST_LEN+1:, :LIST_LEN] = float(\"-inf\")\n",
        "#     # m[LIST_LEN+1:] = all tokens after the special token (i.e. queries of output tokens)\n",
        "#     # m[LIST_LEN+1:, :LIST_LEN] = refers to all keys of tokens before special token\n",
        "#     #  ==> this line explicitly forbids the output tokens (when they are the query) from attending to the input tokens (when they are the key).\n",
        "\n",
        "#     # attention mask for [d1, d2, SEP, o1, o2] looks like this (query rows are horizontal, key columns are vertical):\n",
        "#     # -    d1    d2    SEP    o1    o2   (keys)\n",
        "#     # d1   0    -inf   -inf  -inf  -inf\n",
        "#     # d2   0      0    -inf  -inf  -inf\n",
        "#     # SEP  0      0     0    -inf  -inf\n",
        "#     # o1  -inf  -inf    0      0   -inf\n",
        "#     # o2  -inf  -inf    0      0     0\n",
        "#     # (queries)\n",
        "\n",
        "\n",
        "#     # -    SEP   d1    d2     o1    o2   (keys)\n",
        "#     # d1   0    -inf   -inf  -inf  -inf\n",
        "#     # d2   0      0    -inf  -inf  -inf\n",
        "#     # SEP  0      0     0    -inf  -inf\n",
        "#     # o1  -inf  -inf    0      0   -inf\n",
        "#     # o2  -inf  -inf    0      0     0\n",
        "#     # (queries)\n",
        "#     return m\n",
        "\n",
        "\n",
        "# def make_model(device: str = \"cuda\") -> \"HookedTransformer\":\n",
        "#     cfg = HookedTransformerConfig(\n",
        "#         d_model=D_MODEL,\n",
        "#         d_head=D_MODEL // N_HEAD,\n",
        "#         n_layers=N_LAYER,\n",
        "#         n_heads=N_HEAD,\n",
        "#         n_ctx=SEQ_LEN,\n",
        "#         d_vocab=VOCAB,\n",
        "#         d_vocab_out=VOCAB,\n",
        "#         attn_only=True, # no MLP!\n",
        "#     )\n",
        "#     model = HookedTransformer(cfg).to(device)\n",
        "#     if FREEZE_WV:\n",
        "#         set_WV_identity_and_freeze(model)\n",
        "#     return model\n",
        "\n",
        "\n",
        "# def attach_custom_mask(model: \"HookedTransformer\") -> None:\n",
        "#     def _mask(scores, hook=None):\n",
        "#         t = scores.size(-1)\n",
        "#         scores += build_mask(t).to(scores.device)\n",
        "#         return scores\n",
        "    \n",
        "#     # def _mask2(scores, hook=None):\n",
        "#     #     t = scores.size(-1)\n",
        "#     #     scores += build_mask(t, 0).to(scores.device)\n",
        "#     #     return scores\n",
        "#     # model.blocks[0].attn.hook_attn_scores.add_perma_hook(_mask)\n",
        "#     # model.blocks[1].attn.hook_attn_scores.add_perma_hook(_mask2)\n",
        "\n",
        "#     for block in model.blocks:\n",
        "#         block.attn.hook_attn_scores.add_perma_hook(_mask)\n",
        "\n",
        "\n",
        "\n",
        "# def set_WV_identity_and_freeze(model: \"HookedTransformer\") -> None:\n",
        "#     with torch.no_grad():\n",
        "#         eye = torch.eye(D_MODEL).unsqueeze(0)  # add head dim\n",
        "#         for block in model.blocks:\n",
        "#             block.attn.W_V.copy_(eye)\n",
        "#             block.attn.W_V.requires_grad = False\n",
        "\n",
        "# def train(\n",
        "#     epochs: int = 10,\n",
        "#     batch: int = 1024,\n",
        "#     size: int = 50000,\n",
        "#     val: int = 1000,\n",
        "#     device=\"cuda\",\n",
        "# ) -> HookedTransformer:\n",
        "    \n",
        "#     ds = DigitDataset(size)\n",
        "#     dl = DataLoader(ds, batch, shuffle=True)\n",
        "#     model = make_model(device)\n",
        "#     attach_custom_mask(model)\n",
        "#     opt = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "#     loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#     for epoch in tqdm(range(epochs)):\n",
        "#         epoch_loss = 0\n",
        "#         num_batches = 0\n",
        "#         for seq in dl:\n",
        "#             seq = seq.to(device)\n",
        "            \n",
        "#             # FIXED: Only compute loss on the output tokens (positions 3 and 4)\n",
        "#             # Input sequence: [d1, d2, SEP, d1, d2]\n",
        "#             # We want to predict positions 3,4 from positions 0,1,2,3\n",
        "#             logits = model(seq)  # Shape: [batch, seq_len, vocab]\n",
        "            \n",
        "#             # Only compute loss on the output positions (last LIST_LEN tokens)\n",
        "#             output_logits = logits[:, -LIST_LEN:, :]  # [batch, LIST_LEN, vocab]\n",
        "#             output_targets = seq[:, -LIST_LEN:]       # [batch, LIST_LEN]\n",
        "            \n",
        "#             loss = loss_fn(output_logits.reshape(-1, VOCAB), output_targets.reshape(-1))\n",
        "            \n",
        "#             loss.backward()\n",
        "#             opt.step()\n",
        "#             opt.zero_grad()\n",
        "            \n",
        "#             epoch_loss += loss.item()\n",
        "#             num_batches += 1\n",
        "            \n",
        "#         avg_loss = epoch_loss / num_batches\n",
        "        \n",
        "#         # Validation accuracy\n",
        "#         corr = 0\n",
        "#         for _ in range(val):\n",
        "#             d = [random.randint(0, 9) for _ in range(LIST_LEN)]\n",
        "#             corr += generate(model, d) == d\n",
        "#         acc = corr / val\n",
        "#         print(f\"Epoch {epoch+1}: Loss {avg_loss:.4f}, Acc {acc:.2%}\")\n",
        "#     return model\n",
        "\n",
        "# # def train(\n",
        "# #     epochs: int = 10,\n",
        "# #     batch: int = 1024,\n",
        "# #     size: int = 50000,\n",
        "# #     val: int = 1000,\n",
        "# #     device=\"cuda\",\n",
        "# # ) -> HookedTransformer:\n",
        "\n",
        "# #     ds = DigitDataset(size)\n",
        "# #     dl = DataLoader(ds, batch, shuffle=True)\n",
        "# #     model = make_model(device)\n",
        "# #     attach_custom_mask(model)\n",
        "# #     opt = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "# #     loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# #     for _ in tqdm(range(epochs)):\n",
        "# #         for seq in dl:\n",
        "# #             seq = seq.to(device)\n",
        "# #             # OLD\n",
        "# #             # logits = model(seq[:, :-1])  # predict next token, so exclude last token\n",
        "# #             # loss = loss_fn(logits.reshape(-1, VOCAB), seq[:, 1:].reshape(-1))\n",
        "\n",
        "# #             # NEW\n",
        "# #             logits = model(seq) \n",
        "# #             loss = loss_fn(logits.reshape(-1, VOCAB), seq.reshape(-1))\n",
        "\n",
        "# #             loss.backward()\n",
        "# #             opt.step()\n",
        "# #             opt.zero_grad()\n",
        "# #         corr = 0\n",
        "# #         for _ in range(val):\n",
        "# #             d = [random.randint(0, 9) for _ in range(LIST_LEN)]\n",
        "# #             corr += generate(model, d) == d\n",
        "# #         print(f\"acc {corr / val:.2%}\")\n",
        "# #     return model\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def generate(model: HookedTransformer, digits: list[int]) -> list[int]:\n",
        "#     seq = digits + [SPECIAL]\n",
        "#     out: list[int] = []\n",
        "#     for _ in range(LIST_LEN):\n",
        "#         x = torch.tensor(seq + out, device=next(model.parameters()).device).unsqueeze(0)\n",
        "#         nxt = model(x)[:, -1].argmax(-1).item()\n",
        "#         out.append(nxt)\n",
        "#     return out\n",
        "\n",
        "\n",
        "# def save_model(model: HookedTransformer, path: str = MODEL_PATH):\n",
        "#     os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "#     torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "# def load_model(\n",
        "#     path: str = MODEL_PATH, device: str = \"cuda\"\n",
        "# ) -> HookedTransformer:\n",
        "#     model = make_model(device)\n",
        "#     model.load_state_dict(\n",
        "#         torch.load(path, map_location=device)\n",
        "#     )  # map weights to target device\n",
        "#     attach_custom_mask(model)\n",
        "#     model.eval()\n",
        "#     return model\n",
        "\n",
        "# USAGE\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# path = MODEL_PATH\n",
        "# if os.path.exists(path):\n",
        "#     print(\"Loading model from\", path)\n",
        "#     model = load_model(path, device)\n",
        "# else:\n",
        "#     print(\"Training model\")\n",
        "#     model = train(epochs=20, device=device)\n",
        "#     save_model(model, path)\n",
        "\n",
        "\n",
        "\n",
        "# notes\n",
        "#  - try sep as cls token (at start?) - see bert. at start so always unmasked, and can chec if its a good summary of list\n",
        "#  - try making SEP always unmasked ==> like BERT, means it provides better context. Also experiment with in middle vs start?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### New model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, pandas as pd, itertools\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- constants ----------\n",
        "VOCAB, THINK, SEQ = 101, 100, 5  # vocab size, think token, sequence length\n",
        "DEV = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        ")\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# mask: True = allowed, converted to big negative bias later\n",
        "mask_bool = torch.tril(torch.ones(SEQ, SEQ)).bool()\n",
        "mask_bool[3:, :2] = False\n",
        "mask_bool.fill_diagonal_(False)\n",
        "mask_bias = (~mask_bool).float() * -1e5  # additive bias, shape (T,T)\n",
        "mask_bias = mask_bias.unsqueeze(0).unsqueeze(0)  # (1,1,T,T) broadcastable\n",
        "\n",
        "\n",
        "# ---------- data ----------\n",
        "def make_split(n):\n",
        "    xy = torch.randint(0, 100, (n, 2)) \n",
        "    seq = torch.full((n, SEQ), THINK)\n",
        "    seq[:, :2], seq[:, 3:] = xy, xy\n",
        "    return seq\n",
        "\n",
        "\n",
        "train_ds = TensorDataset(make_split(10_000))\n",
        "val_ds = TensorDataset(make_split(2_000))\n",
        "train_dl = DataLoader(train_ds, 128, shuffle=True, drop_last=True)\n",
        "val_dl = DataLoader(val_ds, 256, drop_last=False)\n",
        "\n",
        "print(train_ds[0][0])  # Example sequence: [d1, d2, THINK, d1, d2]\n",
        "\n",
        "\n",
        "# ---------- config helper ----------\n",
        "def mk_cfg(d_model, ln):\n",
        "    return HookedTransformerConfig(\n",
        "        n_layers=2,\n",
        "        n_heads=1,\n",
        "        d_model=d_model,\n",
        "        d_head=d_model,\n",
        "        n_ctx=SEQ,\n",
        "        d_vocab=VOCAB,\n",
        "        attn_only=True,\n",
        "        act_fn=None,\n",
        "        normalization_type=(\"LN\" if ln else None),\n",
        "    )\n",
        "\n",
        "\n",
        "# ---------- utilities ----------\n",
        "def apply_mask_to_scores(scores, hook):\n",
        "    # scores: (batch, heads, Q, K)\n",
        "    return scores + mask_bias.to(scores.device)\n",
        "\n",
        "\n",
        "def strip_bias(m):\n",
        "    for mod in m.modules():\n",
        "        if hasattr(mod, \"bias\") and mod.bias is not None:\n",
        "            mod.bias.requires_grad_(False)\n",
        "            torch.nn.init.zeros_(mod.bias)\n",
        "\n",
        "\n",
        "def accuracy(m):\n",
        "    m.eval()\n",
        "    hits = tots = 0\n",
        "    with torch.no_grad():\n",
        "        for (seq,) in val_dl:\n",
        "            logits = m(seq.to(DEV))[:, 3:5]  # (batch, 2, vocab)\n",
        "            preds = logits.argmax(-1)\n",
        "            hits += (preds == seq[:, 3:5].to(DEV)).sum().item()\n",
        "            tots += preds.numel()\n",
        "    return hits / tots\n",
        "\n",
        "\n",
        "def train(m, max_steps=3000):\n",
        "    opt, ce = torch.optim.AdamW(m.parameters(), 1e-3), torch.nn.CrossEntropyLoss()\n",
        "    dl = itertools.cycle(train_dl)  # infinite iterator\n",
        "    for step in trange(max_steps, desc=\"Training\"):\n",
        "        (seq,) = next(dl)\n",
        "        logits = m(seq.to(DEV))[:, 3:5].reshape(-1, VOCAB)\n",
        "        loss = ce(logits, seq[:, 3:5].reshape(-1).to(DEV))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        if (step + 1) % 100 == 0 and accuracy(m) > 0.999:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving model to device:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   7%|▋         | 199/3000 [00:01<00:18, 152.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving model to device:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  57%|█████▋    | 1699/3000 [00:06<00:04, 268.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving model to device:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   7%|▋         | 199/3000 [00:01<00:18, 154.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving model to device:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  13%|█▎        | 399/3000 [00:02<00:15, 170.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving model to device:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  20%|█▉        | 599/3000 [00:03<00:13, 179.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving model to device:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 3000/3000 [00:15<00:00, 188.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| model       |   d_model | use_ln   | bias   |   val_acc |\n",
            "|:------------|----------:|:---------|:-------|----------:|\n",
            "| d32_ln_bias |        32 | True     | True   |    1      |\n",
            "| d32_noLN    |        32 | False    | True   |    0.9992 |\n",
            "| d32_noBias  |        32 | True     | False  |    1      |\n",
            "| d16         |        16 | True     | True   |    1      |\n",
            "| d8          |         8 | True     | True   |    1      |\n",
            "| d4          |         4 | True     | True   |    0.9792 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ---------- experiment grid ----------\n",
        "specs = [\n",
        "    (\"d32_ln_bias\", 32, True, True),\n",
        "    (\"d32_noLN\", 32, False, True),\n",
        "    (\"d32_noBias\", 32, True, False),\n",
        "    (\"d16\", 16, True, True),\n",
        "    (\"d8\", 8, True, True),\n",
        "    (\"d4\", 4, True, True),\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, d, ln, keep_bias in specs:\n",
        "    model = HookedTransformer(mk_cfg(d, ln)).to(DEV)\n",
        "\n",
        "    # register the same mask hook on every layer\n",
        "    for l in range(model.cfg.n_layers):\n",
        "        model.blocks[l].attn.hook_attn_scores.add_hook(\n",
        "            apply_mask_to_scores, dir=\"fwd\"\n",
        "        )\n",
        "\n",
        "    if not keep_bias:\n",
        "        strip_bias(model)\n",
        "\n",
        "    train(model)\n",
        "    rows.append(\n",
        "        dict(\n",
        "            model=name,\n",
        "            d_model=d,\n",
        "            use_ln=ln,\n",
        "            bias=keep_bias,\n",
        "            val_acc=round(accuracy(model), 4),\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(pd.DataFrame(rows).to_markdown(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving model to device:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  20%|█▉        | 599/3000 [00:03<00:13, 179.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final accuracy: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# The smallest size that gets perfect accuracy is width = 8, and we don't need\n",
        "# the MLP layers.\n",
        "\n",
        "model = HookedTransformer(mk_cfg(8, True)).to(DEV)\n",
        "for l in range(model.cfg.n_layers):\n",
        "    model.blocks[l].attn.hook_attn_scores.add_hook(apply_mask_to_scores, dir=\"fwd\")\n",
        "train(model)\n",
        "print(\"Final accuracy:\", accuracy(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ no attention leakage onto x₁/x₂\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# We confirm below that the model does not leak attention onto the first two\n",
        "# tokens, which are the inputs to the task. The model should only attend to the\n",
        "# first two tokens when predicting the third token, and not attend to them at all\n",
        "# when predicting the fourth and fifth tokens.\n",
        "\n",
        "def check_attention(m, dataloader, eps=1e-3):\n",
        "    for (seq,) in dataloader:\n",
        "        with torch.no_grad():\n",
        "            _, cache = m.run_with_cache(seq.to(DEV))\n",
        "        for l in range(m.cfg.n_layers):\n",
        "            pat = cache[\"pattern\", l][:, 0]  # (batch, Q, K)\n",
        "            leak = pat[:, 3:, :2].sum(dim=-1)  # mass on forbidden keys\n",
        "            if (leak > eps).any():\n",
        "                raise ValueError(\n",
        "                    f\"Layer {l}: output tokens attend to x₁/x₂ by >{eps:.0e}\"\n",
        "                )\n",
        "    print(\"✓ no attention leakage onto x₁/x₂\")\n",
        "\n",
        "\n",
        "sample = torch.tensor([[12, 34, THINK, 12, 34]], device=DEV)\n",
        "_, cache = model.run_with_cache(sample)\n",
        "\n",
        "fig, axes = plt.subplots(1, model.cfg.n_layers, figsize=(6, 3))\n",
        "if model.cfg.n_layers == 1:\n",
        "    axes = [axes]\n",
        "for l in range(model.cfg.n_layers):\n",
        "    pat = cache[\"pattern\", l][0, 0].cpu()  # (5,5)\n",
        "    ax = axes[l]\n",
        "    im = ax.imshow(pat, cmap=\"viridis\", vmin=0, vmax=1)\n",
        "    ax.set_title(f\"Layer {l} Attention Pattern\")\n",
        "    ax.set_xlabel(\"Key Position\")\n",
        "    ax.set_ylabel(\"Query Position\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "check_attention(model, val_dl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0: all attention patterns identical? False\n",
            "Layer 1: all attention patterns identical? False\n",
            "Accuracy with avg-attn: 0.9995\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# The attention patterns are not the same across inputs. However, we can replace\n",
        "# the attention scores with their average and still get almost perfect\n",
        "# performance.\n",
        "\n",
        "# %%\n",
        "\n",
        "all_pats = [[] for _ in range(model.cfg.n_layers)]\n",
        "for (seq,) in val_dl:\n",
        "    with torch.no_grad():\n",
        "        _, cache = model.run_with_cache(seq.to(DEV))\n",
        "    for l in range(model.cfg.n_layers):\n",
        "        pat = cache[\"pattern\", l][:, 0]  # (batch, Q, K)\n",
        "        all_pats[l].append(pat)\n",
        "all_pats = [torch.cat(pats, dim=0) for pats in all_pats]\n",
        "\n",
        "for l, pats in enumerate(all_pats):\n",
        "    identical = torch.allclose(pats, pats[0].expand_as(pats))\n",
        "    print(f\"Layer {l}: all attention patterns identical? {identical}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    avg_pats = [\n",
        "        torch.zeros(SEQ, SEQ, device=DEV) for _ in range(model.cfg.n_layers)\n",
        "    ]\n",
        "    n = 0\n",
        "    for (seq,) in val_dl:\n",
        "        _, cache = model.run_with_cache(seq.to(DEV))\n",
        "        for l in range(model.cfg.n_layers):\n",
        "            avg_pats[l] += cache[\"pattern\", l][:, 0].sum(0)\n",
        "        n += seq.shape[0]\n",
        "    avg_pats = [p / n for p in avg_pats]\n",
        "\n",
        "\n",
        "def mk_hook(avg):\n",
        "    logits = (avg + 1e-12).log()  # log-prob so softmax≈avg, ε avoids -∞\n",
        "\n",
        "    def f(scores, hook):\n",
        "        return logits.unsqueeze(0).unsqueeze(0).expand_as(scores)\n",
        "\n",
        "    return f\n",
        "\n",
        "for l in range(model.cfg.n_layers):\n",
        "    model.blocks[l].attn.hook_attn_scores.add_hook(\n",
        "        mk_hook(avg_pats[l]), dir=\"fwd\"\n",
        "    )\n",
        "\n",
        "print(\"Accuracy with avg-attn:\", accuracy(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_list = [8, 3] # Define a sample input list to visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHGmCN_7Ug-_"
      },
      "outputs": [],
      "source": [
        "# embedding and unembedding matrix w/ pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The original code used a sample token for ablation. We'll use it again for comparison.\n",
        "full_sequence = sample_list + [SPECIAL] + sample_list\n",
        "sample_tokens = torch.tensor(full_sequence, device=device).unsqueeze(0) # Add batch dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "2s3bU9nBiHYa",
        "outputId": "ba19d926-2819-456f-accb-4218b4f6216c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SPECIAL' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# attn pattern = after softmax\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m full_sequence = sample_list + [\u001b[43mSPECIAL\u001b[49m] + sample_list\n\u001b[32m      4\u001b[39m tokens = torch.tensor(full_sequence, device=device).unsqueeze(\u001b[32m0\u001b[39m) \u001b[38;5;66;03m# Add batch dim\u001b[39;00m\n\u001b[32m      5\u001b[39m token_labels = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_list)] + \\\n\u001b[32m      6\u001b[39m                [\u001b[33m\"\u001b[39m\u001b[33mSEP\u001b[39m\u001b[33m\"\u001b[39m] + \\\n\u001b[32m      7\u001b[39m                [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mo\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_list)]\n",
            "\u001b[31mNameError\u001b[39m: name 'SPECIAL' is not defined"
          ]
        }
      ],
      "source": [
        "# attn pattern = after softmax\n",
        "\n",
        "token_labels = [f\"d{i+1}({d})\" for i, d in enumerate(sample_list)] + \\\n",
        "               [\"SEP\"] + \\\n",
        "               [f\"o{i+1}({d})\" for i, d in enumerate(sample_list)]\n",
        "\n",
        "attn_layer = 0\n",
        "attn_hook_name = \"blocks.\"+str(attn_layer)+\".attn.hook_pattern\"\n",
        "logits, attn_cache = model.run_with_cache(sample_tokens, remove_batch_dim=True, stop_at_layer=attn_layer+1, names_filter=[attn_hook_name])\n",
        "attn = attn_cache[attn_hook_name]\n",
        "\n",
        "print(type(attn_cache))\n",
        "print(attn_cache)\n",
        "attention_pattern = attn_cache[\"pattern\", attn_layer, \"attn\"]\n",
        "print(attention_pattern.shape)\n",
        "print(attention_pattern.cpu().numpy())\n",
        "\n",
        "print(\"Layer \"+ str(attn_layer) + \" Head Attention Patterns:\")\n",
        "#  Remove the batch and head dimensions to get a 2D matrix for plotting.\n",
        "attention_pattern_2d = attention_pattern.squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "print(\"Generating attention heatmap...\")\n",
        "fig = px.imshow(\n",
        "    attention_pattern_2d,\n",
        "    x=token_labels,\n",
        "    y=token_labels,\n",
        "    color_continuous_scale='Viridis',\n",
        "    labels=dict(x=\"Key (Memory)\", y=\"Query (Current Token)\", color=\"Attention Weight\"),\n",
        "    title=f\"Attention Pattern for Layer {attn_layer}\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "eecb365d597549e88e049765d0ff072e"
          ]
        },
        "id": "S4WGF_StLmde",
        "outputId": "63c26772-33f2-48ad-ba9f-b4078f8f8b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating the mean attention pattern for L0H0...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9044a54a6281483aacc7074e0dc38f93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean pattern calculated: \n",
            "[[1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.01400 0.98600 0.00000 0.00000 0.00000]\n",
            " [0.03169 0.43461 0.53370 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.84289 0.15711 0.00000]\n",
            " [0.00000 0.00000 0.44249 0.24507 0.31244]]\n",
            "\n",
            "Original pattern: \n",
            "[[1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            " [0.01287 0.98713 0.00000 0.00000 0.00000]\n",
            " [0.02961 0.39080 0.57959 0.00000 0.00000]\n",
            " [0.00000 0.00000 0.78919 0.21081 0.00000]\n",
            " [0.00000 0.00000 0.31489 0.19574 0.48937]]\n",
            "\n",
            "Original Loss: 0.651\n",
            "Ablated Loss (Using Mean Attention Pattern): 0.652\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- Step 1: Calculate the Mean Attention Pattern ---\n",
        "\n",
        "layer_to_ablate = 0\n",
        "head_index_to_ablate = 0 # fixed\n",
        "\n",
        "# We need a dataset to calculate the average over\n",
        "dataset_for_avg = DigitDataset(2000) # Use a few thousand samples for a stable average\n",
        "dataloader_for_avg = DataLoader(dataset_for_avg, batch_size=128)\n",
        "\n",
        "# Get the correct hook name for the attention pattern\n",
        "attn_pattern_hook_name = utils.get_act_name(\"pattern\", layer_to_ablate)\n",
        "\n",
        "# Initialize a tensor to store the sum of attention patterns\n",
        "# Shape: [query_pos, key_pos]\n",
        "accumulated_patterns = torch.zeros((SEQ_LEN, SEQ_LEN), device=device)\n",
        "num_samples = 0\n",
        "\n",
        "print(\"Calculating the mean attention pattern for L0H0...\")\n",
        "# Loop over the dataset without tracking gradients\n",
        "with torch.no_grad():\n",
        "    for sample_tokens_batch in tqdm(dataloader_for_avg):\n",
        "        sample_tokens_batch = sample_tokens_batch.to(device)\n",
        "        # Run the model and cache the attention patterns\n",
        "        _, cache = model.run_with_cache(\n",
        "            sample_tokens_batch,\n",
        "            names_filter=[attn_pattern_hook_name]\n",
        "        )\n",
        "        # Get the patterns for the specific head\n",
        "        # Shape: [batch, head_index, query_pos, key_pos]\n",
        "        patterns_batch = cache[attn_pattern_hook_name][:, head_index_to_ablate, :, :]\n",
        "        \n",
        "        # Add the patterns to our accumulator\n",
        "        accumulated_patterns += patterns_batch.sum(dim=0)\n",
        "        num_samples += len(sample_tokens_batch)\n",
        "\n",
        "# Calculate the mean by dividing by the total number of samples\n",
        "mean_pattern = accumulated_patterns / num_samples\n",
        "\n",
        "print(f\"Mean pattern calculated: \\n{mean_pattern.numpy()}\\n\")\n",
        "print(f\"Original pattern: \\n{attention_pattern_2d}\\n\")\n",
        "\n",
        "# --- Step 2: Define the Hook and Run the Mean Ablation Experiment ---\n",
        "\n",
        "# This new hook replaces the current attention pattern with the mean pattern\n",
        "def mean_ablation_hook(\n",
        "    pattern, # Float[torch.Tensor, \"batch head_index query_pos key_pos\"]\n",
        "    hook,\n",
        "):\n",
        "    # Replace the pattern for the target head with our pre-calculated mean pattern\n",
        "    # The mean_pattern is [query_pos, key_pos], we broadcast it across the batch dimension\n",
        "    pattern[:, head_index_to_ablate, :, :] = mean_pattern \n",
        "    return pattern\n",
        "\n",
        "# Get original loss (no ablation)\n",
        "original_loss = model(sample_tokens, return_type=\"loss\")\n",
        "\n",
        "# Run the model with the new mean ablation hook\n",
        "mean_ablated_loss = model.run_with_hooks(\n",
        "    sample_tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[(\n",
        "        attn_pattern_hook_name, # Hook the attention PATTERN this time\n",
        "        mean_ablation_hook\n",
        "    )]\n",
        ")\n",
        "\n",
        "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
        "print(f\"Ablated Loss (Using Mean Attention Pattern): {mean_ablated_loss.item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss is similar when using the mean attention pattern (over 2000 diff random 2-lists) on the sample list. This suggests that the individual digits themselves don't matter for accuracy!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1 Ablation\n",
            "BEFORE: \n",
            "[[[1.00000 0.00000 0.00000 0.00000 0.00000]\n",
            "  [0.16118 0.83882 0.00000 0.00000 0.00000]\n",
            "  [0.99574 0.00423 0.00002 0.00000 0.00000]\n",
            "  [0.00000 0.00000 0.99128 0.00872 0.00000]\n",
            "  [0.00000 0.00000 0.45566 0.32340 0.22094]]]\n",
            "AFTER: \n",
            "[[[0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            "  [0.00000 0.00000 0.00000 0.00000 0.00000]\n",
            "  [0.99574 0.00000 0.00000 0.00000 0.00000]\n",
            "  [0.00000 0.00000 0.99128 0.00000 0.00000]\n",
            "  [0.00000 0.00000 0.00000 0.00000 0.00000]]]\n",
            "Original Loss: 0.651\n",
            "Ablated Loss (zeroing attention): 0.823\n"
          ]
        }
      ],
      "source": [
        "# try setting specific attention positions to zero\n",
        "layer_to_ablate = 1 # output digits do nothijg in layer 0\n",
        "print(f\"Layer {layer_to_ablate} Ablation\")\n",
        "# Define which specific attention position you want to zero out\n",
        "query_pos_to_zero, key_pos_to_zero = [4,2] # (top left = [0,0]. query is the row, key is the column)\n",
        "\n",
        "def specific_attention_ablation_hook(\n",
        "    pattern, # Shape: [batch, head_index, query_pos, key_pos]\n",
        "    hook\n",
        "):\n",
        "    # print(f\"Original attention at [{query_pos_to_zero}, {key_pos_to_zero}]: {pattern[0, head_index_to_ablate, query_pos_to_zero, key_pos_to_zero].item():.4f}\")\n",
        "    \n",
        "    # Set specific attention weight to 0\n",
        "    with torch.no_grad():\n",
        "        print(f'BEFORE: \\n{pattern[:, head_index_to_ablate, :, :].cpu().numpy()}')\n",
        "\n",
        "        pattern[:, head_index_to_ablate, query_pos_to_zero, :] = 0.0\n",
        "        pattern[:, head_index_to_ablate, 3, 3] = 0.0\n",
        "        pattern[:, head_index_to_ablate, 0:2, :] = 0.0\n",
        "        pattern[:, head_index_to_ablate, 2, 1:] = 0.0\n",
        "        \n",
        "        print(f'AFTER: \\n{pattern[:, head_index_to_ablate, :, :].cpu().numpy()}')\n",
        "    \n",
        "    return pattern\n",
        "\n",
        "# Get the attention pattern hook name\n",
        "attn_pattern_hook_name = utils.get_act_name(\"pattern\", layer_to_ablate)\n",
        "\n",
        "# Run with the specific position ablated\n",
        "original_loss = model(sample_tokens, return_type=\"loss\")\n",
        "ablated_loss = model.run_with_hooks(\n",
        "    sample_tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[(\n",
        "        attn_pattern_hook_name,\n",
        "        specific_attention_ablation_hook\n",
        "    )]\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
        "print(f\"Ablated Loss (zeroing attention): {ablated_loss.item():.3f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using hook names: blocks.0.hook_resid_post and blocks.1.hook_resid_post\n",
            "4\n",
            "10\n",
            "8\n",
            "3\n",
            "3\n",
            "--- Verifying Prediction from Intermediate Residual Streams ---\n",
            "Sample list: [8, 3], Target final token: 3\n",
            "--------------------------------------------------\n",
            "Prediction using resid_post_L0 @ W_U: 6\n",
            "Is it correct? ❌\n",
            "--------------------------------------------------\n",
            "Prediction using resid_post_L1 @ W_U (final output): 3\n",
            "Is it correct? ✅ (obviously)\n"
          ]
        }
      ],
      "source": [
        "# \"resid after layer 0” @ W_U and see if that gives you the last token\n",
        "\n",
        "# The hook name for the residual stream after layer 0\n",
        "resid_l0_hook_name = utils.get_act_name(\"resid_post\", 0)\n",
        "# The hook name for the residual stream after layer 1 (the final one)\n",
        "resid_l1_hook_name = utils.get_act_name(\"resid_post\", 1)\n",
        "print(f\"Using hook names: {resid_l0_hook_name} and {resid_l1_hook_name}\")\n",
        "\n",
        "# The position from which the final prediction is made is the last one in sample_tokens[:, :-1]\n",
        "prediction_pos = -1\n",
        "# The actual target token\n",
        "target_token = sample_tokens[0, -1].item() # 3\n",
        "\n",
        "# Run the model and cache the activations from both layers\n",
        "with torch.no_grad():\n",
        "    _, cache = model.run_with_cache(\n",
        "        sample_tokens,\n",
        "        names_filter=[\n",
        "            resid_l0_hook_name, \n",
        "            resid_l1_hook_name\n",
        "            ]\n",
        "    )\n",
        "    \n",
        "    # Get the residual stream at the prediction position from after L0\n",
        "    resid_l0 = cache[resid_l0_hook_name][0, prediction_pos, :]\n",
        "    # Get the residual stream at the prediction position from after L1\n",
        "    resid_l1 = cache[resid_l1_hook_name][0, prediction_pos, :]\n",
        "\n",
        "    # Get the unembedding matrix\n",
        "    W_U = model.W_U\n",
        "\n",
        "    all_resid = cache[resid_l1_hook_name][0]\n",
        "    for e in all_resid:\n",
        "        print((e @ W_U).argmax().item())\n",
        "\n",
        "    # Manually calculate logits by multiplying the residual stream by the unembedding matrix\n",
        "    logits_from_l0 = resid_l0 @ W_U\n",
        "    logits_from_l1 = resid_l1 @ W_U\n",
        "\n",
        "    # print(f\"Logits from L0: {logits_from_l0.cpu().numpy()}\")\n",
        "    # print(f\"Logits from L1: {logits_from_l1.cpu().numpy()}\") # len 11 because vocab size is 11 (0-9 & special token)\n",
        "\n",
        "    # Get the predicted token index from the logits\n",
        "    prediction_from_l0 = logits_from_l0.argmax().item()\n",
        "    prediction_from_l1 = logits_from_l1.argmax().item()\n",
        "\n",
        "\n",
        "print(f\"--- Verifying Prediction from Intermediate Residual Streams ---\")\n",
        "print(f\"Sample list: {sample_list}, Target final token: {target_token}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Prediction using resid_post_L0 @ W_U: {prediction_from_l0}\")\n",
        "print(f\"Is it correct? {'✅' if prediction_from_l0 == target_token else '❌'}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Prediction using resid_post_L1 @ W_U (final output): {prediction_from_l1}\")\n",
        "print(f\"Is it correct? {'✅ (obviously)' if prediction_from_l1 == target_token else '❌'}\")\n",
        "\n",
        "\n",
        "# so the first layer isnt enough to get final token\n",
        "# but the 2nd layer doesn't need the last attn row\n",
        "# ==> how can it predict the final token without the last attention row?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJUVmN31U9Z-"
      },
      "outputs": [],
      "source": [
        "# neels grokking paper - formula from model weights that represents\n",
        "# but actual d_i are arbitrary\n",
        "\n",
        "# TODO\n",
        "# attn pattern (try and break it - means, rand values), neels paper - formula ideas, embed/unembed matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHamZW-IComj",
        "outputId": "4b178d3b-af70-4cae-f9b1-948004f49122"
      },
      "outputs": [],
      "source": [
        "# # The position of the separator token, which acts as our compression point\n",
        "# COMPRESSION_POS = LIST_LEN\n",
        "\n",
        "# # The name of the hook point for the residual stream after Layer 1\n",
        "# COMPRESSION_HOOK_NAME = utils.get_act_name(\"resid_post\", 1)\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def get_compressed_representation(model, digits_list):\n",
        "#     \"\"\"\n",
        "#     Runs the model on a list of digits and returns the activation\n",
        "#     at the compression point.\n",
        "#     \"\"\"\n",
        "#     # The input to the model should not contain the second list of digits\n",
        "#     # as the compression happens before generation.\n",
        "#     tokens = torch.tensor([digits_list + [SPECIAL]], device=device)\n",
        "#     _, cache = model.run_with_cache(tokens, names_filter=[COMPRESSION_HOOK_NAME])\n",
        "#     # Get the activation: [batch, position, d_model] -> [d_model]\n",
        "#     compressed_vector = cache[COMPRESSION_HOOK_NAME][0, COMPRESSION_POS].cpu()\n",
        "#     return compressed_vector\n",
        "\n",
        "# # Generate a dataset of compressed representations\n",
        "# num_samples = 2000\n",
        "# compressed_vectors = []\n",
        "# all_digit_lists = [] # Store original lists for hover text\n",
        "# labels = [] # Will store the sum of digits\n",
        "# print(f\"Generating {num_samples} samples...\")\n",
        "# for _ in range(num_samples):\n",
        "#     digit_list = [random.randint(0, 9) for _ in range(LIST_LEN)]\n",
        "#     all_digit_lists.append(digit_list)\n",
        "#     vec = get_compressed_representation(model, digit_list)\n",
        "#     compressed_vectors.append(vec.numpy())\n",
        "#     labels.append(sum(digit_list)) # Label is the sum of the digits\n",
        "\n",
        "# compressed_vectors = np.array(compressed_vectors)\n",
        "\n",
        "# # Use PCA to find the 3 most important dimensions\n",
        "# print(\"Running PCA...\")\n",
        "# pca = PCA(n_components=3)\n",
        "# compressed_pca = pca.fit_transform(compressed_vectors)\n",
        "\n",
        "# # --- 4. VISUALIZATION ---\n",
        "\n",
        "# print(\"Generating plot...\")\n",
        "\n",
        "# # Create a figure for the 3D plot\n",
        "# fig = go.Figure()\n",
        "\n",
        "# # Add the scatter plot of the compressed list vectors\n",
        "# fig.add_trace(go.Scatter3d(\n",
        "#     x=compressed_pca[:, 0],\n",
        "#     y=compressed_pca[:, 1],\n",
        "#     z=compressed_pca[:, 2],\n",
        "#     mode='markers',\n",
        "#     marker=dict(\n",
        "#         size=3,\n",
        "#         color=labels, # Color by the sum of digits\n",
        "#         colorscale='Turbo', # A nice rainbow colorscale\n",
        "#         opacity=0.7,\n",
        "#         colorbar=dict(title='Sum of Digits in List'),\n",
        "#     ),\n",
        "#     name='Compressed Lists',\n",
        "#     # Add informative hover text\n",
        "#     hovertext=[f'List: {dl}<br>Sum: {s}' for dl, s in zip(all_digit_lists, labels)],\n",
        "#     hoverinfo='text'\n",
        "# ))\n",
        "\n",
        "# # Update layout for clarity\n",
        "# fig.update_layout(\n",
        "#     title='Structure of the Compressed Representation (Colored by Sum)',\n",
        "#     scene=dict(\n",
        "#         xaxis_title='Principal Component 1',\n",
        "#         yaxis_title='Principal Component 2',\n",
        "#         zaxis_title='Principal Component 3'\n",
        "#     ),\n",
        "#     margin=dict(r=20, b=10, l=10, t=40)\n",
        "# )\n",
        "\n",
        "# fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
